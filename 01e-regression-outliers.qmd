---
title: "Introduction to Statistical Modeling"
subtitle: "Outliers"
author: "Joris Vankerschaver"
pdf-engine: lualatex
format:
  beamer:
    theme: Pittsburgh
    colortheme: default
    fonttheme: default
    include-in-header:
      - file: header.tex
---

```{r include = FALSE}
CWD <- read.table("./datasets/01-linear-regression/christ.csv", header = T, sep = ",", dec = ".")
attach(CWD)

model <- lm(CWD.BASA ~ RIP.DENS)
model3 <- lm(I(log(CWD.BASA)) ~ RIP.DENS + I(RIP.DENS^2))
summary(model)
confint(model)

needles <- read.table("./datasets/01-linear-regression/needles.txt", header = T, sep = "\t", dec = ".")
attach(needles)
model_l5 <- lm(length ~ nitrogen * phosphor + potassium 
               + phosphor * residu)

library(car)

body.fat <- read.table("./datasets/01-linear-regression/bodyfatNKNW.txt", header = T, dec = ".")
attach(body.fat)

birnh <- read.table("./datasets/01-linear-regression/birnh.txt", header = T, sep = "\t", dec = ".")
attach(birnh)
```

## Outliers / Influential observations 

- Dataset often contains extreme values for outcome $Y$ and/or predictors $X$
- These **can** influence regression line strongly (but don't have to)

## Influence of influential observations 

```{r}
#| out-width: 4in
#| out-height: 3.5in
#| fig-width: 6
#| fig-height: 4.5
#| fig-align: center

set.seed(12345)  
n <- 10
X <- sort(3*runif(n))
Y <- X + 0.3*rnorm(n)

plot_data_outliers <- function(outlier = NULL) {
  X_outliers <- c(X, outlier[1])
  Y_outliers <- c(Y, outlier[2])
  
  m_regular <- lm(Y ~ X)
  m_outliers <- lm(Y_outliers ~ X_outliers)
  
  plot(X, Y, xlim = c(-1, 4), ylim = c(-2, 5), pch = 19,
       xlab = "", ylab = "")
  if (!is.null(outlier)) {
    # Draw the outlier
    points(outlier[1], outlier[2], pch = 19, col = "red")
    # Draw the regression line without the outlier
    abline(m_regular, lty = "dashed")
  }
  abline(m_outliers)
}

plot_data_outliers()
```

## Influence of influential observations 

```{r}
#| out-width: 4in
#| out-height: 3.5in
#| fig-width: 6
#| fig-height: 4.5
#| fig-align: center

plot_data_outliers(c(3, 4))
```


## Influence of influential observations 

```{r}
#| out-width: 4in
#| out-height: 3.5in
#| fig-width: 6
#| fig-height: 4.5
#| fig-align: center

plot_data_outliers(c(0, 4))
```

## Influence of influential observations 

```{r}
#| out-width: 4in
#| out-height: 3.5in
#| fig-width: 6
#| fig-height: 4.5
#| fig-align: center

plot_data_outliers(c(4, -2))
```

## Tracking influential observations

**Residuals**:

- Indicate how far outcome deviates from regression line
- Normally distributed with mean 0 and variance $\sigma^2 = MSE$.

Hence, can be used to identify extreme outcomes:

- 95% of residuals expected in interval $[-2\sigma, 2\sigma]$
- Observations where residual is much larger are probably outliers.

## Exteme outcomes in analysis larches?

```{r}
#| out-width: 4in
#| out-height: 3.5in
#| fig-width: 6
#| fig-height: 4.5
#| fig-align: center

qqnorm(model_l5$resid, pch = 19)
```

## Tracking influential observations

- **Scatterplots** of outcome in function of predictors can be used to identify extreme outcomes and predictors
- When multiple predictors, these plots have serious shortcomings

## Multivariate outliers: $Y$ versus $X_1$ or $X_2$

```{r}
#| out-width: 4in
#| out-height: 3.5in
#| fig-width: 6
#| fig-height: 4.5
#| fig-align: center

set.seed(1234)
n <- 30
x1 <- 3*runif(n)
x2 <- x1 + 0.5*rnorm(n)
y <- 2*x1 + x2 + 5*rnorm(n)

x1_outlier <- 2.5
x2_outlier <- 0
y_outlier <- 5

range <- function(...) {
  c(floor(min(...)), ceiling(max(...)))
}

x1_lim <- range(x1, x1_outlier)
x2_lim <- range(x2, x2_outlier)
y_lim <- range(y, y_outlier)

par(cex = 1.5, mfrow=c(1, 2))
plot(x1, y, xlim = x1_lim, ylim = y_lim)
points(x1_outlier, y_outlier, pch = 15, col = "red")
plot(x2, y, xlim = x2_lim, ylim = y_lim)
points(x2_outlier, y_outlier, pch = 15, col = "red")
```


## Multivariate outliers: $X_1$ versus $X_2$

```{r}
#| out-width: 4in
#| out-height: 3.5in
#| fig-width: 6
#| fig-height: 4.5
#| fig-align: center

plot(x1, x2, xlim = x1_lim, ylim = x2_lim)
points(x1_outlier, x2_outlier, pch = 15, col = "red")
```


## Leverage

\begin{block}{Leverage (influence)} 
\begin{itemize}
\item Diagnostic measure to identify influential predictor-observations
\item Leverage of $i^{th}$ observation is: 
\begin{itemize}
\item measure for distance of predictor for $i^{th}$ observation to average predictor value
\item $i^{th}$ diagonal element of \alert{hat matrix} $H$: function of predictors $X$ that maps outcome on predictions
$$
  \textrm{prediction}=HY
$$
\end{itemize}
\end{itemize}
\end{block}
The larger the leverage for $i^{th}$ observation, the closer $i^{th}$ prediction is to $i^{th}$ outcome

## Interpretation of leverage

- If leverage for $i^{th}$ observation large, then
  - it has predictor values that deviate strongly from the mean
  - it \alert{possibly} has large influence on regression coefficients and predictions

- Leverage is on average $p/n$ with $p$ number of unknown parameters
- \alert{Extreme leverage}: larger than $2p/n$

## Leverage in analysis of larches

```{r echo=FALSE}
p=7
n=26
lev5 <- hatvalues(model_l5)
cutoff <- 2*p/n
plot(lev5, pch = 20, xlab = "", ylab = "Leverage")
abline(h=cutoff)
text(1.3, 0.91, 1)
text(4.3, 0.59, 4)
```
Note: larches model has $p = 7$, so $2p/n = 0.54$.

## Cook's distance

- Diagnostic measure for influence of $i^{th}$ observation on all predictions
- Equivalent, for influence of $i^{th}$ observation on all estimated coefficients
- Cook's distance for $i^{th}$ observation is obtained by comparing each prediction $\hat{Y}_j$ with prediction $\hat{Y}_{j(i)}$ that would be obtained \alert{if $i^{th}$ observation was deleted}
$$
  D_i=\frac{\sum_{j=1}^n(\hat{Y}_j-\hat{Y}_{j(i)})^2}{p \cdot\mathrm{MSE}}
$$ 

## Interpretation Cook's distance

- If Cook's distance $D_i$ large, then $i^{th}$ observation has large influence on predictions and coefficients
- \alert{Extreme Cook's distance}: exceeds  50\% percentile of $F_{p,n-p}$-distribution

\begin{exampleblock}{Example} 
\begin{itemize}
\item In analysis of larches is $p=7, n=26$ and the 50\% percentile of $F_{p,n-p}$-distribution 0.94
\item Cook's distance of first observation is 1.5 and corresponds to 77\% percentile
\item Conclusion: first observation has large influence on estimated regression coefficients 
\end{itemize}
\end{exampleblock}


## Cook's distance in analysis of larches

```{r echo=FALSE}
cd5 <- cooks.distance(model_l5)
plot(cd5, type = "h", ylab = "Cook's distance", xlab = "")
text(1.2, 1.5, 1)
text(6.2, 0.22, 6)
text(7.2, 0.24, 7)
```

## Analysis of larches: residual plots

\centering
```{r echo = FALSE}
par(mfrow=c(2,2))
plot(model_l5)
```

## DFBETAS

On what coefficient(s) will first observation have large influence?

\begin{block}{DFBETAS} 
\begin{itemize}
\item Diagnostic measure for influence of $i^{th}$ observation \alert{on each regression coefficient separately}
\item DFBETAS for $i^{th}$ observation and $j^{th}$ coefficient is obtained by comparing $j^{th}$ coefficient $\hat{\beta}_j$ with coefficient $\hat{\beta}_{j(i)}$ from model \alert{if $i^{th}$ observation would have been deleted}
$$\textrm{DFBETAS}_{j(i)}=\frac{\hat{\beta}_{j}-\hat{\beta}_{j(i)}}{\textrm{SD}(\hat{\beta}_{j})}$$ 
\end{itemize}
\end{block}

## Interpretation DFBETAS

- Sign indicates if deleting observation $i$ causes an increase (DFBETAS$<0$) or decrease (DFBETAS$>0$) in each coefficient
- \alert{Extreme DFBETAS}: exceeds 1 in small to moderate datasets, and $2/\sqrt{n}$ in large datasets

## DFBETAS in analysis of larches

```{r echo = FALSE}
dfb5 <- dfbetas(model_l5)[1,]
plot(dfb5, pch = 20, ylab = "DFBETAS for observation 1", xlab = "Parameter number")
```

## DFBETAS in analysis of larches

First observation has \alert{large influence on interaction between phosphorus and residual ash}:

- current coefficient is -598.08 (SE 290.02);
- DFBETAS is 2.17;
- after deletion of first observation, interaction between phosphorus and residual ash will be around
$$
  -598.08-2.17\times 290.02=-1227.42
$$


## Histogram and QQ-plot of interaction

```{r echo=FALSE}
inter5 <- phosphor * residu
par(mfrow=c(1,2))
hist(inter5, xlab = "phosphor * residual ash", main = "Histogram of phosphor * residu")
qqnorm(inter5)
```

## Analysis of larches after deletion $1^{st}$ observation

\small
```{r}
nitrogen2 <- nitrogen[-1]
phosphor2 <- phosphor[-1]
potassium2 <- potassium[-1]
residu2 <- residu[-1]
length2 <- length[-1]
model_l6 <- lm(length2 ~ nitrogen2 *  phosphor2 + potassium2 
               + phosphor2 * residu2)
```
```{r echo=FALSE}
summary(model_l6)$coefficients
```

## Analysis of larches after deletion interaction

\small
```{r}
model_l7 <- lm(length ~ nitrogen *  phosphor + potassium 
               + residu)
```
```{r echo=FALSE}
summary(model_l7)$coefficients
```

## Analysis of larches: final model

\small
```{r}
model_l8 <- lm(length ~ nitrogen *  phosphor + potassium)
```
```{r echo=FALSE}
summary(model_l8)$coefficients
```

## Final analysis: leverage

\centering
```{r echo=FALSE}
p2=5
lev8 <- hatvalues(model_l8)
cutoff2 <- 2*p2/n
par(mfrow=c(1,2))
plot(lev5, pch = 20, xlab = "", ylab = "Leverage")
abline(h=cutoff)
text(1.5, 0.91, 1)
text(4.5, 0.59, 4)
plot(lev8, pch = 20, xlab = "", ylab = "Leverage", ylim = c(0.0,0.9))
abline(h=cutoff2)
text(4.5, 0.59, 4)
```

## Final analysis: Cook's distance

\centering
```{r echo=FALSE}
cd8 <- cooks.distance(model_l8)
plot(cd8, type = "h", ylab = "Cook's distance", xlab = "")
text(4.2, 0.12, 4)
text(6.2, 0.17, 6)
text(7.2, 0.37, 7)
```

## Final analysis: residual plots

\centering
```{r echo=FALSE}
par(mfrow=c(2,2))
plot(model_l8)
```