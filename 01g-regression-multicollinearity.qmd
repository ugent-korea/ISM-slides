---
title: "Introduction to Statistical Modeling"
subtitle: "Multicollinearity"
author: "Joris Vankerschaver"
pdf-engine: lualatex
format:
  beamer:
    theme: Pittsburgh
    colortheme: default
    fonttheme: default
    include-in-header:
      - file: header.tex
---

```{r include = FALSE}
needles <- read.table("./datasets/01-linear-regression/needles.txt", header = T, sep = "\t", dec = ".")
attach(needles)
model_l5 <- lm(length ~ nitrogen * phosphor + potassium 
               + phosphor * residu)
model_l8 <- lm(length ~ nitrogen *  phosphor + potassium)

body.fat <- read.table("./datasets/01-linear-regression/bodyfatNKNW.txt", header = T, dec = ".")
attach(body.fat)
```

## Multicollinearity 

- There is **multicollinearity** when 2 or more predictors are correlated
- **Can possibly cause problems**: if there is strong correlation between 2 predictors $X_1$ and $X_2$, it becomes difficult to discern effect of $X_1$ of effect of $X_2$

Example: If $X_1=X_2$, then 
$$
  E(Y|X_1,X_2)=\beta_0+\beta_1X_1+\beta_2X_2=\beta_0+(\beta_1+\beta_2)X_1
$$

Consequences

- Numerically instable estimates
- Estimates with large standard errors
- Difficult interpretation of coefficients

## Diagnosing multicollinearity

Multicollinearity can be recognized through:

- **Instability**:
  - Large changes in coefficients after adding a predictor
  - Very wide confidence intervals
  - Unexpected results 
- **Strong correlation** between predictors:
  - Example: usually strong correlation between $X_f$ and $X_fX_s$
  - Can sometimes be eliminated by **centering** (subtracting the mean): 
$$
  X \to X-\bar X.
$$


## Impact of centering

```{r}
#| out-width: 4in
#| out-height: 3.5in
#| fig-width: 6
#| fig-height: 4.5
#| fig-align: center

x <- rnorm(100, 5, 1)
cx <- x - mean(x)
y <- x^2
cy <- cx^2
r1 <- cor(x,y)
r2 <- cor(cx,cy)
par(mfrow=c(1,2))
plot(x, y, pch = 20, main = paste("Correlation = ", round(r1,2)), xlab = "x", ylab = expression(x^2))
abline(lm(y ~ x), col = "blue")
plot(cx, cy, pch = 20, main = paste("Correlation = ", round(r2,2)), xlab = "cx", ylab = expression(cx^2))
abline(lm(cy ~ cx), col = "blue")
``` 

## Scatterplot matrix - before centering

```{r}
nitrogenphospor <- nitrogen * phosphor 
minerals <- cbind(nitrogen, phosphor, potassium, nitrogenphospor)
colnames(minerals)[4] <- "nitrogen*phosphor"
pairs(minerals)
```

## Scatterplot matrix - after centering

```{r}
cnitrogen <- nitrogen - mean(nitrogen)
cphosphor <- phosphor - mean(phosphor)
cpotassium <- potassium - mean(potassium)
cresidu <- residu - mean(residu)
cnitrogenphospor <- cnitrogen * cphosphor 
minerals2 <- cbind(cnitrogen, cphosphor, cpotassium, cnitrogenphospor)
colnames(minerals2)[4] <- "cnitrogen*cphosphor"
pairs(minerals2)
```

## Diagnosing multicollinearity

Previous diagnostics are limited:

- Even if pairwise correlations between predictors $X_1,X_2,X_3$ low, there can be strong multicollinearity.
- E.g., when strong correlation between $X_1$ and a linear combination of $X_2$ and $X_3$.

**Variance inflation factor** for $k^{th}$ coefficient:
$$
  \textrm{VIF}_k=\left(1-R_k^2\right)^{-1}
$$
with $R_k^2$ the $R^2$ of linear regression of $k^{th}$ predictor on other predictors.

## Interpretation VIF

- $\textrm{VIF}_k \geq 1$; $\textrm{VIF}_k=1$ if $k^{th}$ predictor not linearly associated with other predictors.
- Expresses how much larger variance on $k^{th}$ coefficient is than when all predictors were independent.
- Average quadratic distance between estimated and true coefficients is proportionate with average VIF.
- Critical multicollinearity: maximum VIF of at least 10.

## Variance inflation factors

```{r}
#| out-width: 4in
#| out-height: 3.5in
#| fig-width: 6
#| fig-height: 4.5
#| fig-align: center

library(car)
model_l8b <- lm(length ~ cnitrogen *  cphosphor + cpotassium)
model_l5b <- lm(length ~ cnitrogen * cphosphor + cpotassium + cphosphor * cresidu)
v1 <- vif(model_l5)
v2 <- vif(model_l5b)
par(mfrow=c(1,2))
plot(v1, pch = 20, main = "Before centering", ylab = "VIF", xlab = "Parameter number")
abline(h=10, lty = 2, col = "blue")
z <- seq(1.4,2.8,0.2)
plot(v2, pch = 20, main = "After centering", ylab = "VIF", xlab = "Parameter number", yaxp = c(1.4, 2.8, 7))
```

## Simpler interpretation of coefficients

Coefficients (without centering)
\footnotesize
```{r echo=FALSE}
summary(model_l8)$coefficients
```
\normalsize
Coefficients (with centering)
\footnotesize
```{r echo=FALSE}
summary(model_l8b)$coefficients
```

## Example: Prediction body fat

- Determining percentage body fat difficult and expensive
- Study investigates association between
  - $Y$: body fat
  - $X_1$: triceps skinfold thickness
  - $X_2$: thigh circumference
  - $X_3$: midarm circumference
- 20 healthy women between 25 and 34 years old

## Analysis in R

\footnotesize
```{r echo=FALSE}
model_bf <- lm(bodyfat ~ triceps.skinfold.thickness + 
                         thigh.circumference +
                         midarm.circumference)
summary(model_bf)
```

## Scatterplot matrix

```{r}
pairs(body.fat[,-4])
```

## Variance inflation factors

\small
```{r}
library(car)
vif_bodyfat <- vif(model_bf)
as.data.frame(vif_bodyfat)
```
\normalsize

- VIF on average 460.
- Large VIF for midarm circumference, although weakly correlated with other predictors.
- **How to correct for multicollinearity?**
  - Centering variables only valid option when higher order terms are in play.
  - Combine predictors, e.g., through principal component regression.
  - Ridge regression: allow some bias in exchange for increased precision and lower risk of overfitting.

## Multicollinearity and confounding

- A lot of textbooks advise to remove predictors from model in case of multicollinearity
- However, multicollinearity can also indicate strong confounding!
