{
  "hash": "440bbe3140943dd8ed2fcadb80d63dac",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 1: Linear regression\"\nsubtitle: \"Predictivity and variability\"\nauthor: \"Joris Vankerschaver\"\nheader-includes:\n  - \\useinnertheme[shadow=true]{rounded}\n  - \\usecolortheme{rose}\n  - \\setbeamertemplate{footline}[frame number]\n  - \\usepackage{color}\n  - \\usepackage{graphicx}\n  - \\usepackage{amsmath}\n  - \\graphicspath{{./images/01-linear-regression}}\noutput: \n  beamer_presentation:\n    theme: \"default\"\n    keep_tex: true\n    includes:\n      in_header: columns.tex\n---\n\n\n\n\n# Prediction\n\n## Example\n\nUse model to predict length of larch based on mineral composition of needles.\n\n\\footnotesize\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n                     Estimate Std. Error    t value    Pr(>|t|)\n(Intercept)         160.66283  175.61424  0.9148622 0.370649894\nnitrogen            -76.49677   92.34000 -0.8284250 0.416746264\nphosphor          -1120.70470  711.42841 -1.5752881 0.130135986\npotassium           138.06170   41.29966  3.3429260 0.003084272\nnitrogen:phosphor   724.38231  353.05353  2.0517634 0.052870451\n```\n\n\n:::\n:::\n\n\\normalsize\n\n- Percentages: nitrogen = 1.9, phosphorus = 0.2, potassium = 0.7.\n- Predicted **average** length:\n\\begin{multline*}\n160.66-76.5\\times 1.9-1120.7\\times 0.2+138.06\\times 0.7 + \\\\\n724.38\\times 1.9\\times 0.2=163.1.\n\\end{multline*}\n\n## Accuracy of prediction\n\nTo determine the accuracy of a prediction, we need to take into account the \n\n- **variability** of the observations around the regression line\n- **precision** of the estimated regression line.\n\n## Estimating variability via the residual standard error\n\n\\alert{Residual standard error} (RSE):\n\n- CWD basal area: 1.01 on 13 degrees of freedom\n- Larches: 35.55 on 21 degrees of freedom.\n\nResidual standard deviation tells that 95\\% of lengths, given nitrogen, phosphorus and potassium percentages of 1.9, 0.2 and 0.7, are expected to lie within a distance\n\\[\n  2\\times 35.55=71.1\n\\]\nof the mean.\n\n## Residual standard error in \\texttt{R}\n\n\\scriptsize\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model_l8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = length ~ nitrogen * phosphor + potassium)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-54.051 -24.544   5.934  21.866  69.243 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)   \n(Intercept)         160.66     175.61   0.915  0.37065   \nnitrogen            -76.50      92.34  -0.828  0.41675   \nphosphor          -1120.70     711.43  -1.575  0.13014   \npotassium           138.06      41.30   3.343  0.00308 **\nnitrogen:phosphor   724.38     353.05   2.052  0.05287 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 35.55 on 21 degrees of freedom\nMultiple R-squared:  0.8836,\tAdjusted R-squared:  0.8614 \nF-statistic: 39.85 on 4 and 21 DF,  p-value: 1.603e-09\n```\n\n\n:::\n:::\n\n\\normalsize\n\n## Residual standard error (by hand)\n\nRSE can be calculated as\n\\[\n  RSE = \\sqrt{\\frac{SSE}{n - p}} = \\sqrt{MSE}\n\\]\nwith SSE, the sum-squared of the residuals, given by\n\\[\n  SSE = \\sum_{i = 1}^n(y_i - \\hat{y}_i)^2 = \\sum_{i = 1}^n e_i^2.\n\\]\nand $p$ the number of parameters in the model.\n\nFor example (larches, $p = 5$):\n\n::: {.cell}\n\n```{.r .cell-code}\nSSE <- sum(model_l8$residuals^2)\nRSE <- sqrt(SSE/(26 - 5))\nRSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 35.54858\n```\n\n\n:::\n:::\n\n\n## Prediction/confidence intervals \n\n- **Prediction intervals** combine both inaccuracies\n  - Variability around the regression line\n  - Precision of the regression line.\n- Designed to contain, with 95\\% confidence, a random observation (e.g. CWD basal area or tree length) for given predictor values (e.g. tree density or given proportions of nitrogen, phosphorus, and potassium)\n\n- **Confidence intervals** incorporate only the precision of the regression line.\n- Designed to contain, with 95\\% confidence, the **average** of random observations for given predictor values.\n\n## Prediction intervals in \\texttt{R}: CWD basal area\n\n\\small\n\n::: {.cell}\n\n```{.r .cell-code}\np <- predict(model3, newdata = data.frame(RIP.DENS=800:2200), \n             interval = \"confidence\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n        fit        lwr      upr\n1 0.9474953 -0.1563700 2.051361\n2 0.9568141 -0.1433865 2.057015\n3 0.9661229 -0.1304244 2.062670\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np <- predict(model3, newdata = data.frame(RIP.DENS=800:2200), \n             interval = \"prediction\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n        fit       lwr      upr\n1 0.9474953 -1.497766 3.392757\n2 0.9568141 -1.486795 3.400423\n3 0.9661229 -1.475844 3.408090\n```\n\n\n:::\n:::\n\n\n## Prediction intervals in \\texttt{R}: Larches\n\n\\small\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata <- data.frame(nitrogen = 1.9, phosphor = 0.2, \n                      potassium = 0.7)\nnewdata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  nitrogen phosphor potassium\n1      1.9      0.2       0.7\n```\n\n\n:::\n\n```{.r .cell-code}\npredict.lm(model_l8, newdata, interval = \"confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 163.0865 140.6258 185.5472\n```\n\n\n:::\n\n```{.r .cell-code}\npredict.lm(model_l8, newdata, interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 163.0865 85.82246 240.3505\n```\n\n\n:::\n:::\n\n\n# Variability in regression models\n\n## Predictivity \n\nAnother way to gain insight in predictivity compares \n\\begin{itemize}\n\\item variability \\alert{around} regression line\n\\item with variability \\alert{on} the regression line, explained by the regression line\n\\end{itemize}\n\n## Total and residual variability\n\nIdea: compare variability of residuals and variability of (centered) predictions.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01_Regression-3_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=672 height=50%}\n:::\n:::\n\n\n\n## High predictivity: low variability around line \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01_Regression-3_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=672 height=50%}\n:::\n:::\n\n\n**High predictivity**: variability of residuals is much **lower** than variability of predictions.\n\n## Low predictivity: small variability on line\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01_Regression-3_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=672 height=50%}\n:::\n:::\n\n\n**Low predictivity**: variability of residuals is much **higher** than variability of predictions.\n\n## Sum of squares\n\n- Let $\\hat{y}_i$ be the prediction for observation $i$, then\n\\begin{align*}\nSST & = \\sum_{i=1}^n (y_i-\\bar y)^2 \\\\\n  & = \\sum_{i=1}^n(\\hat{y}_i-\\bar y)^2+ \\sum_{i=1}^n (y_i-\\hat{y}_i)^2\\\\\n  & = \\sum_{i=1}^n (\\hat{y}_i-\\bar y)^2+ \\sum_{i=1}^n e_i^2\\\\\n  & = SSR + SSE.\n\\end{align*}\n- Total sum of squares (SST) =  Regression sum of squares (SSR) +\nResidual sum of squares (SSE).\n- Total variability = Variability captured by regression + Variability in residuals.\n\n## Multiple correlation coefficient\n\n- **Multiple correlation coefficient** or coefficient of determination:\n\\[\n  R^2 = \\frac{SSR}{SST}.\n\\] \n- Expresses the proportion of variability in data is captured by their association with explanatory variable.\n- Measure for \\alert{predictive value} of explanatory variable.\n- Always between 0 and 1.\n- Simple linear regression: the square of the correlation between $X$ and $Y$.\n\n## Multiple correlation coefficient\n\nLook at the R `summary` output:\n\n- CWD basal area:\n\n\n```{.default}\nMultiple R-squared:  0.7159,\tAdjusted R-squared:  0.6722 \n```\n\n71.59\\% of variability on CWD basal area is explained by tree density.\n\n- Larches: \n\n\n```{.default}\nMultiple R-squared:  0.8836,\tAdjusted R-squared:  0.8614 \n```\n\n88.36\\% of variability on tree length is explained by mineral composition of needles.\n\n\\alert{Be careful!} High $R^2$ only demanded for prediction, not to estimate effect of $X$ on $Y$\n\n## Aside: adjusted multiple correlation coefficient\n\n- $R^2$ always increases (gets closer to 1) when model becomes more complex\n- To \"punish\" complexity, use adjusted $R^2$:\n\\[\n  R^2_\\mathrm{adj} = 1 - \\frac{n-1}{n - p}(1 - R^2).\n\\]\n- Adjusted $R^2$ is always lower than $R^2$.\n- Interpretation not so straightforward, used mainly for **model comparison**.\n\nLarches: $n = 26$, $p = 5$, $R^2 = 0.8836$, so\n\\[\n  R^2_\\mathrm{adj} = 1 - \\frac{25}{21}(1 - 0.8836) = 0.8614.\n\\]\n\n# Comparing simple vs. complex models\n\n## Example\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01_Regression-3_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=672 height=50%}\n:::\n:::\n\n\nCompare linear model (line) with model that just predicts the mean (dashed)\n\n- Left: linear model barely better than mean value.\n- Right: linear model **obviously better** than mean value.\n\n## Nested models\n\nNested models:\n\n- Complex model: with many predictors.\n- Simple model: like complex, but some predictors have been removed.\n\nExample (larches):\n\n- Complex: $E(Y|X) = \\alpha + \\beta_N X_N+ \\beta_P X_P + \\beta_K X_K +\\beta_r X_r$\n- Simple: $E(Y|X) = \\alpha + \\beta_P X_P$\n\nHow do we quantify which model is better?\n\n- Single regression: hypothesis test for $\\beta$.\n- Multiple regression: need to compare effect of **all coefficients at once**. \n\n## Intuition: comparing variance\n\nIdea: compare residual variability (SSE) to assess model fit.\n\n- $SSE_\\mathrm{complex}$ *always* lower than $SSE_\\mathrm{simple}$.\n- If it is *much* lower, decide that complex model is better.\n\nFormalized via $F$-test:\n\n- Null hypothesis: simple and complex model fit data equally well.\n- Alternative hypothesis: complex model is better.\n- Test statistic:\n$$\n  f = \\frac{\\frac{SSE_\\mathrm{simple}- SSE_\\mathrm{complex}}{p_\\mathrm{complex} - p_\\mathrm{simple}}}{\\frac{SSE_\\mathrm{complex}}{n - p_\\mathrm{complex} }}\n  \\sim F_{p_\\mathrm{complex} - p_\\mathrm{simple}, n - p_\\mathrm{complex}}.\n$$\n\n## Example: larches\n\nResidual sum of squares:\n\n- $SSE_\\mathrm{simple} = 91404.49$\n- $SSE_\\mathrm{complex} = 30121.92$\n\nNumber of parameters:\n\n- $p_\\mathrm{simple} = 2$\n- $p_\\mathrm{complex} = 5$\n\nHypothesis test:\n\n- Test statistic: $f = 14.24139$\n- $p$-value: $p = 0.00002744$.\n\nConclusion: complex model is significantly better.\n\n## Example in \\texttt{R}: larches\n\n\\footnotesize\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_l1 <- lm(length ~ phosphor)\nmodel_l2 <- lm(length ~ nitrogen + phosphor + potassium + residu)\nanova(model_l1, model_l2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: length ~ phosphor\nModel 2: length ~ nitrogen + phosphor + potassium + residu\n  Res.Df   RSS Df Sum of Sq      F    Pr(>F)    \n1     24 91404                                  \n2     21 30122  3     61283 14.241 2.744e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\\normalsize\n\n## \\texttt{R} summary command\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model_l8)\n```\n:::\n\n\n\\footnotesize\n\\begin{verbatim}\n(...)\nResidual standard error: 35.55 on 21 degrees of freedom\nMultiple R-squared:  0.8836,\tAdjusted R-squared:  0.8614 \nF-statistic: 39.85 on 4 and 21 DF,  p-value: 1.603e-09\n\\end{verbatim}\n\\normalsize\n\nLast line:\n\n- $F$-statistic: compares model to model with intercept only.\n- **\"Is my complex model capturing something meaningful?\"**",
    "supporting": [
      "01_Regression-3_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}