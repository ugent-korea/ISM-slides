{
  "hash": "ae1e37aef8fef3617254af29d7e83233",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to Statistical Modeling\"\nsubtitle: \"Simple Linear Regression\"\nauthor: \"Joris Vankerschaver\"\npdf-engine: lualatex\nformat:\n  beamer:\n    theme: Pittsburgh\n    colortheme: default\n    fonttheme: default\n    include-in-header:\n      - file: header.tex\n---\n\n\n\n\n# Introduction\n\n## Regression\n\nGoal: describe the relationship between 2 series of observations $(X_i,Y_i)$, obtained for individual subjects $i=1,...,n$\n\nExample:\n\nBasal area of coarse woody debris (CWD) versus tree density along 16 North American lakes\n\n- **Dependent variable, outcome, response** $Y$: CWD basal area\n- **Independent variable, explanatory variable, predictor** $X$: tree density (in number per km)\n\n![](./images/01-linear-regression/riparian-small.png){width=\"33%\" fig-align=\"center\"}\n\n## CWD versus tree density\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-2-1.pdf){fig-align='center' width=4in height=3in}\n:::\n:::\n\n\n\n## Regression\n\nFor fixed $X$, $Y$ will be some function of $X$ plus random noise: **observation = signal + noise**\n\nMathematical modelling of observation:\n$$\n  Y_i=f(X_i)+\\epsilon_i\n$$\nwhere $f(x)$ is the expected outcome for subjects with $X_i=x$\n$$\n  E(Y_i|X_i=x) = f(x)\n$$ \nand $\\epsilon_i$ is on average 0 for subjects with same $X_i$:\n$$\n  E(\\epsilon_i|X_i) = 0.\n$$\n\n\n## Linear regression\n\n- To obtain accurate and interpretable results, $f(X)$ is often chosen as linear function of unknown parameters\n- Use **linear regression model**\n$$\n  E(Y|X=x)=\\alpha + \\beta x\n$$\nwith unknown **intercept** $\\alpha$ and **slope** $\\beta$.\n- Linear regression model makes assumption on distribution of $X$ and $Y$, so can be incorrect.\n\n\n## Use of linear regression\n\n- **Prediction**: when $Y$ unknown, but $X$ known, we can predict $Y$ based on $X$:\n$$\n  E(Y|X=x)=\\alpha + \\beta x.\n$$\n- **Association**: describe biological relation between variable $X$ and continuous measurement $Y$\n  - Slope $\\beta$: difference in mean outcome between subjects that differ 1 unit in the value of $X$:\n  \\begin{align*}\n    E(Y|X=x+\\delta) - E(Y|X=x) & = \\alpha + \\beta (x+\\delta)      \n                                    -\\alpha-\\beta x\\\\\n      & = \\beta\\delta.\n  \\end{align*}\n\n\n## Least squares estimates\n\n- Least squares (regression) line: line that `best' fits data.\n- Found by choosing values for $\\alpha$ and $\\beta$ that minimize sum of squares of **residuals**: \n$$\n  \\sum_{i=1}^n (\\underbrace{Y_i-\\alpha-\\beta X_i}_{\\text{Residual}})^2\n$$\n- Estimates for $\\beta$ and $\\alpha$:\n$$\n  \\hat{\\beta} = \\mathrm{Cor}(x,y) \\frac{S_y}{S_x} \\quad \\text{and} \\quad \\hat{\\alpha}=\\bar Y - \\hat{\\beta} \\bar X.\n$$\nwith $\\mathrm{Cor}(x, y)$ the sample correlation between $x$ and $y$ and $S_x$, $S_y$ the sample standard deviation.\n\n## Residuals plot\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-3-1.pdf){fig-align='center' width=4in height=3in}\n:::\n:::\n\n\nSee also: [residuals animation](https://yihui.org/animation/example/least-squares/).\n\n## Output linear regression (coefficients only)\n\n\\footnotesize\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(CWD.BASA ~ RIP.DENS)\nsummary(model)$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) -77.0990778 30.60800907 -2.518918 0.0245520121\nRIP.DENS      0.1155161  0.02343233  4.929772 0.0002216405\n```\n\n\n:::\n:::\n\n\\normalsize\n\nRegression line:\n$$\n  E(Y|X=x)=-77.10+0.12 x\n$$\n\n## Output linear regression (full)\n\n\\footnotesize\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = CWD.BASA ~ RIP.DENS)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-38.62 -22.41 -13.33  26.16  61.35 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -77.09908   30.60801  -2.519 0.024552 *  \nRIP.DENS      0.11552    0.02343   4.930 0.000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 36.32 on 14 degrees of freedom\nMultiple R-squared:  0.6345,\tAdjusted R-squared:  0.6084 \nF-statistic:  24.3 on 1 and 14 DF,  p-value: 0.0002216\n```\n\n\n:::\n:::\n\n\\normalsize\n\n## Interpreting linear regression\n\n- Model: $E(Y|X=x)=-77.10+0.12 x$\n- Expected CWD basal area is 1.2 m$^2$ larger alongside lakes with 10 more trees per km\n- Expected CWD basal area alongside lakes with 1,600 trees per km shoreline: \n$$\n  -77.10 +0.12\\times 1600=108 \\ {\\rm m}^2\n$$\n- Expected CWD basal area alongside lakes with 500 trees per km shoreline:\n$$\n  -77.10 +0.12\\times 500=-17 \\ {\\rm m}^2\n$$\n- **Be careful with extrapolation!** (linearity assumption can only be verified within range of data)\n\n# Assumptions for linear regression\n\n## Verifying linearity assumption\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-6-1.pdf){fig-align='center' width=4in height=3in}\n:::\n:::\n\n\n## Verifying linearity assumption\n\n- An alternative (more convenient when there are multiple predictors) is a **residual plot**.\n- Note: residuals are prediction errors:\n$$\n  e_i = y_i-\\hat{\\alpha}-\\hat{\\beta}x_i\n$$\n- If linear model correct, then scatterplot of $e_i$ versus $x_i$ or predictions $\\hat{\\alpha}+\\hat{\\beta}x_i$ shows no pattern\n\n\n::: {.cell}\n\n:::\n\n\n## Verifying linearity assumption\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-8-1.pdf){fig-align='center'}\n:::\n:::\n\n\n## Inference for simple linear regression\n\nTo be able to draw conclusions about the linear regression model\n$$\n  E(Y|X)=\\alpha+\\beta X\n$$\nwe need extra assumptions:\n\n- **Homoscedasticity**: for fixed $X$, $Y$ has constant variance\n$$\n  \\text{Var}(Y|X)=\\sigma^2,\n$$\nestimated by the residual mean square error:\n$$\n\\text{MSE}=\\sum_{i=1}^n e_i^2/(n-2)\n$$\n- **Normality**: for fixed $X$, $Y$ is normally distributed\n$$\n  Y|X \\sim N(\\alpha+\\beta X, \\sigma^2)\n$$\n\n\n## Homoscedasticity versus heteroscedasticity\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-9-1.pdf){fig-align='center' width=4in height=4in}\n:::\n:::\n\n\n## Homoscedasticity?\n\nHard to check on regression plot directly!\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-10-1.pdf){fig-align='center' width=4in height=3in}\n:::\n:::\n\n\n## Assumption of homoscedasticity\n\n- Squared residuals carry information on residual variability.\n- If these are associated with explanatory variable, then indication of **heteroscedasticity**.\n- Scatterplot of $e_i^2$ or $\\sqrt{|e_i|}$ versus $x_i$ or predictions.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-11-1.pdf){fig-align='center' width=4in height=3in}\n:::\n:::\n\n\n## Assumption of homoscedasticity\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-12-1.pdf){fig-align='center'}\n:::\n:::\n\n\n\n## Normality assumption\n\n- Assumption: outcomes normally distributed **for fixed values of explanatory variable**:\n$$\n  Y|X \\sim N(a + bX, \\sigma^2).\n$$\n- Can be checked using QQ-plot of the residuals.\n\n\n## Normality assumption valid\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-13-1.pdf){fig-align='center' width=4in height=4in}\n:::\n:::\n\n\n\n## Normality assumption not valid\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-14-1.pdf){fig-align='center' width=4in height=4in}\n:::\n:::\n\n\n\n## QQ plot of residuals (Y|X normal)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-15-1.pdf){fig-align='center' width=4in height=4in}\n:::\n:::\n\n\n\n## QQ plot of residuals (Y|X not normal)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-16-1.pdf){fig-align='center' width=4in height=4in}\n:::\n:::\n\n\n\n## Do not use QQ-plot of Y!\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-17-1.pdf){fig-align='center' width=4in height=4in}\n:::\n:::\n\n\n\n## Checking for normality with R diagnostic plots\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-18-1.pdf){fig-align='center'}\n:::\n:::\n\n\n## What if homoscedasticity or normality false?\n\n- Transformation of **dependent variable** can help to obtain normality and homoscedasticity.\n- Example transformations: $\\sqrt{Y}, Y^2, 1/Y, \\exp{Y}, \\exp{(-Y)}, \\ln{Y}$.\n\n- Transformation of **independent variable** does not change distribution of $Y$ for given $X$:\n  - does not help in obtaining normality or homoscedasticity.\n  - does help to obtain linearity if normality and homoscedasticity are ok.\n\n\n## What if homoscedasticity or normality false?\n\n- Often because  outcome can only take on values in certain interval (e.g. $[0, 1]$, positive numbers, ...)\n- **Solution**: transform outcome such that it can take on all real values\n- Example: `CWD.BASA` is always positive: take $\\ln$ to make outcome real-valued:\n\n::: {.cell}\n\n:::\n\n\n## Transforming the outcome\n\n\\footnotesize\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(log(CWD.BASA) ~ RIP.DENS)\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(CWD.BASA) ~ RIP.DENS)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.23086 -0.78379  0.04559  0.72335  2.05022 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -0.5570100  1.0739690  -0.519   0.6121   \nRIP.DENS     0.0031573  0.0008222   3.840   0.0018 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.274 on 14 degrees of freedom\nMultiple R-squared:  0.513,\tAdjusted R-squared:  0.4782 \nF-statistic: 14.75 on 1 and 14 DF,  p-value: 0.001802\n```\n\n\n:::\n:::\n\n\n## Residual plots\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-21-1.pdf){fig-align='center'}\n:::\n:::\n\n\n# Higher-order regression\n\n## What if linearity assumption is false?\n\n- Transformation of dependent variable\n- Transformation of independent variable\n- If residuals reveal **quadratic association**, such that\n$$\n  e_i\\approx \\delta_0+\\delta_1 x_i+\\delta_2 x_i^2\n$$\nthen\n$$\n  y_i=\\hat{\\alpha}+\\hat{\\beta}x_i+e_i\\approx\n(\\hat{\\alpha}+\\delta_0)+(\\hat{\\beta}+\\delta_1)x_i+\\delta_2\nx_i^2\n$$ \n\n\n## Quadratic regression\n\n- We assume\n$$\n  E(Y|X)=\\alpha+\\beta X+\\gamma X^2\n$$\n- Unknown parameters estimated by **least squares method**: minimize\n$$\n  \\sum_{i=1}^n (Y_i-\\alpha-\\beta X_i-\\gamma X_i^2)^2\n$$\n\n\n## Quadratic regression\n\n\\footnotesize\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(CWD.BASA) ~ RIP.DENS + I(RIP.DENS^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6872 -0.4462 -0.1621  0.4214  2.1399 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   -9.686e+00  3.114e+00  -3.110  0.00828 **\nRIP.DENS       1.726e-02  4.673e-03   3.693  0.00270 **\nI(RIP.DENS^2) -4.960e-06  1.628e-06  -3.047  0.00935 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.01 on 13 degrees of freedom\nMultiple R-squared:  0.7159,\tAdjusted R-squared:  0.6722 \nF-statistic: 16.38 on 2 and 13 DF,  p-value: 0.0002801\n```\n\n\n:::\n:::\n\n\n## Residual plots\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-23-1.pdf){fig-align='center'}\n:::\n:::\n\n\n## Building model proceeds hierarchically\n\n\\small\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Estimate   Std. Error   t value    Pr(>|t|)\n(Intercept)   -9.686141e+00 3.114228e+00 -3.110287 0.008281158\nRIP.DENS       1.726034e-02 4.673452e-03  3.693275 0.002704501\nI(RIP.DENS^2) -4.960374e-06 1.627703e-06 -3.047469 0.009345380\n```\n\n\n:::\n:::\n\n\\normalsize\n\n- Add terms to model and keep those as long as they are significantly associated with outcome\n- Example: adding third order term is not significant contribution (p-value 0.26)\n- Adding proceeds **hierarchically**: lower order terms are kept as long as higher order terms are in model\n\n## Results\n\n- We conclude \n$$\n  E\\{\\ln(Y)|X\\}=-9.69+0.017X-4.96 \\ 10^{-6}X^2\n$$\nor equivalently that geometric mean CWD basal area for given tree density $X$ is equal to\n$$\n  \\exp(-9.69+0.017X-4.96 \\ 10^{-6}X^2)\n$$\n- For $X=500$ we now find 0.086 m$^2$ (previously: -17 m$^2$)\n- **How precise is this?**\n\n\n# Interpreting the results of a regression model\n\n## Inference for mean outcome \n\nGiven an input $X = x_h$, what do we expect the outcome $Y$ to be on average?\n\nUse the regression equation:\n\n- $\\hat{y}_h = \\hat{\\alpha} + \\hat{\\beta} x_h$ is unbiased estimator of $E(Y|X = x_h) = \\alpha+\\beta x_h$.\n\nWhat is the uncertainty of this estimator?\n\nFor this we need the standard error of $\\hat{Y}_h$.\n\n## Inference for mean outcome: uncertainty\n\n- Standard error of $\\hat{Y}_h$ is\n$$\n  SE(\\hat{Y}_h)=\\sqrt{MSE\\left\\{\\frac{1}{n}+\\frac{(X_h-\\bar X)^2}{\\sum_i (X_i-\\bar X)^2}\\right\\}}.\n$$\n- Tests and CI for $E(Y|X_h)$ based on\n$$\n  \\frac{\\hat{Y}_h-E(Y|X_h)}{SE(\\hat{Y}_h)}\\sim t_{n-p}\n$$\nwith $p$ number of unknown parameters in model.\n\n## Inference for mean outcome: intuition\n\n- Highest precision for predictions in $X_h=\\bar X$: relative confidence in predictions for $X$ **close to mean**.\n- Lower precision as predictions have $X$ further **away from the mean**.\n\n\n\n## Prediction in R\n\n\\small\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(I(log(CWD.BASA)) ~ RIP.DENS + I(RIP.DENS^2))\np <- predict(model3,\n             newdata = data.frame(RIP.DENS=c(1000, 1500)), \n             interval = \"confidence\")\np\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 2.613829 1.966541 3.261118\n2 5.043534 4.149293 5.937775\n```\n\n\n:::\n:::\n\n\n## Prediction in R\n\nPredictions and lower/upper bound of the CI are for `log(CWD.BASA)` and need to be transformed back:\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        fit       lwr       upr\n1  13.65123  7.145917  26.07867\n2 155.01687 63.389137 379.09068\n```\n\n\n:::\n:::\n\n\n## Expected outcome with 95\\% CI\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-27-1.pdf){fig-align='center' width=4in height=3in}\n:::\n:::\n\n\n\n## Inference for slope $\\beta$\n\n\n- The regression coefficient $\\hat{\\beta}$ is an (unbiased) estimator of $\\beta$, the population regression coefficient.\n- It comes with a measure of uncertainty: standard error of $\\hat{\\beta}$:\n$$\nSE(\\hat{\\beta})=\\sqrt{\\frac{MSE}{\\sum_i (X_i-\\bar X)^2}}.\n$$\nwith $MSE=\\frac{1}{n-2}\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2$\n- Large spread on $X$ improves precision.\n\n## Spread and precision\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01b-regression-simple-regression_files/figure-beamer/unnamed-chunk-28-1.pdf){fig-align='center' width=4in height=4in}\n:::\n:::\n\n\n\n## Association tree density vs.\\ CWD\n\nTests and confidence intervals for $\\beta$ are based on\n$$\n  \\frac{\\hat{\\beta}-\\beta}{SE(\\hat{\\beta})}\\sim t_{n-2}\n$$\n\n\n\n\\small\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) -77.0990778 30.60800907 -2.518918 0.0245520121\nRIP.DENS      0.1155161  0.02343233  4.929772 0.0002216405\n```\n\n\n:::\n:::\n\n\n## Association tree density vs.\\ CWD\n\n- 95\\% CI for $\\beta$ needs $t_{14,0.975} = 2.14$\n- CI is given by\n$$\n  [0.116 - 2.14\\times 0.0234,0.116 + 2.14\\times 0.0234]=[0.066,0.166]\n$$\n\n\\vfill\n\n\\small\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    2.5 %      97.5 %\n(Intercept) -142.74672817 -11.4514274\nRIP.DENS       0.06525871   0.1657734\n```\n\n\n:::\n:::\n",
    "supporting": [
      "01b-regression-simple-regression_files/figure-beamer"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}