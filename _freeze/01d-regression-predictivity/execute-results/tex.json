{
  "hash": "a169c788352eeb6289e2c56992933778",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to Statistical Modeling\"\nsubtitle: \"Predictivity and variability\"\nauthor: \"Joris Vankerschaver\"\npdf-engine: lualatex\nformat:\n  beamer:\n    theme: Pittsburgh\n    colortheme: default\n    fonttheme: default\n    include-in-header:\n      - file: header.tex\n---\n\n\n\n\n# Prediction\n\n## Example\n\nUse model to predict length of larch based on mineral composition of needles.\n\n\\footnotesize\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n                     Estimate Std. Error    t value    Pr(>|t|)\n(Intercept)         160.66283  175.61424  0.9148622 0.370649894\nnitrogen            -76.49677   92.34000 -0.8284250 0.416746264\nphosphor          -1120.70470  711.42841 -1.5752881 0.130135986\npotassium           138.06170   41.29966  3.3429260 0.003084272\nnitrogen:phosphor   724.38231  353.05353  2.0517634 0.052870451\n```\n\n\n:::\n:::\n\n\\normalsize\n\n- Percentages: nitrogen = 1.9, phosphorus = 0.2, potassium = 0.7.\n- Predicted **average** length:\n\\begin{multline*}\n160.66-76.5\\times 1.9-1120.7\\times 0.2+138.06\\times 0.7 + \\\\\n724.38\\times 1.9\\times 0.2=163.1.\n\\end{multline*}\n\n## Accuracy of prediction\n\nTo determine the accuracy of a prediction, we need to take into account the \n\n- **variability** of the observations around the regression line\n- **precision** of the estimated regression line.\n\n## Estimating variability via the residual standard error\n\n\\alert{Residual standard error} (RSE):\n\n- CWD basal area: 1.01 on 13 degrees of freedom\n- Larches: 35.55 on 21 degrees of freedom.\n\nResidual standard deviation tells that 95\\% of lengths, given nitrogen, phosphorus and potassium percentages of 1.9, 0.2 and 0.7, are expected to lie within a distance\n$$\n  2\\times 35.55=71.1\n$$\nof the mean.\n\n## Residual standard error in \\texttt{R}\n\n\\scriptsize\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = length ~ nitrogen * phosphor + potassium)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-54.051 -24.544   5.934  21.866  69.243 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)   \n(Intercept)         160.66     175.61   0.915  0.37065   \nnitrogen            -76.50      92.34  -0.828  0.41675   \nphosphor          -1120.70     711.43  -1.575  0.13014   \npotassium           138.06      41.30   3.343  0.00308 **\nnitrogen:phosphor   724.38     353.05   2.052  0.05287 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 35.55 on 21 degrees of freedom\nMultiple R-squared:  0.8836,\tAdjusted R-squared:  0.8614 \nF-statistic: 39.85 on 4 and 21 DF,  p-value: 1.603e-09\n```\n\n\n:::\n:::\n\n\\normalsize\n\n## Residual standard error (by hand)\n\nRSE can be calculated as\n$$\n  RSE = \\sqrt{\\frac{SSE}{n - p}} = \\sqrt{MSE}\n$$\nwith SSE, the sum-squared of the residuals, given by\n$$\n  SSE = \\sum_{i = 1}^n(y_i - \\hat{y}_i)^2 = \\sum_{i = 1}^n e_i^2.\n$$\nand $p$ the number of parameters in the model.\n\nFor example (larches, $p = 5$):\n\n::: {.cell}\n\n```{.r .cell-code}\nSSE <- sum(model_l8$residuals^2)\nRSE <- sqrt(SSE/(26 - 5))\nRSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 35.54858\n```\n\n\n:::\n:::\n\n\n## Prediction/confidence intervals \n\n- **Prediction intervals** combine both inaccuracies\n  - Variability around the regression line\n  - Precision of the regression line.\n- Designed to contain, with 95\\% confidence, a random observation (e.g. CWD basal area or tree length) for given predictor values (e.g. tree density or given proportions of nitrogen, phosphorus, and potassium)\n\n- **Confidence intervals** incorporate only the precision of the regression line.\n- Designed to contain, with 95\\% confidence, the **average** of random observations for given predictor values.\n\n## Prediction intervals in \\texttt{R}: CWD basal area\n\n\\small\n\n::: {.cell}\n\n```{.r .cell-code}\np <- predict(model3, newdata = data.frame(RIP.DENS=800:2200), \n             interval = \"confidence\")\np[1:3,]  # print first 3 predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        fit        lwr      upr\n1 0.9474953 -0.1563700 2.051361\n2 0.9568141 -0.1433865 2.057015\n3 0.9661229 -0.1304244 2.062670\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np <- predict(model3, newdata = data.frame(RIP.DENS=800:2200), \n             interval = \"prediction\")\np[1:3,]  # print first 3 predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        fit       lwr      upr\n1 0.9474953 -1.497766 3.392757\n2 0.9568141 -1.486795 3.400423\n3 0.9661229 -1.475844 3.408090\n```\n\n\n:::\n:::\n\n\n\n## Prediction intervals in \\texttt{R}: Larches\n\n\\small\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata <- data.frame(nitrogen = 1.9, phosphor = 0.2, \n                      potassium = 0.7)\nnewdata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  nitrogen phosphor potassium\n1      1.9      0.2       0.7\n```\n\n\n:::\n\n```{.r .cell-code}\npredict.lm(model_l8, newdata, interval = \"confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 163.0865 140.6258 185.5472\n```\n\n\n:::\n\n```{.r .cell-code}\npredict.lm(model_l8, newdata, interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 163.0865 85.82246 240.3505\n```\n\n\n:::\n:::\n\n\n# Variability in regression models\n\n## Predictivity \n\nAnother way to gain insight in predictivity compares \n\n- variability **around** regression line\n- with variability **on** the regression line, explained by the regression line.\n\n## Total and residual variability\n\nIdea: compare variability of residuals and variability of (centered) predictions.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01d-regression-predictivity_files/figure-beamer/unnamed-chunk-8-1.pdf){fig-align='center' width=4.5in height=4.5in}\n:::\n:::\n\n\n\n## High predictivity: low variability around line \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01d-regression-predictivity_files/figure-beamer/unnamed-chunk-9-1.pdf){fig-align='center' width=4.5in height=4.5in}\n:::\n:::\n\n\n**High predictivity**: variability of residuals is much **lower** than variability of predictions.\n\n## Low predictivity: small variability on line\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01d-regression-predictivity_files/figure-beamer/unnamed-chunk-10-1.pdf){fig-align='center' width=4.5in height=4.5in}\n:::\n:::\n\n\n**Low predictivity**: variability of residuals is much **higher** than variability of predictions.\n\n## Sum of squares\n\n- Let $\\hat{y}_i$ be the prediction for observation $i$, then\n\\begin{align*}\nSST & = \\sum_{i=1}^n (y_i-\\bar y)^2 \\\\\n  & = \\sum_{i=1}^n(\\hat{y}_i-\\bar y)^2+ \\sum_{i=1}^n (y_i-\\hat{y}_i)^2\\\\\n  & = \\sum_{i=1}^n (\\hat{y}_i-\\bar y)^2+ \\sum_{i=1}^n e_i^2\\\\\n  & = SSR + SSE.\n\\end{align*}\n- Total sum of squares (SST) =  Regression sum of squares (SSR) +\nResidual sum of squares (SSE).\n- Total variability = Variability captured by regression + Variability in residuals.\n\n## Multiple correlation coefficient\n\n- **Multiple correlation coefficient** or coefficient of determination:\n$$\n  R^2 = \\frac{SSR}{SST}.\n$$ \n- Expresses the proportion of variability in data is captured by their association with explanatory variable.\n- Measure for **predictive value** of explanatory variable.\n- Always between 0 and 1.\n- Simple linear regression: the square of the correlation between $X$ and $Y$.\n\n## Multiple correlation coefficient\n\nLook at the R `summary` output:\n\n- CWD basal area:\n\n\n```{.default}\nMultiple R-squared:  0.7159,\tAdjusted R-squared:  0.6722 \n```\n\n71.59\\% of variability on CWD basal area is explained by tree density.\n\n- Larches: \n\n\n```{.default}\nMultiple R-squared:  0.8836,\tAdjusted R-squared:  0.8614 \n```\n\n88.36\\% of variability on tree length is explained by mineral composition of needles.\n\n**Note:** High $R^2$ only demanded for prediction, not to estimate effect of $X$ on $Y$\n\n## Aside: adjusted multiple correlation coefficient\n\n- $R^2$ always increases (gets closer to 1) when model becomes more complex\n- To \"punish\" complexity, use adjusted $R^2$:\n$$\n  R^2_\\mathrm{adj} = 1 - \\frac{n-1}{n - p}(1 - R^2).\n$$\n- Adjusted $R^2$ is always lower than $R^2$.\n- Interpretation not so straightforward, used mainly for **model comparison**.\n\nLarches: $n = 26$, $p = 5$, $R^2 = 0.8836$, so\n$$\n  R^2_\\mathrm{adj} = 1 - \\frac{25}{21}(1 - 0.8836) = 0.8614.\n$$\n\n# Comparing simple vs. complex models\n\n## Example\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01d-regression-predictivity_files/figure-beamer/unnamed-chunk-13-1.pdf){fig-align='center' width=4in height=4in}\n:::\n:::\n\n\nCompare linear model (line) with model that just predicts the mean (dashed)\n\n- Left: linear model barely better than mean value.\n- Right: linear model **obviously better** than mean value.\n\n## Nested models\n\nNested models:\n\n- Complex model: with many predictors.\n- Simple model: like complex, but some predictors have been removed.\n\nExample (larches):\n\n- Complex: $E(Y|X) = \\alpha + \\beta_N X_N+ \\beta_P X_P + \\beta_K X_K +\\beta_r X_r$\n- Simple: $E(Y|X) = \\alpha + \\beta_P X_P$\n\nHow do we quantify which model is better?\n\n- Single regression: hypothesis test for $\\beta$.\n- Multiple regression: need to compare effect of **all coefficients at once**. \n\n## Intuition: comparing variance\n\nIdea: compare residual variability (SSE) to assess model fit.\n\n- $SSE_\\mathrm{complex}$ *always* lower than $SSE_\\mathrm{simple}$.\n- If it is *much* lower, decide that complex model is better.\n\nFormalized via $F$-test:\n\n- Null hypothesis: simple and complex model fit data equally well.\n- Alternative hypothesis: complex model is better.\n- Test statistic:\n$$\n  f = \\frac{\\frac{SSE_\\mathrm{simple}- SSE_\\mathrm{complex}}{p_\\mathrm{complex} - p_\\mathrm{simple}}}{\\frac{SSE_\\mathrm{complex}}{n - p_\\mathrm{complex} }}\n  \\sim F_{p_\\mathrm{complex} - p_\\mathrm{simple}, n - p_\\mathrm{complex}}.\n$$\n\n## Example: larches\n\nResidual sum of squares:\n\n- $SSE_\\mathrm{simple} = 91404.49$\n- $SSE_\\mathrm{complex} = 30121.92$\n\nNumber of parameters:\n\n- $p_\\mathrm{simple} = 2$\n- $p_\\mathrm{complex} = 5$\n\nHypothesis test:\n\n- Test statistic: $f = 14.24139$\n- $p$-value: $p = 0.00002744$.\n\nConclusion: complex model is significantly better.\n\n## Example in \\texttt{R}: larches\n\n\\footnotesize\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_l1 <- lm(length ~ phosphor)\nmodel_l2 <- lm(length ~ nitrogen + phosphor + potassium + residu)\nanova(model_l1, model_l2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: length ~ phosphor\nModel 2: length ~ nitrogen + phosphor + potassium + residu\n  Res.Df   RSS Df Sum of Sq      F    Pr(>F)    \n1     24 91404                                  \n2     21 30122  3     61283 14.241 2.744e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\\normalsize\n\n## \\texttt{R} summary command\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model_l8)\n```\n:::\n\n\n\\footnotesize\n\\begin{verbatim}\n(...)\nResidual standard error: 35.55 on 21 degrees of freedom\nMultiple R-squared:  0.8836,\tAdjusted R-squared:  0.8614 \nF-statistic: 39.85 on 4 and 21 DF,  p-value: 1.603e-09\n\\end{verbatim}\n\\normalsize\n\nLast line:\n\n- $F$-statistic: compares model to model with intercept only.\n- **\"Is my complex model capturing something meaningful?\"**",
    "supporting": [
      "01d-regression-predictivity_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}