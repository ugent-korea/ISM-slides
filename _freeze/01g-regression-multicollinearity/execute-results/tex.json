{
  "hash": "a592c8d3bd52e7e2c1b62e289a3cfc83",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to Statistical Modeling\"\nsubtitle: \"Multicollinearity\"\nauthor: \"Joris Vankerschaver\"\npdf-engine: lualatex\nformat:\n  beamer:\n    theme: Pittsburgh\n    colortheme: default\n    fonttheme: default\n    include-in-header:\n      - file: header.tex\n---\n\n\n\n\n## Multicollinearity \n\n- There is **multicollinearity** when 2 or more predictors are correlated\n- **Can possibly cause problems**: if there is strong correlation between 2 predictors $X_1$ and $X_2$, it becomes difficult to discern effect of $X_1$ of effect of $X_2$\n\nExample: If $X_1=X_2$, then \n$$\n  E(Y|X_1,X_2)=\\beta_0+\\beta_1X_1+\\beta_2X_2=\\beta_0+(\\beta_1+\\beta_2)X_1\n$$\n\nConsequences\n\n- Numerically instable estimates\n- Estimates with large standard errors\n- Difficult interpretation of coefficients\n\n## Diagnosing multicollinearity\n\nMulticollinearity can be recognized through:\n\n- **Instability**:\n  - Large changes in coefficients after adding a predictor\n  - Very wide confidence intervals\n  - Unexpected results \n- **Strong correlation** between predictors:\n  - Example: usually strong correlation between $X_f$ and $X_fX_s$\n  - Can sometimes be eliminated by **centering** (subtracting the mean): \n$$\n  X \\to X-\\bar X.\n$$\n\n\n## Impact of centering\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01g-regression-multicollinearity_files/figure-beamer/unnamed-chunk-2-1.pdf){fig-align='center' width=4in height=3.5in}\n:::\n:::\n\n\n## Scatterplot matrix - before centering\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01g-regression-multicollinearity_files/figure-beamer/unnamed-chunk-3-1.pdf)\n:::\n:::\n\n\n## Scatterplot matrix - after centering\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01g-regression-multicollinearity_files/figure-beamer/unnamed-chunk-4-1.pdf)\n:::\n:::\n\n\n## Diagnosing multicollinearity\n\nPrevious diagnostics are limited:\n\n- Even if pairwise correlations between predictors $X_1,X_2,X_3$ low, there can be strong multicollinearity.\n- E.g., when strong correlation between $X_1$ and a linear combination of $X_2$ and $X_3$.\n\n**Variance inflation factor** for $k^{th}$ coefficient:\n$$\n  \\textrm{VIF}_k=\\left(1-R_k^2\\right)^{-1}\n$$\nwith $R_k^2$ the $R^2$ of linear regression of $k^{th}$ predictor on other predictors.\n\n## Interpretation VIF\n\n- $\\textrm{VIF}_k \\geq 1$; $\\textrm{VIF}_k=1$ if $k^{th}$ predictor not linearly associated with other predictors.\n- Expresses how much larger variance on $k^{th}$ coefficient is than when all predictors were independent.\n- Average quadratic distance between estimated and true coefficients is proportionate with average VIF.\n- Critical multicollinearity: maximum VIF of at least 10.\n\n## Variance inflation factors\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01g-regression-multicollinearity_files/figure-beamer/unnamed-chunk-5-1.pdf){fig-align='center' width=4in height=3.5in}\n:::\n:::\n\n\n## Simpler interpretation of coefficients\n\nCoefficients (without centering)\n\\footnotesize\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n                     Estimate Std. Error    t value    Pr(>|t|)\n(Intercept)         160.66283  175.61424  0.9148622 0.370649894\nnitrogen            -76.49677   92.34000 -0.8284250 0.416746264\nphosphor          -1120.70470  711.42841 -1.5752881 0.130135986\npotassium           138.06170   41.29966  3.3429260 0.003084272\nnitrogen:phosphor   724.38231  353.05353  2.0517634 0.052870451\n```\n\n\n:::\n:::\n\n\\normalsize\nCoefficients (with centering)\n\\footnotesize\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n                    Estimate Std. Error   t value     Pr(>|t|)\n(Intercept)         184.1200   9.244736 19.916194 4.079334e-15\ncnitrogen           105.0167  21.458692  4.893901 7.703187e-05\ncphosphor           252.5570 156.336392  1.615472 1.211339e-01\ncpotassium          138.0617  41.299658  3.342926 3.084272e-03\ncnitrogen:cphosphor 724.3823 353.053531  2.051763 5.287045e-02\n```\n\n\n:::\n:::\n\n\n## Example: Prediction body fat\n\n- Determining percentage body fat difficult and expensive\n- Study investigates association between\n  - $Y$: body fat\n  - $X_1$: triceps skinfold thickness\n  - $X_2$: thigh circumference\n  - $X_3$: midarm circumference\n- 20 healthy women between 25 and 34 years old\n\n## Analysis in R\n\n\\footnotesize\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = bodyfat ~ triceps.skinfold.thickness + thigh.circumference + \n    midarm.circumference)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7263 -1.6111  0.3923  1.4656  4.1277 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)\n(Intercept)                 117.085     99.782   1.173    0.258\ntriceps.skinfold.thickness    4.334      3.016   1.437    0.170\nthigh.circumference          -2.857      2.582  -1.106    0.285\nmidarm.circumference         -2.186      1.595  -1.370    0.190\n\nResidual standard error: 2.48 on 16 degrees of freedom\nMultiple R-squared:  0.8014,\tAdjusted R-squared:  0.7641 \nF-statistic: 21.52 on 3 and 16 DF,  p-value: 7.343e-06\n```\n\n\n:::\n:::\n\n\n## Scatterplot matrix\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01g-regression-multicollinearity_files/figure-beamer/unnamed-chunk-9-1.pdf)\n:::\n:::\n\n\n## Variance inflation factors\n\n\\small\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n                           vif_bodyfat\ntriceps.skinfold.thickness    708.8429\nthigh.circumference           564.3434\nmidarm.circumference          104.6060\n```\n\n\n:::\n:::\n\n\\normalsize\n\n- VIF on average 460.\n- Large VIF for midarm circumference, although weakly correlated with other predictors.\n- **How to correct for multicollinearity?**\n  - Centering variables only valid option when higher order terms are in play.\n  - Combine predictors, e.g., through principal component regression.\n  - Ridge regression: allow some bias in exchange for increased precision and lower risk of overfitting.\n\n## Multicollinearity and confounding\n\n- A lot of textbooks advise to remove predictors from model in case of multicollinearity\n- However, multicollinearity can also indicate strong confounding!\n",
    "supporting": [
      "01g-regression-multicollinearity_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}