{
  "hash": "40b885cf026451bbc79da9e502342f79",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Principal component analysis: theory and concepts\"\nsubtitle: Introduction to Statistical Modelling\nauthor: Prof. Joris Vankerschaver\nformat:\n  beamer:\n    theme: Pittsburgh\n    colortheme: default\n    fonttheme: default\n    header-includes: |\n      \\setbeamertemplate{frametitle}[default][left]\n      \\setbeamertemplate{footline}[frame number]\n\n---\n\n\n\n\n\n## Goal of dimensionality reduction\n\n- Pre-processing\n  - Remove collinear predictors (multicollinearity)\n- Computational efficiency\n  - Retain import features to speed up computational processing\n- Visualization\n\n## Learning outcomes\n\nAt the end of this lecture, you should be able to:\n\n1. Explain the ideas behind PCA\n2. Do a PCA by hand given a covariance matrix\n3. Do a PCA with R\n4. Interpret and explain the PCA results\n5. Build and explain a PCR model\n\n## References\n\n- *Introduction to statistical modeling*. Chapter available on Ufora.\n- *An Introduction to Statistical Learning*. Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Available for free online at https://www.statlearning.com/. \n  - PCA: section 6.3\n\n## Reminder: multicollinearity\n\nBodyfat dataset: 20 observations, predict amount of body fat from three body measurements.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n- Linear regression model:\n\\begin{align*}\n  \\texttt{bodyfat} & = 117.085 \\\\\n  & + 4.334 \\cdot \\texttt{triceps} \\\\\n  & - 2.857 \\cdot \\texttt{thigh} \\\\\n  & - 2.186 \\cdot \\texttt{midarm}\n\\end{align*}\n\n- **Do you see anything wrong with this?**\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-2-1.pdf)\n:::\n:::\n\n\n:::\n::::\n\n\n## \n\n\\scriptsize\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = bodyfat ~ ., data = bodyfat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7263 -1.6111  0.3923  1.4656  4.1277 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)\n(Intercept)                 117.085     99.782   1.173    0.258\ntriceps.skinfold.thickness    4.334      3.016   1.437    0.170\nthigh.circumference          -2.857      2.582  -1.106    0.285\nmidarm.circumference         -2.186      1.595  -1.370    0.190\n\nResidual standard error: 2.48 on 16 degrees of freedom\nMultiple R-squared:  0.8014,\tAdjusted R-squared:  0.7641 \nF-statistic: 21.52 on 3 and 16 DF,  p-value: 7.343e-06\n```\n\n\n:::\n:::\n\n\n# Principal component analysis\n\n## Directions of maximal variability\n\nIntuitively:\n\n- Find directions of maximal variability in the dataset\n- Discard directions in which there is neglible variability\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-4-1.pdf){fig-align='center'}\n:::\n:::\n\n\n\n## Directions of less variability\n\nSince `triceps` and `thigh` are highly correlated, specifying both is superfluous. What do we lose if we throw away one of these variables?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-5-1.pdf){fig-align='center' width=4in height=3in}\n:::\n:::\n\n\n## Notation\n\nDataset: \n\n- $N$ observations $\\mathbf{x}_k$, $k = 1, \\ldots, N$\n- Each observation is a (column) vector in $\\mathbb{R}^D$\n\nData matrix:\n$$\n  \\mathbf{X} = \\begin{bmatrix}\n    \\mathbf{x}_1^T \\\\\n    \\mathbf{x}_2^T \\\\\n    \\vdots \\\\\n    \\mathbf{x}_N^T\n  \\end{bmatrix} \\in \\mathbb{R}^{N \\times D}\n$$\n\nColumns of the data matrix:\n\n- Referred to as features, predictors, independent variables\n- Denoted by $\\mathbf{X}_i$, $i = 1, \\ldots, D$\n\n## Notation: example (body fat dataset)\n\n- 20 observations with 3 features each\n- Data matrix is $20 \\times 3$ matrix\n- Features:\n  - $\\mathbf{X}_1$: `triceps.skinfold.thickness`\n  - $\\mathbf{X}_2$: `thigh.circumference`\n  - $\\mathbf{X}_3$: `midarm.circumference`\n\n\n## The covariance matrix\n\nGiven observations $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\in \\mathbb{R}^D$, the variance-covariance matrix $\\mathbf{S}$ is defined as:\n$$\n  \\mathbf{S} = \\frac{1}{N} \\sum_{k = 1}^N \\left(\\mathbf{x}_k\\mathbf{x}_k^T - \\bar{\\mathbf{x}}\\bar{\\mathbf{x}}^T \\right).\n$$  \n\nStructure: variances and covariances between components of the data.\n$$\n  \\mathbf{S} = \\begin{bmatrix}\n    \\text{Var}(x_1) & \\text{Cov}(x_1, x_2) & \\cdots & \\text{Cov}(x_1, x_D) \\\\\n    \\text{Cov}(x_2, x_1) & \\text{Var}(x_2) & \\cdots & \\text{Cov}(x_2, x_D) \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\text{Cov}(x_D, x_1) & \\text{Cov}(x_D, x_2) & \\cdots & \\text{Var}(x_D)\n  \\end{bmatrix}\n$$\n\n## The covariance matrix: examples\n\n\n::: {.cell}\n\n:::\n\n\n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-7-1.pdf){width=1.5in height=1.5in}\n:::\n:::\n\n\n$$\n  \\mathbf{S} = \\begin{bmatrix} \n    6 & 0 \\\\\n    0 & 1 \n  \\end{bmatrix}\n$$\n\n:::\n\n::: {.column width=\"33%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-8-1.pdf){width=1.5in height=1.5in}\n:::\n:::\n\n$$\n  \\mathbf{S} = \\begin{bmatrix} \n    1 & 0 \\\\\n    0 & 1 \n  \\end{bmatrix}\n$$\n:::\n::: {.column width=\"33%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-9-1.pdf){width=1.5in height=1.5in}\n:::\n:::\n\n$$\n  \\mathbf{S} = \\begin{bmatrix} \n    5 & 2 \\\\\n    2 & 2 \n  \\end{bmatrix}\n$$\n\n:::\n\n\n::::\n\n\nClearly the covariance matrix will help us find directions of maximum variability, but how?\n\n## Linear combination of features\n\n- The **first principal component** $\\mathbf{Z}_1$ is a linear combination of the columns of $\\mathbf{X}$:\n$$\n  \\mathbf{Z}_1 = v_1 \\mathbf{X}_1 + \\cdots + v_D \\mathbf{X}_D,\n$$\nwhere we will choose the coefficients $v_i$ so that the variances is maximal, in some sense.\n\n- The coefficients $v_i$ are referred to as the **loadings** and the vector \n$$\n  \\mathbf{v} = \\begin{bmatrix} \n    v_1 \\\\\n    \\vdots \\\\\n    v_D\n  \\end{bmatrix}\n$$\nis the **loadings vector**.\n\n- Variance of $\\mathbf{Z}_1$:\n$$\n  \\text{Var}(\\mathbf{Z}_1) = \\mathbf{v}^T \\mathbf{S} \\mathbf{v}.\n$$\n\n## Maximizing the variance\n\n- Idea: choose loadings $\\mathbf{v}$ so that $\\text{Var}(\\mathbf{Z}_1)$ is **maximal**.\n- Problem: just by increasing the norm of $\\mathbf{v}$, variance can become as large as we want. Solution: impose that $\\left\\Vert \\mathbf{v} \\right\\Vert = 1$.\n\n\n::: {.callout-tip}\n## PC1: maximization of variance\n\nThe loadings vector $\\mathbf{v}$ for the first principal component is found by solving the following maximization problem:\n$$\n  \\text{maximize $\\mathbf{v}^T \\mathbf{S} \\mathbf{v}$ so that $\\mathbf{v}^T \\mathbf{v} = 1$.}\n$$\n:::\n\n## Geometric interpretation\n\n- $\\mathbf{Z}_1$: projection of data on the line in the direction of $\\mathbf{v}$.\n- Find direction $\\mathbf{v}$ so that variance is maximal (blue)\n\n![](./images/02-pca/max-variance-projection.svg){fig-align=center}\n\n## Eigenvalue problem\n\nThrough Lagrange multipliers, can show that maximizing variance is equivalent to finding eigenvalues and eigenvectors of $\\mathbf{S}$.\n\n::: {.callout-tip}\n## PC1: eigenvalues\n\nThe loadings vector $\\mathbf{v}$ for the first principal component is the eigenvector of $\\mathbf{S}$ with the largest eigenvalue:\n$$\n  \\mathbf{S} \\mathbf{v} = \\lambda \\mathbf{v}\n$$\n:::\n\nEigenvectors are typically quite efficient to compute.\n\n## Amount of variance explained\n\nTake the eigenvalue equation\n$$\n  \\mathbf{S} \\mathbf{v} = \\lambda \\mathbf{v},\n$$\nand left-multiply by $\\mathbf{v}^T$ to get\n$$\n  \\lambda = \\lambda \\mathbf{v}^T \\mathbf{v} = \\mathbf{v}^T \\mathbf{S}\\mathbf{v} = \\text{Var}(\\mathbf{Z}_1).\n$$\n\n::: {.callout-note}\n## Eigenvalues and eigenvectors\n- The largest eigenvalue of $\\mathbf{S}$ is equal to the variance contained in (\"explained by\") the first principal component $\\mathbf{Z}_1$.\n- The corresponding eigenvector gives the loadings vector $\\mathbf{v}$.\n:::\n\n## Example\n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-10-1.pdf){width=1.5in height=1.5in}\n:::\n:::\n\n:::\n\n::: {.column width=\"33%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-11-1.pdf){width=1.5in height=1.5in}\n:::\n:::\n\n\n:::\n::: {.column width=\"33%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-12-1.pdf){width=1.5in height=1.5in}\n:::\n:::\n\n:::\n::::\n\nThe loadings vectors may point in the opposite direction of what you expected... Why is this not a problem?\n\n\n## The remaining principal components\n\nNext principal components $\\mathbf{Z}_2, \\mathbf{Z}_3, \\ldots$ involve variation in the data after $\\mathbf{Z}_1$ has been taken into account. \n\n- For $\\mathbf{Z}_2$:\n  $$\n    \\text{maximize Var($\\mathbf{Z}_2$) so that Cov($\\mathbf{Z}_1$, $\\mathbf{Z}_2$) = 0}\n  $$\n- Equivalent to: find second largest eigenvalue $\\lambda_2$ and eigenvector $\\mathbf{v}_2$.\n\nSame story for remaining principal components.\n\n::: {.callout-note}\nThe principal components are *uncorrelated* linear combinations of features that *maximize variance*.\n:::\n\n\n## How many principal components are there?\n\nRecall:\n\n- $\\mathbf{S}$ is a symmetric $D \\times D$ matrix\n- Such a matrix always has $D$ eigenvalues and eigenvectors\n\nWhen $D \\le N$ (more data points than features)\n\n- In general, $D$ non-zero principal components\n\nWhen $D > N$:\n\n- $\\mathbf{S}$ has rank at most $N$: $N$ non-zero principal components\n- Can happen in high-dimensional datasets (e.g. gene assays)\n\n## Percentage of variance explained\n\n- Eigenvalue $\\lambda_i$ is amount of variance explained by PC $i$.\n- Total amount of variance: $\\lambda_1 + \\lambda_2 + \\cdots + \\lambda_D$\n- Percentage of variance explained by PC $i$:\n$$\n  \\frac{\\lambda_i}{\\lambda_1 + \\cdots + \\lambda_D}\n$$\n\nIn many cases, the first few PCs will explain the majority of variance (80% to 90%).\n\n**Dimensionality reduction:** we can omit the remaining principal components with only a small loss of information\n\n## Example\n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-13-1.pdf){width=1.5in height=1.5in}\n:::\n:::\n\n:::\n\n::: {.column width=\"33%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-14-1.pdf){width=1.5in height=1.5in}\n:::\n:::\n\n\n:::\n::: {.column width=\"33%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-15-1.pdf){width=1.5in height=1.5in}\n:::\n:::\n\n:::\n::::\n\nBlue: first PC, red: second PC.\n\n## Worked out example (by hand)\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\nDataset is chosen so that\n$$\n  \\mathbf{S} = \\begin{bmatrix}\n    5 & 2 \\\\\n    2 & 2\n  \\end{bmatrix}.\n$$\nEigenvalues:\n$$\n  \\lambda_1 = 6, \\quad \\lambda_2 = 1.\n$$\nEigenvectors:\n$$\n  \\mathbf{v}_1 = \\frac{1}{\\sqrt{5}} \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix},\n  \\quad\n  \\mathbf{v}_2 = \\frac{1}{\\sqrt{5}} \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}.\n$$\n:::\n\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-16-1.pdf){width=2in height=1.2in}\n:::\n:::\n\n:::\n::::\n\n## Worked out example (with R)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprcomp(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandard deviations (1, .., p=2):\n[1] 2.44949 1.00000\n\nRotation (n x k) = (2 x 2):\n        PC1        PC2\nX 0.8944272 -0.4472136\nY 0.4472136  0.8944272\n```\n\n\n:::\n:::\n\n\n\nNote:\n\n- Standard deviations are **square roots** of eigenvalues\n- Columns of rotation matrix give loadings vectors\n\n## Example: body fat dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca <- prcomp(bodyfat_predictors)\npca\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandard deviations (1, .., p=3):\n[1] 7.2046011 3.7432587 0.1330841\n\nRotation (n x k) = (3 x 3):\n                                 PC1        PC2        PC3\ntriceps.skinfold.thickness 0.6926671  0.1511979  0.7052315\nthigh.circumference        0.6985058 -0.3842734 -0.6036751\nmidarm.circumference       0.1797272  0.9107542 -0.3717862\n```\n\n\n:::\n:::\n\n\n## Percentage of variance explained\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2     PC3\nStandard deviation     7.2046 3.7433 0.13308\nProportion of Variance 0.7872 0.2125 0.00027\nCumulative Proportion  0.7872 0.9997 1.00000\n```\n\n\n:::\n:::\n\n- First 2 PCs explain over 99% of variance in data\n- Interpretation:\n\\begin{align*}\n  \\text{PC}_1 & = 0.693 \\cdot \\texttt{triceps} + 0.699 \\cdot \\texttt{thigh} + 0.179 \\cdot \\texttt{midarm} \\\\\n  \\text{PC}_2 & = 0.151 \\cdot \\texttt{triceps} - 0.384 \\cdot \\texttt{thigh} - 0.910 \\cdot \\texttt{midarm}\n\\end{align*}\n\n## Standardizing the features\n\nOften, data are standardized before running PCA:\n$$\n  \\mathbf{Y}_i = \\frac{\\mathbf{X}_i - \\bar{\\mathbf{X}}_i}{\\text{SD}(\\mathbf{X}_i)}\n$$\n\n- Standardization puts all features on the same scale and affects the outcome of your PCA.\n- Often a good idea when features have different units (e.g. mm, Watt, sec).\n- **Not** a good idea when features have the same units (e.g. pixel intensities in an image).\n\nIn R: `prcomp(df, center = TRUE, scale = TRUE)`.\n\n\n# Interpretation of PCA results\n\n## Score plot\n\n- Scatter plot of two PC (usually PC1 and PC2)\n- Can be used to spot patterns in data (see later)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-20-1.pdf){fig-align='center'}\n:::\n:::\n\n\n## Loadings plot\n\n- Shows how much each variable contributes to each PC\n- Useful to discern patterns in PCs\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-21-1.pdf){fig-align='center'}\n:::\n:::\n\n\n## Scree plot\n\n- Shows percentage of variance explained per PC\n- Useful to determine the PCs that contribute most to variance\n- Can be made with R's `screeplot` command, but better to make your own (it's just a line plot)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-22-1.pdf){fig-align='center'}\n:::\n:::\n\n\n## Selecting the number of principal components to retain\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\\vspace*{2cm}\n\n\nMany heuristics exist for selecting \"optimal\" number of PCs:\n\n- Explain fixed percentage (e.g. 80%) of variance\n- \"Elbow\" in scree plot\n- ...\n\nCan also determine number of PCs dynamically (e.g. if doing regression on PCs, look at $R^2$)\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](./images/02-pca/Yamnuska_bottom_cliff.jpg)\n\n\n\\scriptsize\nImage credit: https://en.wikipedia.org/wiki/Scree (Kevin Lenz, CC BY-SA 2.5)\n:::\n\n::::\n\n\n## Biplot\n\n- Biplot = loadings plot + score plot\n- Numbers: data for first two PC\n- Arrows: contribution of variables to first two PC\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbiplot(pc)\n```\n\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-23-1.pdf){fig-align='center' width=2in height=2in}\n:::\n:::\n\n\n\n# Principal component regression\n\n## Principal component regression (PCR)\n\nIdea:\n\n- Do a PCA (usually with standardized features)\n- Build a linear model on reduced number of PCs\n\nWhy do we do this?\n\n- PCs are uncorrelated, so takes care of multicollinearity\n- Results in a simpler model where only important features play a role\n\nNot always the right thing to do, alternatives exist (e.g. ridge regression, see later)\n\n## PCR by hand: starting with PC 1\n\n\\scriptsize\n\n::: {.cell}\n\n```{.r .cell-code}\npc1 <- pc$x[, \"PC1\"]\nmodel_1 <- lm(bodyfat$bodyfat ~ pc1)\nsummary(model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = bodyfat$bodyfat ~ pc1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1357 -1.8821  0.2682  1.7107  3.4992 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 20.19500    0.58688  34.411  < 2e-16 ***\npc1          0.61366    0.08358   7.343 8.13e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.625 on 18 degrees of freedom\nMultiple R-squared:  0.7497,\tAdjusted R-squared:  0.7358 \nF-statistic: 53.91 on 1 and 18 DF,  p-value: 8.128e-07\n```\n\n\n:::\n:::\n\n\n## PCR by hand: adding PC 2\n\n\\scriptsize\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = bodyfat$bodyfat ~ pc1 + pc2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9876 -1.8822  0.2562  1.3209  4.0285 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 20.19500    0.56604  35.678  < 2e-16 ***\npc1          0.61366    0.08061   7.613 7.12e-07 ***\npc2         -0.23785    0.15514  -1.533    0.144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.531 on 17 degrees of freedom\nMultiple R-squared:  0.7801,\tAdjusted R-squared:  0.7542 \nF-statistic: 30.15 on 2 and 17 DF,  p-value: 2.564e-06\n```\n\n\n:::\n:::\n\n\n## Aside: variance inflation factors\n\nReminder: principal components are uncorrelated by definition, so all the VIFs will be equal to 1.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-26-1.pdf)\n:::\n:::\n\n\n## PCR by hand: putting together the final model\n\n- We would probably select `model_1`:\n$$\n  \\texttt{bodyfat} =  20.195 + 0.614 \\cdot \\texttt{PC}_1\n$$\n- This model uses the principal components as predictors, but we typically want the original predictors. Recall\n$$\n  \\text{PC}_1 = 0.693 \\cdot \\texttt{triceps} + 0.699 \\cdot \\texttt{thigh} + 0.179 \\cdot \\texttt{midarm}\n$$\n- Putting these two together gives the final PCR model:\n\\begin{multline*}\n  \\texttt{bodyfat} = 20.195 \\ + \\\\\n    0.426 \\cdot \\texttt{triceps} + 0.429 \\cdot \\texttt{thigh} + 0.110 \\cdot \\texttt{midarm}\n\\end{multline*}\n\nStepwise building a model and rewriting it back in terms of the original predictors is a lot of work. Is there a better way?\n\n## PCR via the pls package\n\n\\scriptsize\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pls)\n\npcr_model <- pcr(bodyfat ~ ., data = bodyfat, validation = \"CV\")\nsummary(pcr_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: \tX dimension: 20 3 \n\tY dimension: 20 1\nFit method: svdpc\nNumber of components considered: 3\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps\nCV           5.239    2.711    2.735    2.867\nadjCV        5.239    2.695    2.713    2.832\n\nTRAINING: % variance explained\n         1 comps  2 comps  3 comps\nX          78.72    99.97   100.00\nbodyfat    74.97    78.01    80.14\n```\n\n\n:::\n:::\n\n\n## Selecting the optimal number of components\n\n- \"1-sigma rule\": select model with least number of components whose cross-validation error is at most 1 standard deviation away from optimal model\n- In human language: 2 PCs gives lowest RMSEP, but we can go down to 1 PC without losing too much\n\n\n::: {.cell}\n\n```{.r .cell-code}\nselectNcomp(pcr_model, method = \"onesigma\", plot = TRUE)\n```\n\n::: {.cell-output-display}\n![](02b-pca-theory_files/figure-beamer/unnamed-chunk-28-1.pdf){height=3in}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\n## Ridge regression (optional)\n\n- PCR is not the only way to \"regularize\" a regression model\n- Ridge regression: like ordinary regression, but punish model for coefficients that become too large.\n- Objective function:\n$$\n  J_{\\text{ridge}}(\\alpha, \\beta) = J(\\alpha, \\beta) + \\lambda (\\alpha^2 + \\beta^2).\n$$\n- Parameter $\\lambda$ set to a fixed value or determined via cross-validation.\n  - $\\lambda = 0$: ridge regression = ordinary regression\n  - $\\lambda \\to \\infty$: all coefficients become zero\n- Can be done with the `glmnet` package (not very userfriendly)\n\n## Example session\n\n\\scriptsize\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\npredictors <- data.matrix(bodyfat_predictors)\noutcome <- bodyfat$bodyfat\n\nlambdas <- 10^seq(2, -2, by = -.1)\nridge_cv <- cv.glmnet(predictors, outcome, alpha = 0, lambda = lambdas)\n\nbest <- ridge_cv$lambda.min\nbest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06309573\n```\n\n\n:::\n\n```{.r .cell-code}\nbest_ridge <- glmnet(predictors, outcome, alpha = 0, lambda = best)\ncoef(best_ridge)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                                   s0\n(Intercept)                -4.6532088\ntriceps.skinfold.thickness  0.6433295\nthigh.circumference         0.2965686\nmidarm.circumference       -0.2391984\n```\n\n\n:::\n:::\n",
    "supporting": [
      "02b-pca-theory_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}