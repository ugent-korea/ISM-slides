{
  "hash": "f8a8bbf82b6149ee138165225551fbc9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic Regression and Classification\"\nsubtitle: Introduction to Statistical Modelling\nauthor: Prof. Joris Vankerschaver\nformat:\n  beamer:\n    theme: Pittsburgh\n    colortheme: default\n    fonttheme: default\n    header-includes: |\n      \\setbeamertemplate{frametitle}[default][left]\n      \\setbeamertemplate{footline}[frame number]\n\n---\n\n\n\n## Overview\n\n1. Introduction: what are classification problems?\n2. K-nearest neighbors classification\n3. Logistic regression\n4. Classification \n\n# Introduction\n\n## Classification\n\nIn many problems, the outcome is a **categorical** variable:\n\n- Figure out whether mutation is deleterious (yes/no), based on DNA sequencing data.\n- Predict a person's eye color (blue/brown/green)\n- Predict the outcome of surgery (success/failure) for patients with ovarian cancer, based on patient characteristics\n- Classify iris (flower) variety given dimensions of leaves\n\nThese problems are examples of **classification** problems.\n\n## Techniques for classification\n\n- **Logistic regression**\n- **K-nearest neighbors**\n- Linear discriminant analysis\n- Support vector classification (SVC)\n- Decision trees\n- ...\n\nThe techniques in **bold** are discussed in this lecture.\n\nEach technique has its advantages and disadvantages.\n\n## References\n\n- *An Introduction to Statistical Learning*. Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Available for free online at https://www.statlearning.com/. \n  - Logistic regression: sections 4.1 - 4.3\n\n## Dataset\n\n`bdiag` -- Wisconsin breast cancer diagnostic dataset (*Nuclear feature extraction for breast tumor diagnosis.* W. Street, W. Wolberg, O. Mangasarian. Electronic imaging 29 (1993))\n\n\\vspace*{0.5cm}\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n  - Cell nuclei from 569 tumor samples\n  - Classified as malignant or benign\n  - Features:\n    - radius of the cell nucleus\n    - texture (variance of gray-scale values)\n\n:::\n::: {.column width=\"40%\"}\n\n![](./images/02-logistic-regression/Invasive_Ductal_Carcinoma_40x.jpg)\n:::\n::::\n\n\n\n\n\n\n\n\n\n## A first look at the data\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02d-logistic-regression_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n# K-nearest neighbors classification\n\n## Principle\n\n- Find $K$ nearest neighbors to $x$\n- Probability of belonging to class $i$ is proportional to number of neighbors in that class\n\n![](./images/02-logistic-regression/knearest.pdf)\n\n- $P(\\text{red}|X = x) = \\frac{3}{5} = 60\\%$\n- $P(\\text{green} | X = x) = \\frac{2}{5} = 40\\%$\n\n## Properties\n\nK-nearest neighbor (KNN) classification estimates probabilities\n$$\n  P(Y = j | X = x) = \\frac{1}{|\\mathcal{N}_K(x)|} \\sum_{i = 1}^{|\\mathcal{N}_K(x)|} I(y_i = j)\n$$\nHere:\n\n- $\\mathcal{N}_K(x)$ is the set of $K$ nearest datapoints to $x$\n- $I(y_i = j)$ is equal to $1$ if $y_i = j$ and to $0$ otherwise\n\n## Advantages and disadvantages\n\nAdvantages:\n\n- No \"training\" necessary \n- Robust to outliers\n- Can easily deal with more than 2 labels\n\nDisadvantages:\n\n- Not very interpretable -- why was class decided?\n- Memory-intensive\n\n## Decision boundary ($K = 5$)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# decision boundary plot code adapted from https://www.thomaspuschel.com/post/decision_boundary_plot2/\n\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: lattice\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'caret'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_knn_5 <- knn3(diagnosis ~ radius_mean + texture_mean, data = train, k = 5)\n\n\nlibrary(scales)  # for hue_pal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'scales'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:purrr':\n\n    discard\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:readr':\n\n    col_factor\n```\n\n\n:::\n\n```{.r .cell-code}\n# these will become arguments\nmodel <- model_knn_5\ndata <- train\nn <- 100\nn_classes <- 2  # derive from data?\n\n# plot code. TODO: turn into function\nxgrid <- with(data, expand_grid(\n  radius_mean = seq(0.95 * min(radius_mean), 1.05 * max(radius_mean), length.out = n),\n  texture_mean = seq(0.95 * min(texture_mean), 1.05 * max(texture_mean), length.out = n)))\ny_class_probs <- predict(model, newdata = xgrid, type = \"prob\")\n\ny_max_prob <- apply(y_class_probs, 1, max)\ny_max_i <- apply(y_class_probs, 1, which.max)\n\nbg_cols <- hue_pal()(n_classes)\n\nggplot(xgrid, aes(x = radius_mean, y = texture_mean)) +\n  geom_raster(aes(fill = y_max_i), alpha = y_max_prob) +\n  scale_fill_gradientn(colours=bg_cols,\n                       breaks = c(1, 2)) +\n  geom_point(data = data, aes(x = radius_mean, y = texture_mean, fill = as.numeric(diagnosis)),\n             pch = 21, color = \"black\", size = 2, alpha = 1) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](02d-logistic-regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n# Logistic regression\n\n## Reminder: odds\n\n- If $\\pi$ is the probability of having a malignant tumor, then the **odds** are defined as\n$$\n  \\text{Odds} = \\frac{\\pi}{1 - \\pi}.\n$$\nFor example: if $\\pi = 0.8$ then $\\text{Odds} = 4$, meaning that for every benign tumor there are 4 malignant ones (on average).\n\n- Odds range from 0 (impossible event) to $+\\infty$ (almost certain).\n\n## Reminder: odds ratio\n\n- **Odds ratio** (OR): indicates by how much the odds change between two treatments. For example: suppose in the treatment group the probability of a malignant tumor drops to $\\pi_T = 0.75$ (compared to $\\pi_C = 0.8$ in the untreated group). Then\n$$\n  \\text{OR} = \\frac{\\text{Odds}(T)}{\\text{Odds}(C)} = \\frac{3}{4} = 0.75\n$$\n- If $\\text{OR} < 1$, then the odds for treatment 1 decrease compared to treatment 2. If $\\text{OR} > 1$, the odds increase.\n\n\n## Log-odds (logits)\n\nOften it makes sense to work with the logarithm of the odds (**logits**):\n$$\n  \\text{logit}(\\pi) = \n    \\ln \\text{Odds} = \n    \\ln \\left( \\frac{\\pi}{1 - \\pi} \\right).\n$$\nTo convert back to probabilities, use the **logistic** function:\n$$\n  \\pi = \\frac{1}{1 + e^{-\\text{logit}}}.\n$$\n\nLogits are unbounded: $\\text{logit} \\to \\pm\\infty$ for $p \\to 0, 1$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tibble(x = seq(-5, 5, length.out = 100)), aes(x)) +\n  geom_function(fun = plogis) +\n  xlab(\"Logit\") +\n  ylab(\"Probability\")\n```\n\n::: {.cell-output-display}\n![](02d-logistic-regression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n## Regression for classification\n\n- Given data $(X_1, Y_1), \\ldots, (X_n, Y_n)$ where:\n  - Outcomes $Y_i$ are categorical (0 or 1)\n  - Predictors $X_i$ can be continuous or discrete\n\n- We will model $Y_i$ as a Bernoulli random variable ($0$ or $1$) with probability $\\pi(X_i)$:\n\\begin{align*}\n  Y_i & = 0 \\quad \\text{with probability $\\pi(X_i)$} \\\\\n  Y_i & = 1 \\quad \\text{with probability $1 - \\pi(X_i)$}\n\\end{align*}\n\n- Now we need to determine how $\\pi(X)$ depends on $X$.\n\n\n## Idea 1: linear regression (bad)\n\n- One predictor $X = \\mathtt{radius\\_mean}$, outcome $Y = 0$ (benign) or $Y = 1$ (malignant).\n- Assume $\\pi(X) = \\alpha + \\beta X$ and determine $\\alpha, \\beta$ through linear regression.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(train, aes(x = radius_mean, y = diagnosis_binary)) +\n  geom_point(aes(color = diagnosis)) +\n  stat_smooth(method=\"lm\", se=FALSE, color = \"gray40\") +\n  ylab(\"Probability\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](02d-logistic-regression_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=2in height=1.5in}\n:::\n:::\n\n\n\nProblems:\n\n- Fitted probabilities can take on values outside $[0, 1]$.\n- Does not easily generalize to more than two classes.\n\n## Idea 2: logistic regression (better)\n\n- Let $\\pi(X)$ depend on $X$ through the logistic function\n$$\n  \\pi(X) = \\frac{1}{1 + \\exp(-(\\alpha + \\beta X))}.\n$$\n- **Nonlinear** model in parameters $\\alpha$, $\\beta$\n- Alternatively, apply the logit transformation\n$$\n  \\text{logit}(\\pi) = \\alpha + \\beta X.\n$$\n- Linear in the logits.\n\n## Determining the regression parameters: MLE\n\n- **Likelihood function** $\\mathcal{L}$: probability of observing the data given the parameters $\\alpha$, $\\beta$:\n$$\n  \\mathcal{L}(\\alpha, \\beta) = \\prod_{i = 1}^n P(Y = Y_i | X = X_i),\n$$\nwhere \n$$\n  P(Y = Y_i | X = X_i) = \\pi(X_i)^{Y_i}(1 - \\pi(X_i))^{1 - {Y_i}}.\n$$\nis the probability of observing one data point $(X_i, Y_i)$.\n\n- In practice, often better to use the log of the likelihood:\n$$\n  \\ell(\\alpha, \\beta) = \\ln \\mathcal{L}(\\alpha, \\beta).\n$$\n\n## Determining the regression parameters: MLE\n\n- **Maximum likelihood estimation** (MLE): find parameters that maximize $\\mathcal{L}(\\alpha, \\beta)$ or $\\ell(\\alpha, \\beta)$\n- Finding maximum: set partial derivatives (score functions) equal to zero:\n$$\n  \\frac{\\partial \\ell}{\\partial \\alpha} = 0, \\quad\n  \\frac{\\partial \\ell}{\\partial \\beta} = 0.\n$$\n- Complicated equations, usually maximum cannot be found analytically (unlike least squares)\n- Use numerical methods to find maximum (R does this automatically with the `glm` command)\n\n## Simplified example: MLE for binomial variable\n\n- Suppose there are *no* predictors. We just have a bunch of categorical outcomes $Y_i = 0, 1$, e.g.\n$$\n  Y = (0, 0, 1, 0, 1, 0, \\ldots, 1, 1, 0, 0, 1)\n$$\n\n- In semester 1 we saw that a good estimate for the probability $\\pi = P(Y = 1)$ is given by the proportion of 1s in the data:\n$$\n  \\hat{\\pi} = \\frac{1}{N} \\sum_{i = 1}^N Y_i = \\bar{Y}.\n$$\n\n- We'll use MLE to re-derive this result.\n\n## Simplified example: MLE for binomial variable\n\n- Likelihood\n\\begin{align*}\n  \\mathcal{L}(\\pi) \n    & = \\Pi_{i = 1}^n P(Y = Y_i) \\\\\n    & = \\pi^{n\\bar{Y}} (1 - \\pi)^{n(1 - \\bar{Y})}\n\\end{align*}\n\n- Log likelihood: $\\ell(\\pi) = n \\bar{Y} \\ln \\pi + n(1-\\bar{Y})\\ln(1 - \\pi)$.\n\n- Maximum occurs when first derivative vanishes:\n$$\n  \\frac{d \\ell}{d \\pi} = \\frac{n \\bar{Y}}{\\pi} - \\frac{n(1 - \\bar{Y})}{1 - \\pi} = 0.\n$$\n- Simplifies to $\\hat{\\pi} = \\bar{Y}$.\n\n\n## MLE for logistic regression in R\n\n\\scriptsize\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_simple <- glm(diagnosis ~ radius_mean, data = train, family = \"binomial\")\nsummary(m_simple)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = diagnosis ~ radius_mean, family = \"binomial\", data = train)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -15.8086     1.5310 -10.326   <2e-16 ***\nradius_mean   1.0662     0.1066   9.998   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 604.40  on 454  degrees of freedom\nResidual deviance: 256.54  on 453  degrees of freedom\nAIC: 260.54\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\n\n## The log likelihood\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_lh <- function(x, y, alpha, beta) {\n  lp <- alpha + beta * x\n  px <- 1 / (1 + exp(-lp))\n  \n  llh <- log(px)\n  llh[y == 0] <- log(1 - px[y == 0])\n  sum(llh)\n}\n\nalpha <- seq(-20, -10, length.out = 20)\nbeta <- seq(0, 2, length.out = 20)\n\nx <- train$radius_mean\ny <- train$diagnosis_binary\n\nllh <- matrix(nrow = length(alpha), ncol = length(beta))\nfor (i in seq_along(alpha)) {\n  for (j in seq_along(beta)) {\n    llh[i, j] <- log_lh(x, y, alpha[[i]], beta[[j]])\n  }\n}\n\nfilled.contour(alpha, beta, llh,\n               xlab = \"alpha\",\n               ylab = \"beta\",\n               plot.axes = {\n                 axis(1)\n                 axis(2)\n                 points(-15.8086, 1.0662, pch = \"x\", cex = 2, col = \"white\")\n               })\n```\n\n::: {.cell-output-display}\n![](02d-logistic-regression_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n- Value of log likelihood at MLE: $\\ell = -128.2701$.\n- R reports (residual) deviance: $D = -2 \\times \\ell = 256.54$\n\n## Multiple logistic regression\n\n- Like in linear regression, often the outcome $Y$ is influenced by several predictors $X_1, X_2, \\ldots, X_p$.\n- For example: `diagnosis` depends on `radius_mean` and `texture_mean`:\n\\begin{multline*}\n  \\mathrm{logit}(\\pi) =\n  \\alpha +\n  \\beta_1 \\cdot \\mathtt{radius\\_mean} +\n  \\beta_2 \\cdot \\mathtt{texture\\_mean}.\n\\end{multline*}\n- Parameters $\\alpha, \\beta_1, \\ldots, \\beta_p$ determined through MLE.\n\n## In R\n\n\\scriptsize\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_multi <- glm(diagnosis ~ radius_mean + texture_mean,\n               data = train, family = \"binomial\")\nsummary(m_multi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = diagnosis ~ radius_mean + texture_mean, family = \"binomial\", \n    data = train)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -20.51694    2.04729 -10.021  < 2e-16 ***\nradius_mean    1.09536    0.11727   9.341  < 2e-16 ***\ntexture_mean   0.21749    0.04034   5.391 7.01e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 604.40  on 454  degrees of freedom\nResidual deviance: 223.68  on 452  degrees of freedom\nAIC: 229.68\n\nNumber of Fisher Scoring iterations: 7\n```\n\n\n:::\n:::\n\n\n\n## Interactions between variables\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_inter <- glm(diagnosis ~ radius_mean * texture_mean,\n               data = train, family = \"binomial\")\n```\n:::\n\n\n\n::: {#tbl:model-coefficients}\n| Coefficient               | Estimate | SE         | z value | p value  |\n|---------------------------|---------:|-----------:|--------:|---------:|\n| (Intercept)               |  -8.3046 |     7.4554 |  -1.114 |   0.2653 |\n| radius                    |   0.2182 |     0.5288 |   0.413 |   0.6798 |\n| texture                   |  -0.4133 |     0.3855 |  -1.072 |   0.2836 |\n| radius:texture            |   0.0455 |     0.0276 |   1.647 |   0.0995 |\n:::\n\nInteraction between radius and texture is not significant\n\n\n## Making predictions (by hand)\n\nWhat is the probability of a tumor being malignant if the radius is 13 mm?\n\n\\begin{align*}\n \\pi(\\mathtt{radius\\_mean} = 13) \n     & = \\frac{1}{1 + \\exp(15.8086 - 1.0662 \\times 13)} \\\\\n     & = 0.1247716\n\\end{align*}\n\nNo easy formula for confidence interval on the prediction.\n\n## Making predictions (using R)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(m_simple,\n        newdata = data.frame(radius_mean = 13), \n        type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1 \n0.1247961 \n```\n\n\n:::\n:::\n\n\n\n## Computing a confidence interval for the prediction\n\nProceeds in three steps:\n\n1. Make a prediction on the **logit** scale (`type = \"link\"`)\n2. Compute CI on logit scale from SE (`se.fit = TRUE`)\n3. Map CI back to probabilities\n\nFor step 3, use `plogis` to undo the logit transformation:\n$$\n  \\text{plogis}(x) = \\frac{1}{1 + \\exp(-x)}.\n$$\n\n\n## Computing an CI: example\n\n**Step 1**: Prediction on the logit scale.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- predict(m_simple,\n                newdata = data.frame(radius_mean = 13), \n                type = \"link\", se.fit = TRUE)\n```\n:::\n\n\n\n**Step 2**: CI on the logit scale.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci_logits <- c(pred$fit - 1.96 * pred$se.fit,\n               pred$fit + 1.96 * pred$se.fit)\nci_logits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         1 \n-2.358216 -1.537335 \n```\n\n\n:::\n:::\n\n\n\n##\n\n**Step 3**: CI on the original scale (probabilities)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci_probs <- c(plogis(ci_logits[1]), plogis(ci_logits[2]))\nci_probs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         1          1 \n0.08641491 0.17692307 \n```\n\n\n:::\n:::\n\n\n\nOriginal prediction:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_probs <- plogis(pred$fit)\npred_probs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1 \n0.1247961 \n```\n\n\n:::\n:::\n\n\n\n**Conclusion**:  The predicted probability that a tumor of radius 13mm is malignant is 12.5% (95% CI: [8.6%, 17.7%])\n\n\n## Making predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(train, aes(x = radius_mean, y = diagnosis_binary)) +\n  geom_vline(xintercept = 13, linetype = \"dashed\", color = \"gray60\") +\n  geom_hline(yintercept = 0.1247961, linetype = \"dashed\", color = \"gray60\") +\n  geom_point(aes(color = diagnosis)) +\n  stat_smooth(method=\"glm\", se=FALSE, color = \"gray40\",\n              method.args = list(family=binomial))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](02d-logistic-regression_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Quantifying the strength of an association\n\nWrite the logistic regression model in terms of odds as\n$$\n  \\text{logit}(\\pi) = \\ln \\text{Odds} = \n  \\alpha + \\beta X.\n$$\nAfter some algebra:\n$$\n  e^\\beta = \\frac{\\text{Odds}(X + 1)}{\\text{Odds(X)}}.\n$$\nIn other words: $e^\\beta$ is the odds ratio (OR) associated to a 1-unit increase in $X$.\n\n::: {.callout-note}\n## Breast cancer dataset\nHere $\\beta = 1.0662$, so $\\text{OR} = \\exp(1.0662) = 2.90$. An increase in 1 mm in tumor radius is associated with odds \nthat are 2.90 times higher (risk increase).\n:::\n\n## Testing an association\n\n- Often, we want to test whether a model coefficient $\\beta$ is significant.\n- Related: check if complex and simple nested models are equivalent (recall $F$-test from linear regression).\n\nSeveral ways of testing:\n\n- Wald test (reported in `summary`): can be conservative\n- Likelihood ratio test (via `anova` command): more power, preferred\n- Score test (not covered)\n\n## Testing an association: Wald test\n\n- Null hypothesis $H_0: \\beta = 0$, alternative hypothesis $H_A: \\beta \\ne 0$\n- Test statistic follows $N(0, 1)$ under $H_0$\n$$\n  z = \\frac{\\hat{\\beta}}{SE(\\beta)} \\sim N(0, 1)\n  \\quad \\text{under $H_0$}.\n$$\n- Reported in the R regression output (`summary`):\n\n::: {#tbl:model-coefficients}\n| Coefficient   | Estimate | SE | z value | p value      |\n|---------------|---------:|-----------:|--------:|-------------:|\n| (Intercept)   | -20.5169 |     2.0473 | -10.021 |   < 2e-16    |\n| radius_mean   |   1.0954 |     0.1173 |   9.341 |   < 2e-16    |\n| texture_mean |   0.2175 |     0.0403 |   5.391  |   7.01e-08   |\n:::\n\n\n\n## Testing an association: Likelihood ratio test\n\nUseful for:\n\n- Comparing nested models (simple/complex)\n- Testing single coefficient\n\nHypothesis:\n\n- $H_0$: simple and complex model are equivalent\n- $H_A$: complex model is better\n\nTest statistic: **deviance**\n\\begin{align*}\n  D & = -2 \\ln \\frac{\\mathcal{L}(\\text{simple})}{\\mathcal{L}(\\text{complex})} \\\\\n    & = -2 \\ell(\\text{simple}) + 2 \\ell(\\text{complex}).\n\\end{align*}\n\nUnder $H_0$, $D$ follows a $\\chi^2_k$ distribution, where $k$ is the number of extra parameters in the complex model.\n\n\n## Worked out example\n\nNested models:\n\n- Simple: includes `radius_mean` only\n- Complex: includes both `radius_mean` and `texture_mean`.\n\nFrom R summary (listed as residual deviance) or direct calculation:\n\n- $-2\\ell(\\text{simple}) = 256.54$\n- $-2\\ell(\\text{complex}) = 223.68$\n\nHence $D = 256.54 - 223.68 = 32.86 > 3.841459 = \\chi^2_{1; 0.95}$. \n\nConclusion: reject $H_0$, significant evidence to decide (at 5% significance level) that complex model is better.\n\n## Likelihood ratio test in R (single variable)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(m_simple, m_multi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel 1: diagnosis ~ radius_mean\nModel 2: diagnosis ~ radius_mean + texture_mean\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n1       453     256.54                          \n2       452     223.68  1   32.864 9.882e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\nCompare with critical values for $\\chi^2_1$ to draw conclusion\n\n\n## Likelihood ratio test in R (groups of variables)\n\nNested models:\n\n- Simple: includes `radius_mean` and `texture_mean`.\n- Complex: adds `concavity_mean` and `symmetry_mean`.\n\nR output:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(m_multi, m_multi_4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel 1: diagnosis ~ radius_mean + texture_mean\nModel 2: diagnosis ~ radius_mean + texture_mean + concavity_mean + symmetry_mean\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n1       452     223.68                          \n2       450     129.99  2   93.686 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\nCompare with critical value $\\chi^2_{2; 0.95} = 5.991465$ to conclude that complex model is better.\n\n## Confidence interval for regression parameters\n\nWald-type **approximate** $(1 - \\alpha) \\times 100\\%$ confidence interval for $\\beta$:\n$$\n  \\hat{\\beta} \\pm z_{1 - \\alpha/2} \\cdot SE(\\beta)\n$$\n\n::: {.callout-note}\n## Breast cancer dataset\n95% confidence interval for $\\beta_{\\mathtt{radius\\_mean}}$:\n$$\n  1.095 \\pm 1.96 \\times 0.117 = [0.866, 1.324].\n$$\n::: \n\n\n## Confidence interval for regression parameters in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(m_multi)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWaiting for profiling to be done...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   2.5 %      97.5 %\n(Intercept)  -24.8846042 -16.8233485\nradius_mean    0.8840034   1.3456187\ntexture_mean   0.1405734   0.2993915\n```\n\n\n:::\n:::\n\n\n\nR uses the so-called profile method to compute CI:\n\n- Different from Wald method (narrower CIs, but close)\n- Preferred to use this method through R\n\n## Confidence interval for odds ratio\n\n- Recall that $\\exp(\\beta) = \\text{OR}$ for a 1-unit change in $X$\n- $(1 - \\alpha) \\times 100\\%$ confidence interval for the $\\text{OR}$:\n$$\n  \\exp\\left( \\hat{\\beta} \\pm z_{1 - \\alpha/2} \\cdot SE(\\beta) \\right).\n$$\n\n::: {.callout-note}\n## Breast cancer dataset\n95% confidence interval for $\\text{OR}_{\\mathtt{radius\\_mean}}$:\n\\begin{align*}\n  \\exp(1.095 \\pm 1.96 \\times 0.117)\n    & = [\\exp(0.866), \\exp(1.324)] \\\\\n    & = [2.377, 3.759]\n\\end{align*}\n:::\n\n# Classification\n\n## Decision boundary (no interaction terms)\n\n- Model: $\\text{logit}(\\texttt{diagnosis}) \\sim \\texttt{radius} + \\texttt{texture}$\n- Decision boundary is **straight** line\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(scales)  # for hue_pal()\n\n# these will become arguments\nmodel <- m_multi\ndata <- train\nn <- 100\nn_classes <- 2  # derive from data?\n\n# plot code. TODO: turn into function\nxgrid <- with(data, expand_grid(\n  radius_mean = seq(0.95 * min(radius_mean), 1.05 * max(radius_mean), length.out = n),\n  texture_mean = seq(0.95 * min(texture_mean), 1.05 * max(texture_mean), length.out = n)))\n\n\ny_response <- predict(model, newdata = xgrid, type = \"response\")\ny_class_probs <- cbind(1 - y_response, y_response)\ncolnames(y_class_probs) <- c(\"B\", \"M\")\n\ny_max_prob <- apply(y_class_probs, 1, max)\ny_max_i <- apply(y_class_probs, 1, which.max)\n\nbg_cols <- hue_pal()(n_classes)\n\nggplot(xgrid, aes(x = radius_mean, y = texture_mean)) +\n  geom_raster(aes(fill = y_max_i), alpha = y_max_prob) +\n  scale_fill_gradientn(colours=bg_cols,\n                       breaks = c(1, 2)) +\n  geom_point(data = data, aes(x = radius_mean, y = texture_mean, fill = as.numeric(diagnosis)),\n             pch = 21, color = \"black\", size = 2, alpha = 1) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](02d-logistic-regression_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n\n## Decision boundary (with interaction terms)\n\n- Model: $\\text{logit}(\\texttt{diagnosis}) \\sim \\texttt{radius} + \\texttt{texture} + \\texttt{radius}:\\texttt{texture}$\n- Decision boundary is **curved** line\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# these will become arguments\nmodel <- m_inter\ndata <- train\nn <- 100\nn_classes <- 2  # derive from data?\n\n# plot code. TODO: turn into function\nxgrid <- with(data, expand_grid(\n  radius_mean = seq(0.95 * min(radius_mean), 1.05 * max(radius_mean), length.out = n),\n  texture_mean = seq(0.95 * min(texture_mean), 1.05 * max(texture_mean), length.out = n)))\n\n\ny_response <- predict(model, newdata = xgrid, type = \"response\")\ny_class_probs <- cbind(1 - y_response, y_response)\ncolnames(y_class_probs) <- c(\"B\", \"M\")\n\ny_max_prob <- apply(y_class_probs, 1, max)\ny_max_i <- apply(y_class_probs, 1, which.max)\n\nbg_cols <- hue_pal()(n_classes)\n\nggplot(xgrid, aes(x = radius_mean, y = texture_mean)) +\n  geom_raster(aes(fill = y_max_i), alpha = y_max_prob) +\n  scale_fill_gradientn(colours=bg_cols,\n                       breaks = c(1, 2)) +\n  geom_point(data = data, aes(x = radius_mean, y = texture_mean, fill = as.numeric(diagnosis)),\n             pch = 21, color = \"black\", size = 2, alpha = 1) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](02d-logistic-regression_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n\n\n\n\n## Classification\n\n- Once we have a (logistic) model for $\\pi(X)$, we can use it to classify new data $X$ as negative ($Y = 0$) or positive ($Y = 1$), by comparing $\\pi(X)$ with a fixed threshold $C$:\n$$\n  Y = 1 \\quad \\text{if $\\pi(X) > C$, otherwise $Y = 0$}.\n$$\n- Performance **depends on choice of $C$**\n\n::: {.callout-note}\n## Breast cancer dataset\nWe computed earlier that $\\pi(\\mathtt{radius\\_mean} = 13) = 0.12$. Assuming that the threshold for malignant samples is $C = 0.5$, this sample would be classified as **benign**.\n:::\n\n## Confusion matrix\n\nBy comparing labels given by our model with \"actual\" labels, we can get an idea of the performance of our classifier.\n\n![](./images/02-logistic-regression/confusion_matrix.png)\n\nFigure source: \\url{https://en.wikipedia.org/wiki/Confusion_matrix}\n\n## Performance metrics\n\n| Name | Definition | Value for example|\n|------|------------|-------:|\n| Accuracy | (TP + TN)/(P + N) | 0.84 |\n| Sensitivity (recall) | TP / P | 0.93 |\n| Specificity | TN / N | 0.67 |\n| PPV (precision) | TP / PP | 0.84 |\n| NPV | TN / PN | 0.84 |\n\n- Many other metrics exist\n- Which one is important depends on the problem\n- Metrics can give surprising results in case of unbalanced data \n\n## In R (via caret package)\n\n\\scriptsize\n\\centering\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\n\npred_test <- predict(m_simple, test, type=\"response\")\nclass_test <- ifelse(pred_test >= 0.2, \"M\", \"B\")\n\nconf_matrix <- confusionMatrix(as.factor(class_test), test$diagnosis, positive = \"M\")\nprint(conf_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  B  M\n         B 64  6\n         M 11 33\n                                         \n               Accuracy : 0.8509         \n                 95% CI : (0.772, 0.9107)\n    No Information Rate : 0.6579         \n    P-Value [Acc > NIR] : 3.039e-06      \n                                         \n                  Kappa : 0.6786         \n                                         \n Mcnemar's Test P-Value : 0.332          \n                                         \n            Sensitivity : 0.8462         \n            Specificity : 0.8533         \n         Pos Pred Value : 0.7500         \n         Neg Pred Value : 0.9143         \n             Prevalence : 0.3421         \n         Detection Rate : 0.2895         \n   Detection Prevalence : 0.3860         \n      Balanced Accuracy : 0.8497         \n                                         \n       'Positive' Class : M              \n                                         \n```\n\n\n:::\n:::\n\n\n\n## Trading sensitivity and specificity\n\nWhat is important?\n\n- Diagnostic test: **sensitivity** (don't tell people with tumor that they are healthy). Choose low threshold.\n- Classifying email as spam: **specificity** (don't put regular email in the spam folder). Choose high threshold.\n\nBy changing the threshold, sensitivity and specificity can be traded against one another.\n\n- Lowering threshold: $\\text{Sensitivity} \\uparrow$, $\\text{Specificity} \\downarrow$.\n- Increasing threshold: $\\text{Sensitivity} \\downarrow$, $\\text{Specificity} \\uparrow$.\n\n##\n\n::: {.callout-note}\n## Breast cancer dataset\n\n- For $C = 0.5$: sensitivity 0.84\n\n| Prediction \\ Reference | B  | M  |\n|------------------------|----|----|\n| B                      | 70 | **13** |\n| M                      | 5  | 26 |\n\n- For $C = 0.2$: sensitivity **0.85**\n\n| Prediction \\ Reference | B  | M  |\n|------------------------|----|----|\n| B                      | 64 | **6**  |\n| M                      | 11 | 33 |\n:::\n\n\n## Sensitivity and specificity as a function of threshold\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_test <- predict(m_simple, test, type=\"response\")\nse_sp_for_C <- function(C) {\n  class_test <- ifelse(pred_test >= C, \"M\", \"B\")\n  conf_matrix <- confusionMatrix(as.factor(class_test), test$diagnosis, positive = \"M\")\n  tibble(C = C,\n         se = conf_matrix[[\"byClass\"]][[\"Sensitivity\"]],\n         sp = conf_matrix[[\"byClass\"]][[\"Specificity\"]])\n}\n\nse_sp_curves <- seq(0, 1, length.out = 50) |>\n  map(se_sp_for_C) |>\n  list_rbind()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in confusionMatrix.default(as.factor(class_test), test$diagnosis, :\nLevels are not in the same order for reference and data. Refactoring data to\nmatch.\nWarning in confusionMatrix.default(as.factor(class_test), test$diagnosis, :\nLevels are not in the same order for reference and data. Refactoring data to\nmatch.\n```\n\n\n:::\n\n```{.r .cell-code}\nse_sp_curves |> \n  reshape2::melt(id.var = \"C\") |>\n  ggplot(aes(x = C, y = value, color = variable)) +\n  geom_line() +\n  xlab(\"C (Threshold)\") +\n  ylab(NULL) +\n  scale_color_hue(labels = c(se = \"Sensitivity\", sp = \"Specificity\"), name = NULL)\n```\n\n::: {.cell-output-display}\n![](02d-logistic-regression_files/figure-html/unnamed-chunk-24-1.png){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n\nAs threshold increases:\n\n- Sensitivity **decreases** (less true positives)\n- Specificity **increases** (less false positives)\n\n\n## ROC curve\n\n- By varying $C$ from 0 to 1, sensitivity and specificity change continuously and trace out the **Receiver Operator Curve (ROC)**.\n- The closer the curve sticks to the upper left corner, the better\n- Can be used to compare classifiers\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(pROC)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nType 'citation(\"pROC\")' for a citation.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'pROC'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n```\n\n\n:::\n\n```{.r .cell-code}\npred_test <- predict(m_simple, test, type=\"response\")\nroc_logis <- roc(test$diagnosis_binary, pred_test)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting levels: control = 0, case = 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting direction: controls < cases\n```\n\n\n:::\n\n```{.r .cell-code}\nauc <- round(auc(test$diagnosis_binary, pred_test), 4)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting levels: control = 0, case = 1\nSetting direction: controls < cases\n```\n\n\n:::\n\n```{.r .cell-code}\nggroc(roc_logis) +\n    ggtitle(paste0('ROC Curve ', '(AUC = ', auc, ')'))\n```\n\n::: {.cell-output-display}\n![](02d-logistic-regression_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=2.5in height=2.5in}\n:::\n:::\n\n\n\n## ROC: KNN versus logistic regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ROC for KNN\npred_test_knn <- predict(model_knn_5, newdata = test, type = \"prob\")[, 2]\nroc_knn <- roc(test$diagnosis_binary, pred_test_knn)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting levels: control = 0, case = 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting direction: controls < cases\n```\n\n\n:::\n\n```{.r .cell-code}\nggroc(list(knn=roc_knn, logis=roc_logis)) +\n  scale_color_hue(labels = c(knn = \"KNN\", logis = \"Logistic regression\"), name = NULL)\n```\n\n::: {.cell-output-display}\n![](02d-logistic-regression_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n\n\n## AUC: Area under the ROC\n\nSingle number to quantify performance of classifier:\n\n- $\\text{AUC} = 1.0$: distinguishes perfectly between two classes\n- $\\text{AUC} = 0.5$: classifier no better than guessing randomly\n- $0.5 < \\text{AUC} < 1.0$: varying degrees of performance.\n\n## AUC: Link with concordance probability\n\n**Concordance probability**: probability that classifier will give a negative sample a lower probability than a positive sample.\n\nThe AUC is equal to the concordance probability \n$$\n  \\text{AUC} = P(\\pi(x_{\\text{neg}}) \\le \\pi(x_{\\text{pos}}))\n$$\n\nImportant for model calibration:\n\n- Often, we don't care much about probability $\\pi$ to belong to the positive class\n- But, want negative samples to have lower probability than positive samples\n\n## In clinical research\n\nThe ROC and AUC are often reported in clinical research.\n\n![](./images/02-logistic-regression/di-donna-roc.png){fig-align=center width=75%}\n\n\n## \n\n![](./images/02-logistic-regression/di-donna-auc.png){fig-align=center width=75%}\n\\scriptsize\nFigure and text from Di Donna *et al.*, Concordance of Radiological, Laparoscopic and Laparotomic Scoring to Predict Complete Cytoreduction in Women with Advanced Ovarian Cancer. Cancers (2023)\n\n\n",
    "supporting": [
      "02d-logistic-regression_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}