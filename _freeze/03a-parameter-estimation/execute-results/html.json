{
  "hash": "3c800551f49795b5226d1e0947d296a8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Nonlinear Modeling: Parameter Estimation\"\nsubtitle: Introduction to Statistical Modelling\nauthor: Prof. Joris Vankerschaver\npdf-engine: lualatex\nformat:\n  beamer:\n    theme: Pittsburgh\n    colortheme: default\n    fonttheme: default\n    header-includes: |\n      \\setbeamertemplate{frametitle}[default][left]\n      \\setbeamertemplate{footline}[frame number]\n      \\usepackage{emoji}\n      \\usepackage{luatexko}\n\n---\n\n\n\n\n\n## Outline\n\n\\tableofcontents\n\n## Learning outcomes\n\nYou should be able to\n\n- Determine the parameters of a nonlinear model via minimization (using R)\n- Understand the principles behind various minimization algorithms, as well as their advantages and disadvantages\n- Be able to assess the fit of a model\n\n# Example: building a stock-recruitment model\n\n## M. merluccius: stock-recruitment model\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\\vspace*{0.5cm}\n\nEuropean hake (*M. merluccius*)\n\n- Deep water fish\n- Important for European fisheries\n- Similar to 명태 in Korea\n\n:::\n::: {.column width=\"50%\"}\n![](./images/03a-parameter-estimation/2560px-Fish_-_Mercato_Orientale_-_Genoa,_Italy_-_DSC02485.jpeg)\n\n:::\n:::\n\n::: {.callout-note}\n\n## Stock-recruitment model\n\nModel of **number of adult fish** (recruitment) as a function of **spawning biomass** (fish that can reproduce).\n:::\n\n## M.merluccius: Dataset\n\n15 observations, 3 features:\n\n- `spawn.biomass`: spawning (stock) biomass\n- `num.fish`: number of fish (recruitment)\n- `year`: not used\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nload(\"datasets/03-parameter-estimation/M.merluccius.rda\")\n\nM.merluccius |>\n  ggplot(aes(spawn.biomass, num.fish)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n\n\n## M. merluccius: Beverton-Holt model\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\\vspace*{0.5cm}\n\nBeverton-Holt model (1956):\n$$\n  f(S; \\alpha, k) = \\frac{\\alpha S}{1 + S/k}\n$$\n:::\n::: {.column width=\"50%\"}\n![](./images/03a-parameter-estimation/beverton-holt.pdf){fig-align=center width=75%}\n:::\n::::\n\n\nParameters:\n\n- $\\alpha$: initial growth rate (for $S = 0$)\n  $$\n    \\alpha = f'(0; \\alpha, k)\n  $$\n- $k$: related to behavior for large $S$\n  $$\n    k \\alpha = \\lim_{S \\to +\\infty} f(S; \\alpha, k)\n  $$\n\n\n## Beverton-Holt: Effect of varying $\\alpha$ and $k$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbeverton_holt <- function(S, theta) {\n  alpha <- theta[[1]]\n  k <- theta[[2]]\n  y <- alpha * S / (1 + S / k)\n  tibble(S = S, alpha = alpha, k = k, y = y)\n}\n\nalpha <- 2.0\nks <- c(0.5, 1.0, 2.0)\nalphas <- c(1.0, 2.0, 3.0)\n\nS <- seq(0, 10, length.out = 100)\nplot_data_ks <- map(ks, \\(k) beverton_holt(S, c(alpha, k))) |>\n  list_rbind() |>\n  mutate(k = as.factor(k))\n\np <- ggplot(plot_data_ks, aes(x = S, y = y, color = k, group = k)) +\n  geom_line() +\n  ggtitle(paste0(\"Alpha = \", alpha, \", varying k\"))\n\nk <- 1.0\nplot_data_alphas  <- map(alphas, \\(alpha) beverton_holt(S, c(alpha, k))) |>\n  list_rbind() |>\n  mutate(alpha = as.factor(alpha))\n\nq <- ggplot(plot_data_alphas, aes(x = S, y = y, color = alpha, group = alpha)) +\n  geom_line() +\n  ggtitle(paste0(\"Varying alpha (k = \", k, \")\"))\n\ngrid.arrange(p, q, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=4.5in height=2in}\n:::\n:::\n\n\n\n## Goals\n\n- **Parameter estimation**: Find values $\\hat{\\alpha}$ and $\\hat{k}$ that best fit data.\n- **Uncertainty quantification**: Provide a measure of uncertainty for parameter values (confidence interval)\n- **Sensitivity analysis**: Understand how model changes if parameters are varied\n\n# Parameter estimation\n\n## What is parameter estimation?\n\nDetermining the **optimal values for the parameters** using the experimental data, assuming that the model is known.\n\nExample: For *M. merluccius*, we will see that $\\hat{\\alpha} = 5.75$, $\\hat{k} = 33.16$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbh_fit <- nls(\n  num.fish ~ alpha * spawn.biomass / (1 + spawn.biomass / k),\n  data = M.merluccius,\n  start = list(alpha = 6, k = 20))\n\nggplot(M.merluccius, aes(spawn.biomass, num.fish)) +\n  geom_point() +\n  geom_function(\n    fun = \\(S) predict(bh_fit, newdata = data.frame(spawn.biomass = S)), \n    xlim = c(0, 80))\n```\n\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n\n## Specifying a model\n\nAssume that we are **given** a nonlinear model\n$$\n  y = f(x; \\theta) + \\epsilon\n$$\nwhere $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ is normally distributed noise.\n\n- $x$: inputs, predictors, features (e.g. `spawn.biomass`)\n- $y$: outcome, depent variable (e.g. `num.fish`)\n- $\\theta$: (vector of) parameters (e.g. $\\theta = (\\alpha, k)$)\n\nWe will not talk about **building** a model (see one of your many other courses)\n\n## The objective function\n\nGiven a dataset $(x_1, y_1), \\ldots, (x_N, y_N)$, we want to quantify how well the model fits the data.\n\n**Objective function**: measures difference (squared) between predictions $f(x_i; \\theta)$ and actual values $y_i$:\n$$\n  J(\\theta) = \\sum_{i=1}^N (y_i - f(x_i; \\theta))^2\n$$\n\n## Minimizing the objective function\n\n**Goal:** Find the parameter value(s) $\\hat{\\theta}$ so that $J(\\theta)$ is minimal:\n$$\n  \\hat{\\theta} = \\text{argmin}_\\theta\\, J(\\theta).\n$$\nProblems:\n\n- Depending on $f(x; \\theta)$ this can be very difficult\n- There may be multiple (local) minima\n- Almost always needs to be done numerically\n\n## Example: linear regression\n\nIn linear regression, $f(x; \\theta) = \\alpha + \\beta x$, so that\n$$\n  J(\\alpha, \\beta) = \\sum_{i = 1}^N (y_i - \\alpha - \\beta x_i)^2.\n$$\nMinimizing $J(\\alpha, \\beta)$ can be done by setting the partial derivatives equal to zero and gives the usual formulas:\n$$\n  \\hat{\\beta} = R \\frac{s_y}{s_x}, \\quad \\hat{\\alpha} = \\bar{y} - \\hat{\\beta} \\bar{x}.\n$$\n\nIn general, **no closed-form formula exists** for the optimal parameters.\n\n## Before parameter estimation: select parameters\n\nMore parameters = more work and less certainty:\n\n- Solver may not converge\n- Wider uncertainty estimates for parameters and outputs\n- Correlations between parameters can make it impossible to find parameters\n\nConsider selecting subset of parameters to estimate:\n\n- Fix parameters at experimental values\n- Omit least sensitive parameters\n\n\n## Before parameter estimation: select initial values\n\nNumerical optimization algorithm requires a good **starting guess** for the parameters. When choice is bad:\n\n- Algorithm will converge slowly (take many iterations)\n- Optimization will fail altogether\n\nHow to find initial guess:\n\n- Determine from model properties (growth rate, asymptotes)\n- Use (known) experimental values\n- Use trial and error (select from grid of values)\n\nDoesn't need to be overly precise, a rough estimate is usually sufficient. \n\n## Initial values for M. merluccius\n\n- Slope: $\\alpha_0 = \\frac{75}{15} = 6$\n- Horizontal asymptote: $k_0 \\alpha_0 = 120$, so $k_0 = 20$.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeasure <- arrow(angle = 90, ends = \"both\", length = unit(0.1, \"in\"))\n\nM.merluccius |>\n  ggplot(aes(spawn.biomass, num.fish)) +\n  annotate(\"segment\",\n           x = 15, xend = 30, y = 30, yend = 30, \n           color = \"red\", linewidth = 1, arrow = measure) +\n  annotate(\"text\", x = 22.5, y = 35, size = 5, color = \"red\", label = \"15\") +\n  annotate(\"segment\",\n           x = 32, xend = 32, y = 30, yend = 105, \n           color = \"red\", linewidth = 1, arrow = measure) +\n  annotate(\"text\", x = 34, y = 75, size = 5, color = \"red\", label = \"75\") +\n  geom_hline(yintercept = 120, linewidth = 1, linetype = \"dashed\",\n             color = \"blue\") +\n  annotate(\"text\", x = 25, y = 125, size = 5, color = \"blue\",\n           label = \"min.fish = 120\") +\n  geom_point() \n```\n\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n\nLater, we will see that the initial guesses are close to the optimal parameters $\\hat{\\alpha} = 5.75$, $\\hat{k} = 33.16$.\n\n## Preparation: Determining boundaries for parameters\n\nSome parameters come with bounds, for example:\n\n- Kinetic rate: $k > 0$\n- Probability: $0 \\le p \\le 1$\n\nTwo ways of accounting for parameter bounds:\n\n- Adding penalty terms to the objective function\n- Transforming the parameter so it becomes unconstrained\n\n## Adding penalty terms\n\nSuppose we want $\\alpha \\le \\theta \\le \\beta$. Add **penalty term** to objective function:\n$$\n  J_\\text{constrained}(\\theta) = J_\\text{unconstrained}(\\theta) + J_\\text{penalty}(\\theta)\n$$\nwhere $J_\\text{penalty}(\\theta)$ is\n\n- Roughly zero between $\\alpha$ and $\\beta$\n- Very large for $\\theta < \\alpha$ or $\\theta > \\beta$.\n\n![](./images/03a-parameter-estimation/penalty.pdf){fig-align=center height=75%}\n\n## Transformation parameters\n\nTransform constrained problem into equivalent **unconstrained** problem.\n\nSome examples:\n\n- If $\\theta > 0$: write $\\theta = \\exp \\varphi$ or $\\theta = \\varphi^2$\n- If $-1 < \\theta < 1$: write $\\theta = \\tanh \\varphi$\n\nIn either case, $\\varphi$ is unconstrained (can range from $-\\infty$ to $+\\infty$). Now substitute this transformation into the objective function, and optimize in terms of $\\varphi$.\n\n## Preparation: Dealing with non-identifiability\n\nIn some cases, parameters cannot be determined uniquely. For example, exponential model with parameters $A, B, C$:\n$$\n  y = A \\exp(Bx + C) = (A e^C) e^{Bx}\n$$\nOnly $B$ and the combination $Ae^C$ can be determined.\n\n- **Structural** identifiability: all parameters can be uniquely determined, given perfect data.\n- **Practical** identifiability: same, but from finite, noisy data.\n\n## Preparation: Dealing with non-identifiability\n\nMinimization of objective function will **fail** if some parameters are not identifiable. Workarounds:\n\n- Add penalty term to $J$ to privilege certain parameter values\n- Rewrite $J$ so all parameters are identifiable\n\nExample: put $k = A e^C$ and write exponential model as\n$$\n  y = k \\exp(Bx).\n$$\nBoth $k$ and $B$ are identifiable.\n\n# Minimizing the objective function\n\n## General approach\n\nRecall that we are trying to find $\\theta$ so that\n$$\n  J(\\theta) = \\sum_{i=1}^N (y_i - f(x_i; \\theta))^2\n$$\nis minimized.\n\n- For **linear** model: direct, one-step solution\n- For **nonlinear** model: iterative algorithm. Typically:\n  1. Start with initial guess for $\\hat{\\theta}$\n  2. Slightly change $\\hat{\\theta}$ and compute $J(\\hat{\\theta})$ \n  3. Repeat if $\\hat{\\theta}$ not good enough\n\n## Very simple minimization algorithm: hill descender\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n\\footnotesize\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Initial guess\ntheta <- 5.0\n\nfor (i in 1:100) {\n  # Add random noise to theta\n  theta_new <- \n    theta + 0.5 * rnorm(1)\n\n  # Accept if objective is lower\n  if (J(theta_new) < J(theta)) {\n    theta <- theta_new\n  }\n}\n```\n:::\n\n\n\n:::\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Run the algorithm again but now only plot output\nset.seed(1234)\n\nJ <- function(theta) {\n  (theta - 1)^2\n}\n\ntheta <- 5.0\nsuccessful_thetas <- c(theta)\nfor (i in 1:100) {\n  # Add some random noise to theta\n  theta_new <- theta + 0.5 * rnorm(1)\n\n  # Accept if objective is lower\n  if (J(theta_new) < J(theta)) {\n    theta <- theta_new\n    successful_thetas <- c(successful_thetas, theta)\n  }\n}\n\ntrajectory <- tibble(\n  theta = successful_thetas,\n  J = J(successful_thetas),\n  i = seq(1, length(theta)))\n\nggplot(trajectory) +\n  geom_function(fun = J, xlim = c(-4, 6)) +\n  geom_point(aes(x = theta, y = J), color = \"red\") +\n  geom_text(aes(x = theta, y = J, label = i),\n            color = \"red\", check_overlap = TRUE,\n            nudge_y = 2) +\n  xlab(TeX(\"\\\\theta\")) +\n  ylab(TeX(\"J(\\\\theta)\"))\n```\n\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=2in height=2in}\n:::\n:::\n\n\n\n:::\n::::\n\n## Caveat: local and global minima\n\n- Linear problems: unique minium\n- Nonlinear problems: (typically) several local minima\n\n![](./images/03a-parameter-estimation/local-global-minimum.pdf)\n\nMost minimization algorithms only guarantee **convergence to a local minimum**.\n\n# Minimization algorithms\n\n## Gradient-based minimization algorithms\n\nTwo main classes of minimization algorithms:\n\n1. **Gradient-based methods**\n2. Gradient-free methods\n\nGradient-based methods:\n\n- Are typically faster\n- Require the objective function to be differentiable\n- Can fail to converge \n\nExamples:\n\n- Steepest descent\n- Newton\n- Gauss-Newton\n- Levenberg-Marquardt\n\n\n## Method of steepest descent\n\n::: columns\n::: {.column width=\"50%\"}\nYou want to go down the mountain into the valley as efficiently as possible.\n\n\\vspace*{0.5cm}\n\nThe fog prevents you from seeing more than a few meters in every direction.\n\n\\vspace*{0.5cm}\n\nHow do you proceed?\n\n\\vspace*{0.5cm}\n\n\\emoji{light-bulb} Walk in the direction of **steepest descent**\n:::\n\n::: {.column width=\"50%\"}\n![](images/03a-parameter-estimation/cdf-wanderer.jpeg)\n:::\n:::\n\n## Direction of steepest descent\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n\\vspace*{0.5cm}\n\nGradient:\n\n- Perpendicular to level sets of $J$\n- Direction of steepest ascent\n\n:::\n::: {.column width=\"40%\"}\n![](./images/03a-parameter-estimation/gradient.pdf){fig-align=center width=50%}\n:::\n\n::::\n\n\\vspace*{0.5cm}\n\nTo **decrease** $J(\\theta)$, take a small step in direction of negative gradient:\n\\begin{eqnarray*}\ns_k & = & -\\nabla J(\\theta^k) \\\\\n    & = & -\\left[ \n  \\begin{array}{c}\n    \\frac{\\partial J(\\theta)}{\\partial \\theta_1} |_{\\theta^k} \\\\\n    \\frac{\\partial J(\\theta)}{\\partial \\theta_2} |_{\\theta^k} \\\\\n    \\vdots \\\\\n    \\frac{\\partial J(\\theta)}{\\partial \\theta_n} |_{\\theta^k}\n  \\end{array}\\right].\n\\end{eqnarray*}\n\n\n## Method of steepest descent: Algorithm\n\nAlgorithm:\n\n-   Compute gradient $\\nabla J(\\theta^k)$ at current value $\\theta^k$.\n-   Follow negative gradient to update $\\theta^k$: $$\n    \\theta^{k+1} = \\theta^k - \\alpha_k \\nabla J(\\theta^k),\n    $$ with $\\alpha_k$ the step size.\n-   Repeat until convergence\n\nStep size $\\alpha_k$ can be\n\n-   Fixed: $\\alpha_k = \\alpha$ for a small fixed $\\alpha$ (e.g. $\\alpha = 0.01$).\n-   Adaptive: determine the best $\\alpha_k$ at each step.\n\n## Method of steepest descent: variable step size\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n## Method of steepest descent: disadvantages\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n-   Convergence can be slow (e.g for minimum hidden inside narrow \"valley\")\n-   Steepest descent path will zigzag towards minimum, making little progress at each iteration.\n\n## Method of Newton: 1D case\n\nFind a minimum of $J(\\theta)$ by solving $J'(\\theta) = 0$.\n\n::: columns\n::: {.column width=\"60%\"}\n-   For a starting point $\\theta_k$, look for a search direction $s_k$ such that $J'(\\theta_k + s_k) \\approx 0$.\n\n-   Taylor: $J'(\\theta_k + s_k)$ is approximately $$\n      J'(\\theta_k + s_k) \\approx J'(\\theta_k) + s_k J''(\\theta_k).\n    $$\n\n-   Search direction: $$\n    s_k = -\\frac{J'(\\theta_k)}{J''(\\theta_k)}\n    $$\n:::\n\n::: {.column width=\"30%\"}\n![](images/03a-parameter-estimation/newton2.jpeg)\n:::\n:::\n\nUses information from **first** and **second** derivatives.\n\n## Method of Newton: properties\n\nFor a quadratic function $J(x) = Ax^2 + Bx + C$, Newton's method finds the minimum in **one step**.\n\nGeometric interpretation:\n\n-   Approximate $J(x)$ around $x_k$ by best-fitting parabola.\n-   Jump to bottom of parabola to find $x_{k+1}$.\n-   Repeat!\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## Method of Newton: higher dimensions\n\nSearch direction uses gradient and **Hessian** $$\n    s_k = -\\left[ H(\\theta^k)\\right]^{-1} \\nabla J(\\theta^k)\n    $$ where $$\n    H(\\theta^k) = \\nabla^2 J(\\theta^k) =\n            \\left[ \\begin{array}{cccc}\n                  \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_1^2} |_{\\theta^k} & \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_1 \\partial \\theta_2} |_{\\theta^k} & \\cdots & \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_1 \\partial \\theta_n} |_{\\theta^k} \\\\\n                  \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_2 \\partial \\theta_1} |_{\\theta^k} & \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_2^2} |_{\\theta^k} & \\cdots & \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_2 \\partial \\theta_n} |_{\\theta^k} \\\\\n                  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n                  \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_n \\partial \\theta_1} |_{\\theta^k} & \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_n \\partial \\theta_2} |_{\\theta^k} & \\cdots & \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_n^2} |_{\\theta^k}\n              \\end{array}\\right]\n    $$\n\n- In practice, not necessary to invert $H(\\theta)$\n- Still requires $\\mathcal{O}(D^2)$ computation at each step (expensive)\n\n## Method of Newton: advantages and disadvantages\n\nAdvantages:\n\n-   Less iterations needed\n-   Choice direction more efficient: descent and curvature\n\nDisadvantages:\n\n-   More sensitive to local extrema\n-   First **and** second order differentials\n-   Step size $\\alpha=1$. If initial vector too far from minimum, method will often not converge to minimum.\n\n## Method of Newton: convergence\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n-   Very fast convergence for Rosenbrock function (3 iterations)\n-   In general: **quadratic convergence**\n\n## Many advanced gradient-based methods exist\n\n-   Broyden-Fletcher-Goldfarb-Shanno (BFGS): approximation of Hessian\n-   Levenberg-Marquardt: very popular, combines\n    -   Steepest descent: robust but slow\n    -   Method of Newton: fast, but often not convergent\n-   Powell/Brent: search along set of directions\n\n::: {.callout-note}\n## Optimization in R\n\nUse `optim(par, fn)`, where\n\n- `par`: initial guess\n- `fn`: the function to optimize\n- `method`: \"Nelder-Mead\" (default), \"BFGS\", \"Brent\", ...\n:::\n\n## Worked-out example: M. merluccius\n\n1. Define the objective function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeverton_holt <- function(S, theta) {\n  return(theta[[1]]*S/(1 + S/theta[[2]]))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nJ <- function(theta, x, y) {\n  resid <- y - beverton_holt(x, theta)\n  return(sum(resid^2))\n}\n```\n:::\n\n\n\n2. Specify the initial parameters:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta0 <- c(6, 20)\n```\n:::\n\n\n\n##\n\n3. Run the optimizer\n\n\\footnotesize\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- optim(theta0, J, \n             method = \"BFGS\",\n             x = M.merluccius$spawn.biomass,\n             y = M.merluccius$num.fish)\nfit \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$par\n[1]  5.751024 33.157154\n\n$value\n[1] 2809.001\n\n$counts\nfunction gradient \n      45       15 \n\n$convergence\n[1] 0\n\n$message\nNULL\n```\n\n\n:::\n:::\n\n\n\n##\n\n4. Evaluate the fit\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(M.merluccius, aes(spawn.biomass, num.fish)) +\n  geom_point() +\n  geom_function(\n    fun = \\(S) beverton_holt(S, fit$par),\n    xlim = c(0, 80))\n```\n\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n\nMore sophisticated ways to look at the fit will come later.\n\n## Gradient-free minimization algorithms\n\nTwo main classes of minimization algorithms:\n\n1. Gradient-based methods\n2. **Gradient-free methods**\n\nGradient-free methods:\n\n- Are typically slower\n- Can work even if the objective function is not differentiable\n- Are more robust\n\nExamples:\n\n- Direction set (Powell, Brent)\n- Simplex\n- Global minimisation\n\n## Simplex algorithm (Nelder-Mead 1965)\n\nBasic idea: Capture optimal value inside simplex (triangle, pyramid, ...)\n\n- Start with random simplex.\n- Adjust worst corner of simplex by using different \"actions\".\n- Repeat until convergence.\n\n![](images/03a-parameter-estimation/nelder-mead-actions.pdf){fig-align=center}\n\n## Simplex algorithm\n\n![](images/03a-parameter-estimation/nelder-mead.pdf)\n\n## Simplex algorithm: advantages and disadvantages\n\n- Does not require gradient, Hessian, ... information\n- Robust: often finds a minimum where other optimizers cannot.\n- Can find a rough approximation of a minimum in just a few updates...\n- ... but may take a long time to converge completely.\n\n## Example: M. mercullius\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- optim(theta0, J, \n             method = \"Nelder-Mead\",\n             x = M.merluccius$spawn.biomass,\n             y = M.merluccius$num.fish)\nfit$par\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  5.751348 33.153782\n```\n\n\n:::\n\n```{.r .cell-code}\nfit$count\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction gradient \n      73       NA \n```\n\n\n:::\n:::\n\n\nCompared to BFGS:\n\n- Almost same parameter values\n- More function evaluations, no gradient evaluations\n\n## Global minimization\n\n- Disadvantage local techniques: local minima can never be completely excluded\n- Global techniques insensitive to this problem\n- Disadvantage: needs a lot of evaluations of $J$\n- Types:\n    - Gridding\n    - Random methods\n\n## Global minimisation: Gridding\n\n- Evaluate $J$ for a grid of parameter values $\\theta$\n- Select minimum among grid values\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n## Global minimisation: Gridding\n\nThe finer the grid:\n\n- the more likely to find the optimum,\n- BUT the more calculations needed\n\nIterative:\n\n- Start with a coarse-grained grid\n- Refine parameter domain and repeat\n\nBrute force, inefficient\n\n## Global minimisation: Random methods\n\nEvaluate $J$ for random parameter sets\n\n- Choose PDF for each parameter\n- Random sampling; Latin hypercube sampling\n\nRetain\n\n- Optimal set (with $J_{min}$)\n- Some sets below certain critical value ($J_{crit}$)\n\nExamples:\n\n- Genetic algorithms\n- Shuffled complex evolution\n- Ant colony optimization\n- Particle swarm optimization\n- Simulated annealing\n- ...\n\n\n# Assessing the quality of a fit\n\n## Residuals\n\nModel:\n$$\n  y = f(x; \\theta) + \\epsilon\n$$\nwhere $\\epsilon$ is normally distributed.\n\nIf the model is well-fit, the residuals $e_i = y_i - f(x_i; \\theta)$ should be \n\n- Independent\n- Normally distributed with mean 0 and constant variance.\n\nCan be checked with QQ-plot of residuals\n\n## Example: M. mercullius\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresids <- with(M.merluccius, {\n  num.fish - beverton_holt(spawn.biomass, fit$par)\n})\n\neasy_qqplot <- function(data) {\n  ggplot(tibble(sample = data), aes(sample = sample)) +\n    stat_qq_line(color = \"gray\") + stat_qq() + \n    xlab(NULL) +\n    ylab(NULL)\n}\n\np <- ggplot(tibble(x = seq_along(resids), y = resids), aes(x, y)) +\n  geom_point() +\n  xlab(NULL) +\n  ylab(\"Residual\")\n\nq <- easy_qqplot(resids)\n\ngrid.arrange(p, q, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=4.5in height=2in}\n:::\n:::\n\n\n\nNo pattern in residuals + normality: model appears well-fit.\n\n# Correlations in time series (Optional)\n\n## Residuals: correlation and independence\n\n- We often assume that residuals are independent. But this is not always the case, especially in **time series**.\n- Correlations in residuals are often a sign that something is missing from model fit.\n\nHow can we detect patterns, correlations, ... in residuals?\n\n\\vspace*{1cm}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-20-1.png){fig-align='center' width=4.5in height=2in}\n:::\n:::\n\n\n\n\n## Autocorrelation: how are residuals related?\n\n**Autocorrelation** with lag $\\tau$ answers the following questions:\n\n- To what extent does a residual depend on a previous residual?\n- Is there correlation between residuals in time?\n\n$$\n  r_\\varepsilon(\\tau) = \\frac{1}{r_\\varepsilon(0)}\\sum_{k=1}^{N-\\tau} \\frac{\\varepsilon(t_k) \\cdot \\varepsilon(t_k+\\tau)}{N-\\tau}\n$$\n\nwhere $r_\\varepsilon(0)=\\sum_{k=1}^{N} \\frac{\\varepsilon^2(t_k)}{N}$\n\n## Detecting significant autocorrelations\n\nIf data is uncorrelated, then autocorrelation is normally distributed:\n$$\n  r_\\varepsilon(\\tau) \\sim \\mathcal{N}\\left(0, \\frac{1}{N}\\right).\n$$\nCan be used to detect \"abnormally high\" correlations: \n\n- Only about 5% of values outside range $\\pm 1.96/\\sqrt{N}$.\n- If more, sign that data is correlated.\n\n\n## Example: Energy consumption in Korea (2017-19)\n\nAutocorrelation uncovers repeating patterns in signal:\n\n- Highly correlated over 12-month basis\n- Anticorrelated over 6-month basis\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmonths <- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \n            \"May\", \"Jun\", \"Jul\", \"Aug\", \n            \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\nenergy <- read.csv(\"scripts/03a-parameter-estimation/energy.csv\") %>%\n  fill(Year) %>%\n  mutate(Date = make_date(Year, match(Month, months), 1))\n\nautocorr_plot <- function(x, plot_thresholds = FALSE, conf_level = 0.95) {\n  plotdata <- with(acf(x, plot = FALSE), \n                   data.frame(lag, acf))\n\n  p <- ggplot(plotdata, aes(x = lag, y = acf)) +\n         geom_bar(stat = \"identity\", position = \"identity\") +\n    xlab(\"Lag\") +\n    ylab(\"ACF\")\n\n  if (plot_thresholds) {\n    threshold <- qnorm((1 - conf_level) / 2) / sqrt(length(x))\n    p <- p +\n        geom_hline(yintercept=c(threshold, -threshold),\n                   linetype=\"dashed\", color = \"red\", linewidth = 1)\n  }\n  \n  p\n}\n\np <- energy %>%\n  ggplot(aes(Date, Consumption)) +\n  geom_line() + geom_point() +\n  theme_classic() +\n  scale_x_date(NULL,\n               breaks = scales::breaks_width(\"3 months\"),\n               labels = scales::label_date_short()) +\n  scale_y_continuous(\"Energy consumption (1000s of TOE)\",\n                     breaks = scales::breaks_extended(8),\n  )\n\nq <- autocorr_plot(energy$Consumption, plot_thresholds = TRUE)\n\ngrid.arrange(p, q, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=4.5in height=2in}\n:::\n:::\n\n\n\n\nSource: Korea Energy Economics Institute.\n\n## How to deal with correlations in residuals?\n\n- **Make model bigger**: next slides\n- Subsample data to reduce strength of correlations: not recommended\n- Use modelling technique that does not need uncorrelated residuals (e.g. autoregressive models): outside scope of this course\n\n## Example: Calcium flows (simulated data)\n\nOver the course of exercise, calcium ions flow in and out of the muscle cells. On biological grounds, model calcium concentration as exponentially damped sine:\n$$\n  C(t) = \\exp(-A t) \\sin(t)\n$$\n\nData and model fit:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1234)\n\nt <- seq(0, 20, length.out = 50)\ny_large <- exp(-t/10)*sin(t)\ny_perturb <- 0.2 * cos(t)\ny_noise <- 0.1 * rnorm(length(t))\n\ny_full <- y_large + y_perturb + y_noise\n\nfit <- nls(y ~ exp(-t/A)*sin(t),\n           data = data.frame(t = t, y = y_full),\n           start = list(A = 10))\nA_fitted <- coef(fit)[\"A\"]\n\ntibble(t = t, y_full = y_full, y_predict = predict(fit)) |>\n  ggplot() +\n  geom_point(aes(t, y_full)) +\n  geom_line(aes(t, y_predict)) +\n  xlab(\"Time\") +\n  ylab(\"Calcium conc.\")\n```\n\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n\n## Residual plot\n\nModel fit is good, but not perfect. Clear **repeating pattern** in the residuals.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresids <- y_full - predict(fit)\ntibble(t = t, resids = resids) |>\n  ggplot(aes(t, resids)) +\n  geom_point() +\n  geom_line(linetype = \"dashed\") +\n  xlab(\"Time\") +\n  ylab(\"Residual\")\n```\n\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-23-1.png){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n\n## Autocorrelation plot\n\nLack of model fit, repeating pattern in the residuals can also be seen from the autocorrelation plot.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautocorr_plot(resids, plot_thresholds = TRUE)\n```\n\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-24-1.png){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n\n- Red lines: thresholds $1.96 / \\sqrt{50} = 0.227$.\n- 13 out of 17 autocorrelations (76%) exceed threshold \n\n\n## Expanding the model\n\nPattern in residuals is a clear sign that **something is missing** in our modelling approach. Given the periodic oscillations, propose\n$$\n  C(t) = \\exp(-At)\\sin(t) + B\\cos(\\omega t).\n$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_full <- nls(y ~ exp(-t/A)*sin(t) + B*cos(C*t),\n            data = data.frame(t = t, y = y_full),\n            start = list(A = 10, B = 1.0, C = 1.0))\nA_fitted <- coef(fit)[\"A\"]\nB_fitted <- coef(fit)[\"B\"]\nC_fitted <- coef(fit)[\"C\"]\n\neasy_predict <- function(model, t) {\n  predict(model, newdata = data.frame(t = t))\n}\n\ndata <- tibble(t = t, y_full = y_full)\nt_dense <- seq(min(t), max(t), length.out = 500)\ncurves <- tibble(\n  t = t_dense,\n  y_p = easy_predict(fit, t_dense),\n  y_p_full = easy_predict(fit_full, t_dense))\n\nggplot() +\n  geom_point(data = data, aes(t, y_full)) +\n  geom_line(data = curves,\n            aes(t, y_p, linetype = \"Original\"),\n            color = \"gray50\") +\n  geom_line(data = curves, \n            aes(t, y_p_full, linetype = \"Expanded\")) +\n  xlab(\"Time\") +\n  ylab(\"Calcium conc.\") +\n  scale_linetype_manual(\n    values = c(\"Expanded\" = \"solid\", \"Original\" = \"dashed\"),\n    name = \"Model\")\n```\n\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=4in height=2in}\n:::\n:::\n\n\n\n## Residual and autocorrelation plot\n\nNo residual pattern visible in residuals. The model is well fit.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresids_full <- y_full - predict(fit_full)\np <- tibble(t = t, resids = resids_full) |>\n  ggplot(aes(t, resids)) +\n  geom_point() +\n  geom_line(linetype = \"dashed\") +\n  xlab(\"Time\") +\n  ylab(\"Residual\")\n\nq <- autocorr_plot(resids_full, plot_thresholds = TRUE)\n\ngrid.arrange(p, q, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-26-1.png){fig-align='center' width=4.5in height=2in}\n:::\n:::\n\n\n\n## Residual QQ-plots\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np <- easy_qqplot(resids) +\n  ggtitle(\"Original model\")\nq <- easy_qqplot(resids_full) +\n  ggtitle(\"Expanded model\")\n\ngrid.arrange(p, q, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=4.5in height=2in}\n:::\n:::",
    "supporting": [
      "03a-parameter-estimation_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}