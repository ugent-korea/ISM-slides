{
  "hash": "3c800551f49795b5226d1e0947d296a8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Nonlinear Modeling: Parameter Estimation\"\nsubtitle: Introduction to Statistical Modelling\nauthor: Prof. Joris Vankerschaver\npdf-engine: lualatex\nformat:\n  beamer:\n    theme: Pittsburgh\n    colortheme: default\n    fonttheme: default\n    header-includes: |\n      \\setbeamertemplate{frametitle}[default][left]\n      \\setbeamertemplate{footline}[frame number]\n      \\usepackage{emoji}\n      \\usepackage{luatexko}\n\n---\n\n\n\n\n## Outline\n\n\\tableofcontents\n\n## Learning outcomes\n\nYou should be able to\n\n- Determine the parameters of a nonlinear model via minimization (using R)\n- Understand the principles behind various minimization algorithms, as well as their advantages and disadvantages\n- Be able to assess the fit of a model\n\n# Example: building a stock-recruitment model\n\n## M. merluccius: stock-recruitment model\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\\vspace*{0.5cm}\n\nEuropean hake (*M. merluccius*)\n\n- Deep water fish\n- Important for European fisheries\n- Similar to 명태 in Korea\n\n:::\n::: {.column width=\"50%\"}\n![](./images/03a-parameter-estimation/2560px-Fish_-_Mercato_Orientale_-_Genoa,_Italy_-_DSC02485.jpeg)\n\n:::\n:::\n\n::: {.callout-note}\n\n## Stock-recruitment model\n\nModel of **number of adult fish** (recruitment) as a function of **spawning biomass** (fish that can reproduce).\n:::\n\n## M.merluccius: Dataset\n\n15 observations, 3 features:\n\n- `spawn.biomass`: spawning (stock) biomass\n- `num.fish`: number of fish (recruitment)\n- `year`: not used\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-2-1.pdf){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n\n## M. merluccius: Beverton-Holt model\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\\vspace*{0.5cm}\n\nBeverton-Holt model (1956):\n$$\n  f(S; \\alpha, k) = \\frac{\\alpha S}{1 + S/k}\n$$\n:::\n::: {.column width=\"50%\"}\n![](./images/03a-parameter-estimation/beverton-holt.pdf){fig-align=center width=75%}\n:::\n::::\n\n\nParameters:\n\n- $\\alpha$: initial growth rate (for $S = 0$)\n  $$\n    \\alpha = f'(0; \\alpha, k)\n  $$\n- $k$: related to behavior for large $S$\n  $$\n    k \\alpha = \\lim_{S \\to +\\infty} f(S; \\alpha, k)\n  $$\n\n\n## Beverton-Holt: Effect of varying $\\alpha$ and $k$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-3-1.pdf){fig-align='center' width=4.5in height=2in}\n:::\n:::\n\n\n## Goals\n\n- **Parameter estimation**: Find values $\\hat{\\alpha}$ and $\\hat{k}$ that best fit data.\n- **Uncertainty quantification**: Provide a measure of uncertainty for parameter values (confidence interval)\n- **Sensitivity analysis**: Understand how model changes if parameters are varied\n\n# Parameter estimation\n\n## What is parameter estimation?\n\nDetermining the **optimal values for the parameters** using the experimental data, assuming that the model is known.\n\nExample: For *M. merluccius*, we will see that $\\hat{\\alpha} = 5.75$, $\\hat{k} = 33.16$.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-4-1.pdf){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n## Specifying a model\n\nAssume that we are **given** a nonlinear model\n$$\n  y = f(x; \\theta) + \\epsilon\n$$\nwhere $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ is normally distributed noise.\n\n- $x$: inputs, predictors, features (e.g. `spawn.biomass`)\n- $y$: outcome, depent variable (e.g. `num.fish`)\n- $\\theta$: (vector of) parameters (e.g. $\\theta = (\\alpha, k)$)\n\nWe will not talk about **building** a model (see one of your many other courses)\n\n## The objective function\n\nGiven a dataset $(x_1, y_1), \\ldots, (x_N, y_N)$, we want to quantify how well the model fits the data.\n\n**Objective function**: measures difference (squared) between predictions $f(x_i; \\theta)$ and actual values $y_i$:\n$$\n  J(\\theta) = \\sum_{i=1}^N (y_i - f(x_i; \\theta))^2\n$$\n\n## Minimizing the objective function\n\n**Goal:** Find the parameter value(s) $\\hat{\\theta}$ so that $J(\\theta)$ is minimal:\n$$\n  \\hat{\\theta} = \\text{argmin}_\\theta\\, J(\\theta).\n$$\nProblems:\n\n- Depending on $f(x; \\theta)$ this can be very difficult\n- There may be multiple (local) minima\n- Almost always needs to be done numerically\n\n## Example: linear regression\n\nIn linear regression, $f(x; \\theta) = \\alpha + \\beta x$, so that\n$$\n  J(\\alpha, \\beta) = \\sum_{i = 1}^N (y_i - \\alpha - \\beta x_i)^2.\n$$\nMinimizing $J(\\alpha, \\beta)$ can be done by setting the partial derivatives equal to zero and gives the usual formulas:\n$$\n  \\hat{\\beta} = R \\frac{s_y}{s_x}, \\quad \\hat{\\alpha} = \\bar{y} - \\hat{\\beta} \\bar{x}.\n$$\n\nIn general, **no closed-form formula exists** for the optimal parameters.\n\n## Before parameter estimation: select parameters\n\nMore parameters = more work and less certainty:\n\n- Solver may not converge\n- Wider uncertainty estimates for parameters and outputs\n- Correlations between parameters can make it impossible to find parameters\n\nConsider selecting subset of parameters to estimate:\n\n- Fix parameters at experimental values\n- Omit least sensitive parameters\n\n\n## Before parameter estimation: select initial values\n\nNumerical optimization algorithm requires a good **starting guess** for the parameters. When choice is bad:\n\n- Algorithm will converge slowly (take many iterations)\n- Optimization will fail altogether\n\nHow to find initial guess:\n\n- Determine from model properties (growth rate, asymptotes)\n- Use (known) experimental values\n- Use trial and error (select from grid of values)\n\nDoesn't need to be overly precise, a rough estimate is usually sufficient. \n\n## Initial values for M. merluccius\n\n- Slope: $\\alpha_0 = \\frac{75}{15} = 6$\n- Horizontal asymptote: $k_0 \\alpha_0 = 120$, so $k_0 = 20$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-5-1.pdf){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\nLater, we will see that the initial guesses are close to the optimal parameters $\\hat{\\alpha} = 5.75$, $\\hat{k} = 33.16$.\n\n## Preparation: Determining boundaries for parameters\n\nSome parameters come with bounds, for example:\n\n- Kinetic rate: $k > 0$\n- Probability: $0 \\le p \\le 1$\n\nTwo ways of accounting for parameter bounds:\n\n- Adding penalty terms to the objective function\n- Transforming the parameter so it becomes unconstrained\n\n## Adding penalty terms\n\nSuppose we want $\\alpha \\le \\theta \\le \\beta$. Add **penalty term** to objective function:\n$$\n  J_\\text{constrained}(\\theta) = J_\\text{unconstrained}(\\theta) + J_\\text{penalty}(\\theta)\n$$\nwhere $J_\\text{penalty}(\\theta)$ is\n\n- Roughly zero between $\\alpha$ and $\\beta$\n- Very large for $\\theta < \\alpha$ or $\\theta > \\beta$.\n\n![](./images/03a-parameter-estimation/penalty.pdf){fig-align=center height=75%}\n\n## Transformation parameters\n\nTransform constrained problem into equivalent **unconstrained** problem.\n\nSome examples:\n\n- If $\\theta > 0$: write $\\theta = \\exp \\varphi$ or $\\theta = \\varphi^2$\n- If $-1 < \\theta < 1$: write $\\theta = \\tanh \\varphi$\n\nIn either case, $\\varphi$ is unconstrained (can range from $-\\infty$ to $+\\infty$). Now substitute this transformation into the objective function, and optimize in terms of $\\varphi$.\n\n## Preparation: Dealing with non-identifiability\n\nIn some cases, parameters cannot be determined uniquely. For example, exponential model with parameters $A, B, C$:\n$$\n  y = A \\exp(Bx + C) = (A e^C) e^{Bx}\n$$\nOnly $B$ and the combination $Ae^C$ can be determined.\n\n- **Structural** identifiability: all parameters can be uniquely determined, given perfect data.\n- **Practical** identifiability: same, but from finite, noisy data.\n\n## Preparation: Dealing with non-identifiability\n\nMinimization of objective function will **fail** if some parameters are not identifiable. Workarounds:\n\n- Add penalty term to $J$ to privilege certain parameter values\n- Rewrite $J$ so all parameters are identifiable\n\nExample: put $k = A e^C$ and write exponential model as\n$$\n  y = k \\exp(Bx).\n$$\nBoth $k$ and $B$ are identifiable.\n\n# Minimizing the objective function\n\n## General approach\n\nRecall that we are trying to find $\\theta$ so that\n$$\n  J(\\theta) = \\sum_{i=1}^N (y_i - f(x_i; \\theta))^2\n$$\nis minimized.\n\n- For **linear** model: direct, one-step solution\n- For **nonlinear** model: iterative algorithm. Typically:\n  1. Start with initial guess for $\\hat{\\theta}$\n  2. Slightly change $\\hat{\\theta}$ and compute $J(\\hat{\\theta})$ \n  3. Repeat if $\\hat{\\theta}$ not good enough\n\n## Very simple minimization algorithm: hill descender\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n\\footnotesize\n\n::: {.cell}\n\n```{.r .cell-code}\n# Initial guess\ntheta <- 5.0\n\nfor (i in 1:100) {\n  # Add random noise to theta\n  theta_new <- \n    theta + 0.5 * rnorm(1)\n\n  # Accept if objective is lower\n  if (J(theta_new) < J(theta)) {\n    theta <- theta_new\n  }\n}\n```\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-7-1.pdf){fig-align='center' width=2in height=2in}\n:::\n:::\n\n\n:::\n::::\n\n## Caveat: local and global minima\n\n- Linear problems: unique minium\n- Nonlinear problems: (typically) several local minima\n\n![](./images/03a-parameter-estimation/local-global-minimum.pdf)\n\nMost minimization algorithms only guarantee **convergence to a local minimum**.\n\n# Minimization algorithms\n\n## Gradient-based minimization algorithms\n\nTwo main classes of minimization algorithms:\n\n1. **Gradient-based methods**\n2. Gradient-free methods\n\nGradient-based methods:\n\n- Are typically faster\n- Require the objective function to be differentiable\n- Can fail to converge \n\nExamples:\n\n- Steepest descent\n- Newton\n- Gauss-Newton\n- Levenberg-Marquardt\n\n\n## Method of steepest descent\n\n::: columns\n::: {.column width=\"50%\"}\nYou want to go down the mountain into the valley as efficiently as possible.\n\n\\vspace*{0.5cm}\n\nThe fog prevents you from seeing more than a few meters in every direction.\n\n\\vspace*{0.5cm}\n\nHow do you proceed?\n\n\\vspace*{0.5cm}\n\n\\emoji{light-bulb} Walk in the direction of **steepest descent**\n:::\n\n::: {.column width=\"50%\"}\n![](images/03a-parameter-estimation/cdf-wanderer.jpeg)\n:::\n:::\n\n## Direction of steepest descent\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n\\vspace*{0.5cm}\n\nGradient:\n\n- Perpendicular to level sets of $J$\n- Direction of steepest ascent\n\n:::\n::: {.column width=\"40%\"}\n![](./images/03a-parameter-estimation/gradient.pdf){fig-align=center width=50%}\n:::\n\n::::\n\n\\vspace*{0.5cm}\n\nTo **decrease** $J(\\theta)$, take a small step in direction of negative gradient:\n\\begin{eqnarray*}\ns_k & = & -\\nabla J(\\theta^k) \\\\\n    & = & -\\left[ \n  \\begin{array}{c}\n    \\frac{\\partial J(\\theta)}{\\partial \\theta_1} |_{\\theta^k} \\\\\n    \\frac{\\partial J(\\theta)}{\\partial \\theta_2} |_{\\theta^k} \\\\\n    \\vdots \\\\\n    \\frac{\\partial J(\\theta)}{\\partial \\theta_n} |_{\\theta^k}\n  \\end{array}\\right].\n\\end{eqnarray*}\n\n\n## Method of steepest descent: Algorithm\n\nAlgorithm:\n\n-   Compute gradient $\\nabla J(\\theta^k)$ at current value $\\theta^k$.\n-   Follow negative gradient to update $\\theta^k$: $$\n    \\theta^{k+1} = \\theta^k - \\alpha_k \\nabla J(\\theta^k),\n    $$ with $\\alpha_k$ the step size.\n-   Repeat until convergence\n\nStep size $\\alpha_k$ can be\n\n-   Fixed: $\\alpha_k = \\alpha$ for a small fixed $\\alpha$ (e.g. $\\alpha = 0.01$).\n-   Adaptive: determine the best $\\alpha_k$ at each step.\n\n## Method of steepest descent: variable step size\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-8-1.pdf)\n:::\n:::\n\n\n## Method of steepest descent: disadvantages\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-9-1.pdf)\n:::\n:::\n\n\n-   Convergence can be slow (e.g for minimum hidden inside narrow \"valley\")\n-   Steepest descent path will zigzag towards minimum, making little progress at each iteration.\n\n## Method of Newton: 1D case\n\nFind a minimum of $J(\\theta)$ by solving $J'(\\theta) = 0$.\n\n::: columns\n::: {.column width=\"60%\"}\n-   For a starting point $\\theta_k$, look for a search direction $s_k$ such that $J'(\\theta_k + s_k) \\approx 0$.\n\n-   Taylor: $J'(\\theta_k + s_k)$ is approximately $$\n      J'(\\theta_k + s_k) \\approx J'(\\theta_k) + s_k J''(\\theta_k).\n    $$\n\n-   Search direction: $$\n    s_k = -\\frac{J'(\\theta_k)}{J''(\\theta_k)}\n    $$\n:::\n\n::: {.column width=\"30%\"}\n![](images/03a-parameter-estimation/newton2.jpeg)\n:::\n:::\n\nUses information from **first** and **second** derivatives.\n\n## Method of Newton: properties\n\nFor a quadratic function $J(x) = Ax^2 + Bx + C$, Newton's method finds the minimum in **one step**.\n\nGeometric interpretation:\n\n-   Approximate $J(x)$ around $x_k$ by best-fitting parabola.\n-   Jump to bottom of parabola to find $x_{k+1}$.\n-   Repeat!\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-10-1.pdf)\n:::\n:::\n\n\n## Method of Newton: higher dimensions\n\nSearch direction uses gradient and **Hessian** $$\n    s_k = -\\left[ H(\\theta^k)\\right]^{-1} \\nabla J(\\theta^k)\n    $$ where $$\n    H(\\theta^k) = \\nabla^2 J(\\theta^k) =\n            \\left[ \\begin{array}{cccc}\n                  \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_1^2} |_{\\theta^k} & \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_1 \\partial \\theta_2} |_{\\theta^k} & \\cdots & \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_1 \\partial \\theta_n} |_{\\theta^k} \\\\\n                  \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_2 \\partial \\theta_1} |_{\\theta^k} & \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_2^2} |_{\\theta^k} & \\cdots & \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_2 \\partial \\theta_n} |_{\\theta^k} \\\\\n                  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n                  \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_n \\partial \\theta_1} |_{\\theta^k} & \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_n \\partial \\theta_2} |_{\\theta^k} & \\cdots & \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_n^2} |_{\\theta^k}\n              \\end{array}\\right]\n    $$\n\n- In practice, not necessary to invert $H(\\theta)$\n- Still requires $\\mathcal{O}(D^2)$ computation at each step (expensive)\n\n## Method of Newton: advantages and disadvantages\n\nAdvantages:\n\n-   Less iterations needed\n-   Choice direction more efficient: descent and curvature\n\nDisadvantages:\n\n-   More sensitive to local extrema\n-   First **and** second order differentials\n-   Step size $\\alpha=1$. If initial vector too far from minimum, method will often not converge to minimum.\n\n## Method of Newton: convergence\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-11-1.pdf)\n:::\n:::\n\n\n-   Very fast convergence for Rosenbrock function (3 iterations)\n-   In general: **quadratic convergence**\n\n## Many advanced gradient-based methods exist\n\n-   Broyden-Fletcher-Goldfarb-Shanno (BFGS): approximation of Hessian\n-   Levenberg-Marquardt: very popular, combines\n    -   Steepest descent: robust but slow\n    -   Method of Newton: fast, but often not convergent\n-   Powell/Brent: search along set of directions\n\n::: {.callout-note}\n## Optimization in R\n\nUse `optim(par, fn)`, where\n\n- `par`: initial guess\n- `fn`: the function to optimize\n- `method`: \"Nelder-Mead\" (default), \"BFGS\", \"Brent\", ...\n:::\n\n## Worked-out example: M. merluccius\n\n1. Define the objective function:\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nJ <- function(theta, x, y) {\n  resid <- y - beverton_holt(x, theta)\n  return(sum(resid^2))\n}\n```\n:::\n\n\n2. Specify the initial parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta0 <- c(6, 20)\n```\n:::\n\n\n##\n\n3. Run the optimizer\n\n\\footnotesize\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- optim(theta0, J, \n             method = \"BFGS\",\n             x = M.merluccius$spawn.biomass,\n             y = M.merluccius$num.fish)\nfit \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$par\n[1]  5.751024 33.157154\n\n$value\n[1] 2809.001\n\n$counts\nfunction gradient \n      45       15 \n\n$convergence\n[1] 0\n\n$message\nNULL\n```\n\n\n:::\n:::\n\n\n##\n\n4. Evaluate the fit\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-16-1.pdf){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\nMore sophisticated ways to look at the fit will come later.\n\n## Gradient-free minimization algorithms\n\nTwo main classes of minimization algorithms:\n\n1. Gradient-based methods\n2. **Gradient-free methods**\n\nGradient-free methods:\n\n- Are typically slower\n- Can work even if the objective function is not differentiable\n- Are more robust\n\nExamples:\n\n- Direction set (Powell, Brent)\n- Simplex\n- Global minimisation\n\n## Simplex algorithm (Nelder-Mead 1965)\n\nBasic idea: Capture optimal value inside simplex (triangle, pyramid, ...)\n\n- Start with random simplex.\n- Adjust worst corner of simplex by using different \"actions\".\n- Repeat until convergence.\n\n![](images/03a-parameter-estimation/nelder-mead-actions.pdf){fig-align=center}\n\n## Simplex algorithm\n\n![](images/03a-parameter-estimation/nelder-mead.pdf)\n\n## Simplex algorithm: advantages and disadvantages\n\n- Does not require gradient, Hessian, ... information\n- Robust: often finds a minimum where other optimizers cannot.\n- Can find a rough approximation of a minimum in just a few updates...\n- ... but may take a long time to converge completely.\n\n## Example: M. mercullius\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- optim(theta0, J, \n             method = \"Nelder-Mead\",\n             x = M.merluccius$spawn.biomass,\n             y = M.merluccius$num.fish)\nfit$par\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  5.751348 33.153782\n```\n\n\n:::\n\n```{.r .cell-code}\nfit$count\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction gradient \n      73       NA \n```\n\n\n:::\n:::\n\nCompared to BFGS:\n\n- Almost same parameter values\n- More function evaluations, no gradient evaluations\n\n## Global minimization\n\n- Disadvantage local techniques: local minima can never be completely excluded\n- Global techniques insensitive to this problem\n- Disadvantage: needs a lot of evaluations of $J$\n- Types:\n    - Gridding\n    - Random methods\n\n## Global minimisation: Gridding\n\n- Evaluate $J$ for a grid of parameter values $\\theta$\n- Select minimum among grid values\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-18-1.pdf)\n:::\n:::\n\n\n## Global minimisation: Gridding\n\nThe finer the grid:\n\n- the more likely to find the optimum,\n- BUT the more calculations needed\n\nIterative:\n\n- Start with a coarse-grained grid\n- Refine parameter domain and repeat\n\nBrute force, inefficient\n\n## Global minimisation: Random methods\n\nEvaluate $J$ for random parameter sets\n\n- Choose PDF for each parameter\n- Random sampling; Latin hypercube sampling\n\nRetain\n\n- Optimal set (with $J_{min}$)\n- Some sets below certain critical value ($J_{crit}$)\n\nExamples:\n\n- Genetic algorithms\n- Shuffled complex evolution\n- Ant colony optimization\n- Particle swarm optimization\n- Simulated annealing\n- ...\n\n\n# Assessing the quality of a fit\n\n## Residuals\n\nModel:\n$$\n  y = f(x; \\theta) + \\epsilon\n$$\nwhere $\\epsilon$ is normally distributed.\n\nIf the model is well-fit, the residuals $e_i = y_i - f(x_i; \\theta)$ should be \n\n- Independent\n- Normally distributed with mean 0 and constant variance.\n\nCan be checked with QQ-plot of residuals\n\n## Example: M. mercullius\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-19-1.pdf){fig-align='center' width=4.5in height=2in}\n:::\n:::\n\n\nNo pattern in residuals + normality: model appears well-fit.\n\n# Correlations in time series (Optional)\n\n## Residuals: correlation and independence\n\n- We often assume that residuals are independent. But this is not always the case, especially in **time series**.\n- Correlations in residuals are often a sign that something is missing from model fit.\n\nHow can we detect patterns, correlations, ... in residuals?\n\n\\vspace*{1cm}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-20-1.pdf){fig-align='center' width=4.5in height=2in}\n:::\n:::\n\n\n\n## Autocorrelation: how are residuals related?\n\n**Autocorrelation** with lag $\\tau$ answers the following questions:\n\n- To what extent does a residual depend on a previous residual?\n- Is there correlation between residuals in time?\n\n$$\n  r_\\varepsilon(\\tau) = \\frac{1}{r_\\varepsilon(0)}\\sum_{k=1}^{N-\\tau} \\frac{\\varepsilon(t_k) \\cdot \\varepsilon(t_k+\\tau)}{N-\\tau}\n$$\n\nwhere $r_\\varepsilon(0)=\\sum_{k=1}^{N} \\frac{\\varepsilon^2(t_k)}{N}$\n\n## Detecting significant autocorrelations\n\nIf data is uncorrelated, then autocorrelation is normally distributed:\n$$\n  r_\\varepsilon(\\tau) \\sim \\mathcal{N}\\left(0, \\frac{1}{N}\\right).\n$$\nCan be used to detect \"abnormally high\" correlations: \n\n- Only about 5% of values outside range $\\pm 1.96/\\sqrt{N}$.\n- If more, sign that data is correlated.\n\n\n## Example: Energy consumption in Korea (2017-19)\n\nAutocorrelation uncovers repeating patterns in signal:\n\n- Highly correlated over 12-month basis\n- Anticorrelated over 6-month basis\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-21-1.pdf){fig-align='center' width=4.5in height=2in}\n:::\n:::\n\n\n\nSource: Korea Energy Economics Institute.\n\n## How to deal with correlations in residuals?\n\n- **Make model bigger**: next slides\n- Subsample data to reduce strength of correlations: not recommended\n- Use modelling technique that does not need uncorrelated residuals (e.g. autoregressive models): outside scope of this course\n\n## Example: Calcium flows (simulated data)\n\nOver the course of exercise, calcium ions flow in and out of the muscle cells. On biological grounds, model calcium concentration as exponentially damped sine:\n$$\n  C(t) = \\exp(-A t) \\sin(t)\n$$\n\nData and model fit:\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-22-1.pdf){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n## Residual plot\n\nModel fit is good, but not perfect. Clear **repeating pattern** in the residuals.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-23-1.pdf){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n## Autocorrelation plot\n\nLack of model fit, repeating pattern in the residuals can also be seen from the autocorrelation plot.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-24-1.pdf){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n- Red lines: thresholds $1.96 / \\sqrt{50} = 0.227$.\n- 13 out of 17 autocorrelations (76%) exceed threshold \n\n\n## Expanding the model\n\nPattern in residuals is a clear sign that **something is missing** in our modelling approach. Given the periodic oscillations, propose\n$$\n  C(t) = \\exp(-At)\\sin(t) + B\\cos(\\omega t).\n$$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-25-1.pdf){fig-align='center' width=4in height=2in}\n:::\n:::\n\n\n## Residual and autocorrelation plot\n\nNo residual pattern visible in residuals. The model is well fit.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-26-1.pdf){fig-align='center' width=4.5in height=2in}\n:::\n:::\n\n\n## Residual QQ-plots\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03a-parameter-estimation_files/figure-beamer/unnamed-chunk-27-1.pdf){fig-align='center' width=4.5in height=2in}\n:::\n:::",
    "supporting": [
      "03a-parameter-estimation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}