{
  "hash": "131f207d04752aee1c60323ec598c704",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Nonlinear Modeling: Quality of parameter estimates\"\nsubtitle: Introduction to Statistical Modelling\nauthor: Prof. Joris Vankerschaver\npdf-engine: lualatex\nformat:\n  beamer:\n    theme: Pittsburgh\n    colortheme: default\n    fonttheme: default\n    header-includes: |\n      \\setbeamertemplate{frametitle}[default][left]\n      \\setbeamertemplate{footline}[frame number]\n      \\usepackage{emoji}\n      \\usepackage{luatexko}\n\n---\n\n\n\n\n\n## Learning outcomes\n\nYou should be able to\n\n- Understand the interpretation of measurement noise\n- Explain the role of the Fisher information matrix in quantifying parameter uncertainty\n- Compute a confidence interval for a parameter\n- Compute the correlation between two parameters\n\n\n## Quality of estimation\n\n- Apart from obtaining parameter estimates, we want to know a measure of uncertainty for these values.\n\n- Main idea: use objective function $J(\\theta)$ to quantify uncertainty.\n  - High curvature: low uncertainty (parameters well determined)\n  - Low curvature: high uncertainty (not well determined)\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03b-quality-of-estimation_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n## Synthetic data\n\nModel: logistic curve\n$$\n  y = \\frac{A}{1 + \\exp(k(x_\\text{mid} - x))} + \\epsilon,\n$$\nwhere $\\epsilon \\sim N(0, \\sigma^2)$.\n\n- Parameters: $A = 5.6$, $k = 1.4$, $x_\\text{mid} = 2.5$.\n- Measurement noise: $\\sigma^2 = 0.2$ (the measure).\n\nWe take $n = 20$ data points from this model:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03b-quality-of-estimation_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=672 height=33%}\n:::\n:::\n\n\n\n## Model fit\n\n- From now on, we \"forget\" the true parameters, and we will work with the data only.\n- Nonlinear least squares: $A = 5.359$, $k = 1.597$, $x_\\text{mid} = 2.500$.\n\n\\vspace*{0.5cm}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03b-quality-of-estimation_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=672 height=50%}\n:::\n:::\n\n\n\n\n## Measurement variance\n\n- Typically measurement variance is not known.\n- If model well-fitted: estimate from residuals:\n$$\n  \\sigma^2 \\approx \\frac{J(\\theta_\\text{best})}{N - p}.\n$$\n- Here $\\sigma^2 \\approx 0.523/17 = 0.031$ (true value $\\sigma = 0.2^2 = 0.04$)\n\n\\vspace*{0.5cm}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03b-quality-of-estimation_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=672 height=50%}\n:::\n:::\n\n\n\n## The loss surface\n\n- Surface obtained by plotting $J(\\theta)$ for all $\\theta$ in some range.\n- Optimal parameters are minima on this surface.\n- When more than 2 parameters: focus on subset of parameters.\n- **For visualization only.** (Higher dimensions: calculus)\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n![](./images/03a-parameter-estimation/loss-surface-3d.png)\n:::\n::: {.column width=\"50%\"}\n![](./images/03a-parameter-estimation/loss-surface-2d.png)\n:::\n::::\n\n## Exact confidence region\n\nConfidence region: all $\\theta$ such that\n$$\n  J(\\theta) \\le \\left(1 + \\frac{p}{N-p}F_{p,N-p,1-\\alpha} \\right) \\times J(\\theta_\\text{best}),\n$$\nwhere $F_{p,N-p,1-\\alpha}$ is quantile from $F$-distribution, $\\alpha$ is significance level.\n\n- Reasonably exact for models that are not too nonlinear.\n- Easy to calculate numerically\n- **Hard to describe or use explicitly**\n\n## Exact confidence region\n\n![](./images/03a-parameter-estimation/level-sets-ci.png){fig-align='center'}\n\n## Approximate confidence region\n\nTaylor expansion to second order:\n\\begin{multline*}\n  J(\\theta) \\approx\n    J(\\theta_\\text{best}) +\n    \\sum_{i = 1}^N \\underbrace{\\frac{\\partial J}{\\partial \\theta_i}(\\theta_\\text{best})}_{= 0}(\\theta - \\theta_\\text{best})_i + \\\\\n    \\frac{1}{2} \\sum_{i,j=1}^N \\frac{\\partial^2 J}{\\partial \\theta_i \\partial \\theta_j}\n      (\\theta - \\theta_\\text{best})_i(\\theta - \\theta_\\text{best})_j.\n\\end{multline*}\n\nConfidence region becomes\n$$\n  (\\theta - \\theta_\\text{best})^T \\mathcal{I} (\\theta - \\theta_\\text{best}) \\le p F_{p, N-p, 1-\\alpha}.\n$$\nwith $\\mathcal{I}$ the **Fisher Information Matrix (FIM)**:\n$$\n  \\mathcal{I}_{ij} =\n    \\frac{1}{\\sigma^2} \\sum_{k = 1}^N \\left(\n    \\frac{\\partial y}{\\partial \\theta_i}(x_k, \\theta_\\text{best})\n    \\frac{\\partial y}{\\partial \\theta_j}(x_k, \\theta_\\text{best}) \\right).\n$$\n\n## Interpretation of the FIM\n\n- The FIM tells us how much information the data give us about the model parameters.\n\n- Alternatively, the FIM contains two ingredients:\n  - The **sensitivity functions**, given by\n$$\n    s_i(x, \\theta) = \\frac{\\partial y}{\\partial \\theta_i}.\n$$\n  Variables that are sensitive to perturbations in a parameter contain a lot of information about that parameter, and will contribute a lot to the FIM (and vice versa).\n  - The **measurement noise** $\\sigma^2$. Measurements with lots of noise contain less information about the parameters.\n\n## Approximate confidence region\n\n- Level sets of quadratic approximation are ellipsoids.\n- Good approximation to exact confidence region close to optimum.\n\n![](./images/03a-parameter-estimation/level-sets-ci-quadratic.png){fig-align='center'}\n\n\n## Variance/covariance of parameters\n\n- Often, we want to know variance of individual parameters and covariance between parameters.\n- Encoded in the error covariance matrix:\n$$\n  C = \\begin{bmatrix}\n      \\sigma^2_{\\theta_1} & \\text{cov}(\\theta_1,\\theta_2) & \\cdots & \\text{cov}(\\theta_1,\\theta_p) \\\\\n      \\text{cov}(\\theta_2,\\theta_1) & \\sigma^2_{\\theta_2} & \\cdots & \\text{cov}(\\theta_2,\\theta_p) \\\\\n      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n      \\text{cov}(\\theta_p,\\theta_1) & \\text{cov}(\\theta_p,\\theta_2) & \\cdots & \\sigma^2_{\\theta_p}\n  \\end{bmatrix}.\n$$\n\n- Diagonal: variances, off-diagonal: covariances\n- Can be used to construct correlations between between parameters:\n$$\n  R_{ij} = \\frac{\\text{cov}(\\theta_i, \\theta_j)}{\\sigma_{\\theta_i} \\sigma_{\\theta_j}}.\n$$\n\n\n## Computing the error covariance matrix\n\n- The inverse of the FIM $\\mathcal{I}$ is a lower bound for $C$:\n$$\n  C \\ge \\mathcal{I}^{-1}.\n$$\n- **This is not an obvious result.**\n- In practice, we just take $\\mathcal{I}^{-1}$ as an estimate for $C$.\n- Approximate confidence interval for parameter $\\theta_i$:\n$$\n  (\\theta_\\text{best})_i \\pm t_{N-p, 1-\\alpha/2} \\sqrt{C_{ii}}.\n$$\n\n## Worked-out example: logistic model\n\nTo compute the FIM:\n\n- Measurement noise: $\\sigma^2 \\approx 0.031$ (see earlier).\n- Sensitivity functions:\n$$\n  \\frac{\\partial y}{\\partial A} = \\frac{1}{1 + \\exp(k(x_\\text{mid} - x))}, \\quad\n  \\frac{\\partial y}{\\partial k} = \\ldots, \\quad\n    \\frac{\\partial y}{\\partial x_\\text{mid}} = \\ldots\n$$\n  Often these functions have to be computed **numerically** (see next chapter).\n\n\n## Logistic model with synthetic data\n\n- The inverse of the FIM is given by\n$$\n  \\mathcal{I}^{-1} = \\begin{bmatrix}\n  0.0057 & -0.0034 & 0.0020 \\\\\n -0.0034 & 0.0169 & -0.0015 \\\\\n  0.0020 & -0.0015  & 0.0040 \\\\\n  \\end{bmatrix}.\n$$\n\n- Parameter estimates: $A = 5.359$, $k = 1.597$, $x_\\text{mid} = 2.500$.\n\n- 95% confidence intervals (low, high):\n\n| Parameter | Estimate | Low | High |\n|-----------|----------|-----|------|\n| $A$       | 5.36 | 5.20 | 5.52 |\n| $k$       | 1.60 | 1.32 | 1.87 |\n| $x_\\text{mid}$ | 2.45 | 2.32 | 2.58 |\n\n- Correlation between $A$ and $k$: $R = -0.0034/\\sqrt{0.057 \\times 0.0169} = -0.110$.\n\n## Worked-out example: stock-recruitment model\n\n\n\n\n\n\n\nOptimal parameters:\n\n- $\\alpha = 5.75$\n- $k = 33.16$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbh_fit <- nls(\n  num.fish ~ alpha * spawn.biomass / (1 + spawn.biomass / k),\n  data = M.merluccius,\n  start = list(alpha = 6, k = 20))\n\nggplot(M.merluccius, aes(spawn.biomass, num.fish)) +\n  geom_point() +\n  geom_function(\n    fun = \\(S) predict(bh_fit, newdata = data.frame(spawn.biomass = S)), \n    xlim = c(0, 80))\n```\n\n::: {.cell-output-display}\n![](03b-quality-of-estimation_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=3in height=2in}\n:::\n:::\n\n\n\n## \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJ_sr <- function(theta) {\n  x <- M.merluccius$spawn.biomass\n  y <- M.merluccius$num.fish\n  f <- function(S, alpha, k) {\n    alpha * S / (1 + S / k)\n  }\n  resid <- y - f(x, theta[1], theta[2])\n  sum(resid^2)\n}\n\nJ_theta_sr <- J_sr(coef(bh_fit))\nsigma2_sr <- J_theta_sr / 13\n\nround2 <- function(x) {\n  format(ceiling(x * 100) / 100,nsmall=2)\n}\n```\n:::\n\n\n\n\n1. Measurement noise:\n$$\n  \\sigma^2 = \n    \\frac{J(\\theta_\\text{best})}{N - p} = \n    \\frac{2809.01}{13} = \n    216.08\n$$\n1. Sensitivity functions (for Beverton-Holt model):\n$$\n  \\frac{\\partial f}{\\partial \\alpha} = \\frac{S}{1 + S/k}, \\quad\n  \\frac{\\partial f}{\\partial k} = - \\frac{\\alpha S^2}{(k + S)^2}.\n$$\nAgain, typically you would compute these derivatives numerically.\n\n##\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# partial derivatives at optimal parameters\nS <- M.merluccius$spawn.biomass\nalpha_best <- coef(bh_fit)[\"alpha\"]\nk_best <- coef(bh_fit)[\"k\"]\ndf_dalpha <- S / (1 + S / k_best)\ndf_dk <- - alpha_best * S^2 / (k_best + S)^2\ndf <- matrix(c(df_dalpha, df_dk), byrow = TRUE, nrow = 2)\n\n# Fisher matrix and inverse\nFIM_sr <- df %*% t(df) / sigma2_sr\nFIM_inv_sr <- solve(FIM_sr)\n```\n:::\n\n\n\n\n3. Fisher information matrix:\n$$\n  \\mathcal{I} = \\begin{bmatrix}\n    16.10 & -1.41 \\\\\n    -1.41 & 0.14 \\\\    \n  \\end{bmatrix}\n$$\n4. Error-covariance matrix:\n$$\n  C = \\mathcal{I}^{-1} = \\begin{bmatrix}\n    1.07 & 11.43 \\\\\n    11.43 & 130.11 \\\\    \n  \\end{bmatrix}\n$$\n\n##\n\n\n\n\n\n\n\n\nFrom previous slide:\n$$\n  \\sigma^2_\\alpha = 1.07, \\quad \n  \\sigma^2_k = 130.11, \\quad \n  \\text{Cov}(\\alpha, k) = 11.43.\n$$\n\n5. 95% confidence intervals:\n\n  - For $\\alpha$:\n    $$\n      \\alpha_\\text{best} \\pm 2.16 \\times \\sigma_\\alpha =\n      [3.53,\n      7.99]\n    $$\n  - For $k$:\n    $$\n      k_\\text{best} \\pm 2.16 \\times \\sigma_k =\n      [8.52,\n      57.80]\n    $$\n\n6. Parameter covariance:\n$$\n  R = \\frac{\\text{Cov}(\\alpha, k)}{\\sigma_\\alpha \\times \\sigma_k} =\n  \\frac{11.43}{1.04 \\times  11.41} = 0.98.\n$$\n\n## Spaghetti plot\n\nTo get an idea of the variability in the confidence region, sample parameters from it, and plot resulting fitted curves.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mvtnorm)\n\nparameters <-\n  rmvnorm(100, mean = c(alpha_best, k_best), sigma = FIM_inv_sr)\n\nbeverton_holt <- function(S, theta) {\n  theta[1] * S / (1 + S / theta[2])\n}\n\np <- ggplot(parameters, aes(x = alpha, y = k)) +\n  geom_point()\n\nq <- ggplot(M.merluccius, aes(spawn.biomass, num.fish)) +\n  geom_point()\nfor (i in seq_along(numeric(nrow(parameters)))) {\n  q <- q +\n    geom_function(fun = beverton_holt,\n                  args = list(theta = parameters[i, ]),\n                  color = \"red\",\n                  alpha = 0.1)\n}\nq <- q +\n  geom_function(fun = beverton_holt,\n                args = list(theta = c(alpha_best, k_best)),\n                color = \"red\",\n                alpha = 1.0,\n                linewidth = 1.0)\n\ngrid.arrange(p, q, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](03b-quality-of-estimation_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=4.5in height=2in}\n:::\n:::\n\n\n\n\n\n## Key takeaways\n\n- Quality of parameter estimates depends on **model** and **data**, encoded by the FIM.\n- The FIM provides a way of drawing elliptical confidence regions in parameter space.\n- The FIM gives a lower bound for the error-covariance matrix.\n\n\n\n",
    "supporting": [
      "03b-quality-of-estimation_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}