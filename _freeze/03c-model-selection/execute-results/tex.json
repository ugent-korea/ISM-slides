{
  "hash": "aaf4d486691edaa1155f9905e0d58b94",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Nonlinear Modeling: Model selection\"\nsubtitle: Introduction to Statistical Modelling\nauthor: Prof. Joris Vankerschaver\npdf-engine: lualatex\nformat:\n  beamer:\n    theme: Pittsburgh\n    colortheme: default\n    fonttheme: default\n    header-includes: |\n      \\setbeamertemplate{frametitle}[default][left]\n      \\setbeamertemplate{footline}[frame number]\n      \\usepackage{emoji}\n      \\usepackage{luatexko}\n\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Model Selection\n\n## Model selection\n\n- Also called _structure characterisation_\n- Problem: \"perfect\" model and \"true\" parameters are unknown.\n- Goal:  __Select best model structure from set of candidate models, based on experimental data__\n\n## Which model fits the data the best?\n\n- True model: $y = \\sin(x) + \\sqrt{x} + 0.3 \\epsilon$, where $\\epsilon \\sim N(0, 1)$\n- Four datasets, fit polynomial of degree 1, 2, or 8\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03c-model-selection_files/figure-beamer/unnamed-chunk-2-1.pdf)\n:::\n:::\n\n\n## Two sources of error\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n**Bias**: *How well does the model fit the data?*\n\n- Error due to non-modeled phenomena.\n- Decreases as model gets more complex.\n\n\\vspace*{0.5cm}\n**Variance**: *How well does the model do on new, unseen data?*\n\n- Decreases with more data.\n- Increases as model gets more complex.\n\n:::\n\n::: {.column width=\"40%\"}\n\n![](images/03c-model-selection/bias-variance-metaphor.svg)\n\n:::\n\n::::\n\n## Bias and variance are complementary\n\nFor a model $M_D(x)$ on a dataset $D$, the error decomposes as\n$$\n   \\mathrm{Error}[M_D(x)] = \n   \\mathrm{Bias}[M_D(x)]^2 + \\mathrm{Var}[M_D(x)] + \\mathrm{Noise}.\n$$\nGoal model selection: select model with smallest total error = compromise between bias error and variance error\n\n![](images/03c-model-selection/bias-variance-tradeoff.svg){fig-align=\"center\"}\n\n\n## Model selection for linear models\n\n- Same data as before (slide 1)\n- Polynomial model $y \\sim 1 + x + x^2 + \\cdots + x^d$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03c-model-selection_files/figure-beamer/unnamed-chunk-3-1.pdf){fig-align='center' width=3in height=1.5in}\n:::\n:::\n\n\n- Model of degree 2 (quadratic curve) gives best fit (not too complex, not too simple)\n- Bias and variance in general **difficult to calculate**\n\n\n## Case study: biodegradation test\n\n**Waste treatment**: Measure the oxygen uptake rate (OUR) during oxidation of biodegradable waste products by activated sludge.\n    \n- Shape respirogram depends on degradation kinetics and quantity added products\n- Not known a priori $\\rightarrow$ measure and test several models\n    \n## Case study: biodegradation data\n\n1.5 data points per minute, acquired using dissolved oxygen (DO) sensor.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03c-model-selection_files/figure-beamer/unnamed-chunk-4-1.pdf)\n:::\n:::\n\n\n## Case study: general model\n\n- $k$ pollutants $S_1, \\ldots, S_k$.\n\n- Oxygen uptake rate\n$$\nOUR = \\sum_{i=1}^k (1-Y_i)r_{S_i}\n$$\nwhere $Y_i$ is the yield, (fraction of substrate $S_i$ that is not oxidated but transformed in biomass $X$), and $r_{S_i}$ the degradation rate of $S_i$.\n\n- Candidate models differ in number of pollutants $k$ and choice of degradation rates $r_{S_i}$.\n\n## Case study: candidate models\n\n**Model 1**: degradation of one pollutant according to first-order kinetics. Gives *exponentially* decreasing OUR-curve.\n\\begin{align*}\n  r_{S_1} & = \\dfrac{k_{max1}X}{Y_1}S_1 \\\\\n  OUR & = (1-Y_1)r_{S_1}\n\\end{align*}\n\n## Case study: candidate models\n\n**Model 2**: degradation of one pollutant according to *Monod kinetics*. \n\\begin{align*}\n  r_{S_1} & = \\dfrac{\\mu_{max1}X}{Y_1}\\dfrac{S_1}{K_{S_1}+S_1} \\\\\n  OUR & = (1-Y_1)r_{S_1}\n\\end{align*}\n\n## Case study: candidate models\n\n**Model 3**: simultaneous degradation of two pollutants according to Monod kinetics (*double Monod*) without interaction. \n\\begin{align*}\n  r_{S_1} & = \\dfrac{\\mu_{max1}X}{Y_1}\\dfrac{S_1}{K_{S_1}+S_1} \\\\\n  r_{S_2} & = \\dfrac{\\mu_{max2}X}{Y_1}\\dfrac{S_2}{K_{S_2}+S_2} \\\\\n  OUR & = (1-Y_1)r_{S_1}+(1-Y_2)r_{S_2} \\\\\n\\end{align*}\n\n## Case study: parameter estimation\n\nDataset (dots) and best fits (calibrated candidate models based on an SSE-based objective function) of the different models\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03c-model-selection_files/figure-beamer/unnamed-chunk-5-1.pdf)\n:::\n:::\n\n\n# Methods for model selection\n\n## Methods for model selection\n\n- _A priori_ model selection: before parameter estimation\n    - Reduces number of parameter estimations necessary = time gain\n    - Techniques not easy to determine: ad hoc methods\n- _A posteriori_ model selection: after parameter estimation\n    - General methods available \n    - Need parameter estimation for all candidate models = increase in calculation times\n\n## A priori model selection\n\n- Restrict set of model candidates based on properties of data that are independent of parameters.\n- Biodegradation example: inflection points.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03c-model-selection_files/figure-beamer/unnamed-chunk-6-1.pdf)\n:::\n:::\n\n\n##  A posteriori model selection\n\n- Compose set of candidate models\n- Collect experimental dataset(s)\n- Perform parameter estimation for all models\n- Rank candidate models and select best\n- Methods\n    - Goodness-of-fit and complexity penalization\n    - Evaluation of undermodelling\n    - Statistical hypothesis test\n    - Residual analysis\n\n## Goodness-of-fit and complexity penalization\n\nSelect least complex model that describes data (sufficiently) well.\n\nBalance two terms:\n\n1. **Goodness of fit**, measured by sum-squared of residuals (SSR)\n2. **Complexity of the model**, as a function of number of parameters.\n\nMany different criteria to make this concrete.\n\n## Akaike Information Criterion (AIC)\n\nModel complexity penality: $2p$, with $p$ number of parameters:\n$$\n  AIC = N \\ln\\left( \\frac{SSR}{N} \\right) + 2p.\n$$\nProperties:\n\n- Sometimes preferred when prediction accuracy is important and sample size is small\n- Not necessarily consistent (will not select true model even if sample size is large)\n\n\n## Bayes Information Criterion (BIC)\n\nModel complexity penalty: $p \\ln N$\n$$\n  BIC = N \\ln\\left( \\frac{SSR}{N} \\right) + p \\ln N.\n$$\nProperties:\n\n- Will select a simpler model than AIC.\n- Consistent (under some conditions)\n\n## AIC/BIC: Polynomial example\n\n- SSR always decreases when number of parameters increases\n- Penalty terms cause goodness-of-fit to increase at a certain point\n\nExample: Select best linear model $y \\sim 1 + x + \\cdots + x^d$ according to AIC/BIC/... for given data.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03c-model-selection_files/figure-beamer/unnamed-chunk-7-1.pdf){fig-align='center' width=4.5in height=2in}\n:::\n:::\n\n\n\n## AIC/BIC: Polynomial example\n\nOptimal model provides a **good fit** (SSR low) and is **not too complex** (penalty low).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03c-model-selection_files/figure-beamer/unnamed-chunk-8-1.pdf){fig-align='center' width=4.5in height=2in}\n:::\n:::\n\n\n- Both AIC and BIC select fit of degree 3\n- In general AIC and BIC don't have to agree\n\n## AIC/BIC: Biodegradation example\n\n| Model        | p |   SSR   |    AIC    |    BIC    |\n|--------------|---|--------:|----------:|----------:|\n| Exponential  | 2 |   0.36  |  -303.67  |  -299.48  |\n| Single Monod | 3 |   0.16  |  -348.74  |  -342.45  |\n| Double Monod | 6 |   0.01  |  -508.87  |  -496.30  |\n\n<!-- See script \"monod-plot.R\" for code to compute these numbers -->\n\n\n## Statistical hypothesis test\n\n- Choice between 2 models: simple and more complex\n- Is complex model statistically speaking better?\n- Verify using F-test:\n$$\nF = \\dfrac{\\left( \\dfrac{SSR_{simple}-SSR_{complex}}{p_{complex}-p_{simple}} \\right)}{\\left( \\dfrac{SSR_{complex}}{N-p_{complex}} \\right)}\n$$\n- Compare test criterion with tabulated $F_{1-\\alpha,p_{complex}-p_{simple},N-p_{complex}}$ for significance level $\\alpha$\n- If value larger, complex model better (and vice versa)\n\n## Residual analysis\n\n- Hypothesis: model is appropriate if properties of residuals are same as properties of measurement errors\n- Two popular techniques for evaluation independence of residuals\n    - Autocorrelation test (see Parameter Estimation)\n    - Runs test (nonparametric test)\n\n## Autocorrelation test: Biodegradation example\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03c-model-selection_files/figure-beamer/unnamed-chunk-9-1.pdf)\n:::\n:::\n\n\n## Autocorrelation test: Residuals as a function of time\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03c-model-selection_files/figure-beamer/unnamed-chunk-11-1.pdf)\n:::\n:::\n\n\n## Autocorrelation test\n\n- Residuals show some correlation for all three models, indicating that there is some unresolved structure in the data.\n- Correlations for double Monod decay much quicker than the other two models.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03c-model-selection_files/figure-beamer/unnamed-chunk-12-1.pdf)\n:::\n:::\n",
    "supporting": [
      "03c-model-selection_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}