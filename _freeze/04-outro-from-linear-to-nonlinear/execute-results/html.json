{
  "hash": "da28ebc3b4d895ff9e70e5fba099ba78",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: From linear to non-linear models\noutput:\n  pdf_document: default\n  html_document: default\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n```\n:::\n\n\nNote: this Rmarkdown document uses some heavy-duty external packages (particularly the `torch` package for neural networks) that may be difficult to install properly. I encourage you to read through the document and study it, but it is not necessary to run the document for yourself (unless you want to do so).\n\n# Loading the data\n\nFor this demonstration we will use the Boston dataset from the `MASS` package. This dataset contains the median prices of houses in Boston areas (the `medv` column) as well as 13 different predictors. To keep things manageable, we will use only the following 4 predictors:\n\n- `crim`: the crime rate in that neighborhood;\n- `zn`: the amount of land available for large residential properties;\n- `nox`: the level of nitric oxide (air pollution);\n- `dis`: the weighted distance to 5 employment centers in Boston.\n\nThe dataset contains data for 506 neighborhoods. We split it into a training dataset of 406 observations (approx. 80%) and a test dataset with the remaining 100 observations (20%). The idea is that we will use the training dataset to build our model, and the test dataset to get an idea of the model's performance on entirely unknown data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\n\nlittle_boston <- Boston[c(\"crim\", \"zn\", \"nox\", \"dis\", \"medv\")]\n\n# Split into train/test data\ntest_idx <- sample(nrow(little_boston), 100)\nlittle_boston_train <- little_boston[-test_idx,]\nlittle_boston_test <- little_boston[test_idx,]\n\nn_predictors <- ncol(little_boston_train)\nn_obs <- nrow(little_boston_train)\n\nnrow(little_boston_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 406\n```\n\n\n:::\n\n```{.r .cell-code}\nnrow(little_boston_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 100\n```\n\n\n:::\n:::\n\n\n# Building a linear model, with R\n\nIt is easy to build a linear model with R, something that we have done many times in this course. We get a model where all predictors are significant. Chances are that you can improve this model a bit, by e.g. transforming some of the variables, but we will not do so here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(medv ~ ., data = little_boston_train)\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = medv ~ ., data = little_boston_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.624  -4.808  -1.591   2.446  29.001 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  50.81961    3.81315  13.327  < 2e-16 ***\ncrim         -0.30567    0.04863  -6.286 8.50e-10 ***\nzn            0.15235    0.02183   6.980 1.23e-11 ***\nnox         -37.23945    5.29275  -7.036 8.61e-12 ***\ndis          -2.14007    0.32824  -6.520 2.12e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.586 on 401 degrees of freedom\nMultiple R-squared:  0.3202,\tAdjusted R-squared:  0.3134 \nF-statistic: 47.22 on 4 and 401 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n# Building a linear model, by hand\n\nWe can build the same model by hand, using the tools that we developed in the section on nonlinear modeling. \n\nWe will use matrix algebra for this, so let's convert our dataset in a matrix. We actually build two matrices: the `X`-matrix contains our predictors and has everything other than the last column (the `medv` column) and the `y`-matrix is the last column.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX_train <- as.matrix(little_boston_train[-n_predictors])\ny_train <- as.matrix(little_boston_train[n_predictors])\n```\n:::\n\n\nA convenience function that we will need a few times:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprepend_ones <- function(mat) {\n  ones <- rep(1, nrow(mat))\n  return(cbind(ones, mat))\n}\n\nprepend_ones(matrix(c(5, 6, 7, 8), nrow = 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     ones    \n[1,]    1 5 7\n[2,]    1 6 8\n```\n\n\n:::\n:::\n\n\nWith that, we can write down our linear model and our objective function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model <- function(X, thetas) {\n  y_hat <- prepend_ones(X) %*% thetas\n  return(y_hat)\n}\n\nJtheta <- function(thetas, FUN, X, y) {\n  y_hat <- FUN(X, thetas)\n  y_resid <- y - y_hat\n  J <- sum(y_resid^2)\n  return(J)\n}\n```\n:::\n\n\nLet's check if this model is indeed the same model as what R uses. If we give it the regression coefficients that R obtained earlier, we should get the exact same data as in the regression summary. As a check, we compute the residual standard error of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSS <- Jtheta(coef(m), linear_model, X_train, y_train)\ndf <- n_obs - n_predictors\nsqrt(SS/df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.586127\n```\n\n\n:::\n:::\n\n\nNow we go one step further. We use `optim` to find the regression coefficients, and we verify that they agree with the values that R found. This is definitely not the most efficient way of finding the regression coefficients, but it is pretty cool that we can get the same results as R with just a few lines of code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthetas_init <- rep(0, n_predictors)\nlinear_fit <- optim(thetas_init, Jtheta, FUN = linear_model, X = X_train, y = y_train, method = \"L-BFGS\")\nlinear_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$par\n[1]  50.8195838  -0.3056648   0.1523487 -37.2395367  -2.1400685\n\n$value\n[1] 23077.28\n\n$counts\nfunction gradient \n      55       55 \n\n$convergence\n[1] 0\n\n$message\n[1] \"CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH\"\n```\n\n\n:::\n\n```{.r .cell-code}\nlinear_fit$par - coef(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)          crim            zn           nox           dis \n-2.456964e-05  3.185893e-06  1.004713e-06 -8.345091e-05  1.672181e-06 \n```\n\n\n:::\n:::\n\n\nLast, we evaluate the squared error of the model on the test dataset. This is the first time that we use the test dataset, so we can be sure that this provides an unbiased assessment of the model's performance. We will compute this number with the squared error of other models that we will build later on. Note that when comparing two models, lower is better: a model with lower squared error makes predictions that are closer to the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX_test <- as.matrix(little_boston_test[-n_predictors])\ny_test <- as.matrix(little_boston_test[n_predictors])\n\nJtheta(linear_fit$par, linear_model, X_test, y_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5445.96\n```\n\n\n:::\n:::\n\n\n# Building a nonlinear model\n\nIt is unlikely that Boston house prices follow a linear model (and the regression diagnostic plots would confirm that, had we looked at them). We could try to address that defect by transforming the variables and adding higher-order terms to our model. Here, we will not take that approach. Instead, we will take recourse to a bigger model. We will stack two linear models together, separated by a nonlinearity. The nonlinearity that we choose is the so-called \"Rectified Linear Unit (ReLU)\", defined as $\\mathrm{ReLU}(x) = \\max(x, 0)$. It is a very simple function, but it will do the trick. It is plotted below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrelu <- function(x) {\n  pmax(x, 0) \n}\n\nx_plot <- seq(-5, 5, length.out = 11)\ny_plot <- relu(x_plot)\nplot(x_plot, y_plot, type = \"l\", xlab = \"x\", ylab = \"y\", lwd = 2, main = \"The ReLU function\")\n```\n\n::: {.cell-output-display}\n![](04-outro-from-linear-to-nonlinear_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe nonlinear model that we use consists of two linear \"layers\", separated by the ReLU function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnonlinear_model <- function(X, thetas) {\n  # Parameters for both models\n  thetas_1 <- matrix(thetas[1:15], nrow = 5, ncol = 3, byrow = FALSE)\n  thetas_2 <- matrix(thetas[16:19], nrow = 4, ncol = 1)\n  \n  # Model 1\n  y_1 <- prepend_ones(X) %*% thetas_1 \n  \n  # Nonlinearity\n  y_1 <- relu(y_1)\n    \n  # Model 2\n  y_2 <- prepend_ones(y_1) %*% thetas_2\n    \n  return(y_2)\n}\n```\n:::\n\n\nWe will initialize the parameter fit with random data. If you start with all 0s, chances are that you will obtain a parameter fit that is not very good.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthetas_init <- rnorm(19)\n```\n:::\n\n\nFitting the model. We have to give it some more iterations to converge to an optimum, because the loss landscape is very complex.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnonlinear_fit <- optim(thetas_init, Jtheta, FUN = nonlinear_model, X = X_train, y = y_train, method = \"BFGS\",\n                       control = list(maxit = 1000))\nnonlinear_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$par\n [1]  70.7744933  -1.1925480   0.8814684 -56.9852663  -5.0905821   9.3089156\n [7]  -2.8663462  -0.4720493  -0.1890297  -2.8642088  35.0049797 -47.9457529\n[13]   1.3349524 -46.6470151  -2.9857599   9.7097560   0.6937652  -3.6877419\n[19]  -0.3533591\n\n$value\n[1] 18830.5\n\n$counts\nfunction gradient \n     493       96 \n\n$convergence\n[1] 0\n\n$message\nNULL\n```\n\n\n:::\n:::\n\n\nThe error on the unseen data is a bit lower, so performance is a bit better.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJtheta(nonlinear_fit$par, nonlinear_model, X_test, y_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4115.768\n```\n\n\n:::\n:::\n\n\nThe nonlinearity is **really necessary**. If you remove it, you get a linear model that is no better than the model on four predictors that we built earlier (compare the performance on unseen data for both models). This is because stacking together linear models without any nonlinear functions in between is mathematically equivalent to just building a single linear model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_without_nonlinearity <- function(X, thetas) {\n  # Parameters for both models\n  thetas_1 <- matrix(thetas[1:15], nrow = 5, ncol = 3, byrow = FALSE)\n  thetas_2 <- matrix(thetas[16:19], nrow = 4, ncol = 1)\n  \n  # Model 1\n  y_1 <- prepend_ones(X) %*% thetas_1 \n  \n  # Nonlinearity\n  #y_1 <- relu(y_1)\n    \n  # Model 2\n  y_2 <- prepend_ones(y_1) %*% thetas_2\n    \n  return(y_2)\n}\n\nmodel_without_nonlinearity_fit <- optim(\n  thetas_init, Jtheta, FUN = model_without_nonlinearity,\n  X = X_train, y = y_train, method = \"BFGS\",\n  control = list(maxit = 1000))\n\nJtheta(model_without_nonlinearity_fit$par, \n       model_without_nonlinearity, X_test, y_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5445.965\n```\n\n\n:::\n:::\n\n\n# Building a neural network via a 3-party library\n\nWhat we have built in the section above is an example of a **neural network** consisting of two linear layers separated by a nonlinear activation function (ReLU). Finding the optimal parameters of such a network via black-box optimization is not very efficient, though, and as the number of parameters grows this will become well-nigh impossible. Moreover, our handcrafted model is not very flexible, and only allows us to build small variations on the same kind of network.\n\nWith the explosion of interest in neural networks of the past 10 years, it should be no surprise that there are highly performant libraries that allow us to build industrial-strength neural networks. One such framework is Torch, which comes in a Python version (PyTorch) and an R version (`torch`). \n\nBelow we use Torch to build a neural network with 2 layers. Note that training is an iterative process, starting from randomly initialized weights. If you re-run the training process a few times, you will get different results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nX_train_t <- torch_tensor(X_train, dtype = torch_float())\ny_train_t <- torch_tensor(y_train, dtype = torch_float())\n\nX_test_t <- torch_tensor(X_test, dtype = torch_float())\ny_test_t <- torch_tensor(y_test, dtype = torch_float())\n\nmodel <- nn_sequential(\n  # Layer 1\n  nn_linear(4, 10),\n  nn_relu(),\n  # Layer 2\n  nn_linear(10, 1),\n)\n\ncriterion <- nn_mse_loss()\noptimizer <- optim_adamw(model$parameters, lr = 0.1)\n\nn_epochs <- 1000\nfor (epoch in 1:n_epochs) {\n  optimizer$zero_grad()\n\n  y_pred <- model(X_train_t)\n  loss <- criterion(y_pred, y_train_t)\n  loss$backward()\n  optimizer$step()\n  \n  if (epoch %% 100 == 0) {\n    cat(\"Epoch: \", epoch, \"Loss: \", loss$item(), \"\\n\")\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch:  100 Loss:  61.92327 \nEpoch:  200 Loss:  56.68341 \nEpoch:  300 Loss:  50.76983 \nEpoch:  400 Loss:  50.71936 \nEpoch:  500 Loss:  49.75182 \nEpoch:  600 Loss:  49.83955 \nEpoch:  700 Loss:  49.68452 \nEpoch:  800 Loss:  49.63379 \nEpoch:  900 Loss:  49.7033 \nEpoch:  1000 Loss:  49.37725 \n```\n\n\n:::\n:::\n\n\nLast but not least, we evaluate the performance of the network on the unseen test data. We get something that is (usually) a bit lower than with previous models, though not by much. There are two important conclusions to be drawn from this:\n\n1. Fancy, state-of-the-art models such as neural networks are within reach with the tools and techniques that you have learned in this course.\n2. Despite this, building a simple model with only a few parameters can be a very powerful thing to do. It is easy and efficient to do so, and simple models are usually straightforward to interpret and to reason about. Compare for example the interpretation that we gave to the coefficients of a linear regression -- no such interpretation exists for the weights in a neural network.\n\nDepending on the circumstances, simple models may outshine complex ones.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((model(X_test_t) - y_test_t)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n5115.22\n[ CPUFloatType{} ][ grad_fn = <SumBackward0> ]\n```\n\n\n:::\n:::\n",
    "supporting": [
      "04-outro-from-linear-to-nonlinear_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}